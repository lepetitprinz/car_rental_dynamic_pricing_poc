{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Definition\" data-toc-modified-id=\"Definition-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Definition</a></span><ul class=\"toc-item\"><li><span><a href=\"#Import-Library\" data-toc-modified-id=\"Import-Library-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Import Library</a></span></li><li><span><a href=\"#Define-File-Path\" data-toc-modified-id=\"Define-File-Path-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>Define File Path</a></span></li></ul></li><li><span><a href=\"#Data-Preprocessing\" data-toc-modified-id=\"Data-Preprocessing-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Data Preprocessing</a></span><ul class=\"toc-item\"><li><span><a href=\"#Jeju-Visitors-History\" data-toc-modified-id=\"Jeju-Visitors-History-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Jeju Visitors History</a></span><ul class=\"toc-item\"><li><span><a href=\"#Load-dataset\" data-toc-modified-id=\"Load-dataset-2.1.1\"><span class=\"toc-item-num\">2.1.1&nbsp;&nbsp;</span>Load dataset</a></span></li><li><span><a href=\"#Rename-columns\" data-toc-modified-id=\"Rename-columns-2.1.2\"><span class=\"toc-item-num\">2.1.2&nbsp;&nbsp;</span>Rename columns</a></span></li><li><span><a href=\"#Check-NAs-&amp;-Fill-values\" data-toc-modified-id=\"Check-NAs-&amp;-Fill-values-2.1.3\"><span class=\"toc-item-num\">2.1.3&nbsp;&nbsp;</span>Check NAs &amp; Fill values</a></span></li><li><span><a href=\"#Chnage-Data-Types\" data-toc-modified-id=\"Chnage-Data-Types-2.1.4\"><span class=\"toc-item-num\">2.1.4&nbsp;&nbsp;</span>Chnage Data Types</a></span></li><li><span><a href=\"#Drop-columns\" data-toc-modified-id=\"Drop-columns-2.1.5\"><span class=\"toc-item-num\">2.1.5&nbsp;&nbsp;</span>Drop columns</a></span></li><li><span><a href=\"#Reorder-dataset\" data-toc-modified-id=\"Reorder-dataset-2.1.6\"><span class=\"toc-item-num\">2.1.6&nbsp;&nbsp;</span>Reorder dataset</a></span></li></ul></li><li><span><a href=\"#Reservation-History\" data-toc-modified-id=\"Reservation-History-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>Reservation History</a></span><ul class=\"toc-item\"><li><span><a href=\"#Load-dataset\" data-toc-modified-id=\"Load-dataset-2.2.1\"><span class=\"toc-item-num\">2.2.1&nbsp;&nbsp;</span>Load dataset</a></span></li><li><span><a href=\"#Merge-dataset\" data-toc-modified-id=\"Merge-dataset-2.2.2\"><span class=\"toc-item-num\">2.2.2&nbsp;&nbsp;</span>Merge dataset</a></span></li><li><span><a href=\"#Rename-columns\" data-toc-modified-id=\"Rename-columns-2.2.3\"><span class=\"toc-item-num\">2.2.3&nbsp;&nbsp;</span>Rename columns</a></span></li><li><span><a href=\"#Check-NAs-&amp;-Fill-values\" data-toc-modified-id=\"Check-NAs-&amp;-Fill-values-2.2.4\"><span class=\"toc-item-num\">2.2.4&nbsp;&nbsp;</span>Check NAs &amp; Fill values</a></span></li><li><span><a href=\"#Chnage-data-types\" data-toc-modified-id=\"Chnage-data-types-2.2.5\"><span class=\"toc-item-num\">2.2.5&nbsp;&nbsp;</span>Chnage data types</a></span></li><li><span><a href=\"#Filter-dataset\" data-toc-modified-id=\"Filter-dataset-2.2.6\"><span class=\"toc-item-num\">2.2.6&nbsp;&nbsp;</span>Filter dataset</a></span></li><li><span><a href=\"#Group-data\" data-toc-modified-id=\"Group-data-2.2.7\"><span class=\"toc-item-num\">2.2.7&nbsp;&nbsp;</span>Group data</a></span></li><li><span><a href=\"#Drop-columns\" data-toc-modified-id=\"Drop-columns-2.2.8\"><span class=\"toc-item-num\">2.2.8&nbsp;&nbsp;</span>Drop columns</a></span></li><li><span><a href=\"#Reorder-dataset\" data-toc-modified-id=\"Reorder-dataset-2.2.9\"><span class=\"toc-item-num\">2.2.9&nbsp;&nbsp;</span>Reorder dataset</a></span></li><li><span><a href=\"#Add-necessary-columns\" data-toc-modified-id=\"Add-necessary-columns-2.2.10\"><span class=\"toc-item-num\">2.2.10&nbsp;&nbsp;</span>Add necessary columns</a></span></li></ul></li><li><span><a href=\"#Discount-History\" data-toc-modified-id=\"Discount-History-2.3\"><span class=\"toc-item-num\">2.3&nbsp;&nbsp;</span>Discount History</a></span><ul class=\"toc-item\"><li><span><a href=\"#Load-dataset\" data-toc-modified-id=\"Load-dataset-2.3.1\"><span class=\"toc-item-num\">2.3.1&nbsp;&nbsp;</span>Load dataset</a></span></li><li><span><a href=\"#Check-NAs-&amp;-Fill-values\" data-toc-modified-id=\"Check-NAs-&amp;-Fill-values-2.3.2\"><span class=\"toc-item-num\">2.3.2&nbsp;&nbsp;</span>Check NAs &amp; Fill values</a></span></li><li><span><a href=\"#Chnage-Data-Types\" data-toc-modified-id=\"Chnage-Data-Types-2.3.3\"><span class=\"toc-item-num\">2.3.3&nbsp;&nbsp;</span>Chnage Data Types</a></span></li><li><span><a href=\"#Filter-dataset\" data-toc-modified-id=\"Filter-dataset-2.3.4\"><span class=\"toc-item-num\">2.3.4&nbsp;&nbsp;</span>Filter dataset</a></span></li><li><span><a href=\"#Drop-columns\" data-toc-modified-id=\"Drop-columns-2.3.5\"><span class=\"toc-item-num\">2.3.5&nbsp;&nbsp;</span>Drop columns</a></span></li><li><span><a href=\"#Reorder-dataset\" data-toc-modified-id=\"Reorder-dataset-2.3.6\"><span class=\"toc-item-num\">2.3.6&nbsp;&nbsp;</span>Reorder dataset</a></span></li><li><span><a href=\"#Apply-discount-policy-history\" data-toc-modified-id=\"Apply-discount-policy-history-2.3.7\"><span class=\"toc-item-num\">2.3.7&nbsp;&nbsp;</span>Apply discount policy history</a></span></li><li><span><a href=\"#Correct-naming\" data-toc-modified-id=\"Correct-naming-2.3.8\"><span class=\"toc-item-num\">2.3.8&nbsp;&nbsp;</span>Correct naming</a></span></li></ul></li><li><span><a href=\"#Integrate-History-Dataset\" data-toc-modified-id=\"Integrate-History-Dataset-2.4\"><span class=\"toc-item-num\">2.4&nbsp;&nbsp;</span>Integrate History Dataset</a></span><ul class=\"toc-item\"><li><span><a href=\"#Merge-all-of--history-dataset\" data-toc-modified-id=\"Merge-all-of--history-dataset-2.4.1\"><span class=\"toc-item-num\">2.4.1&nbsp;&nbsp;</span>Merge all of  history dataset</a></span></li><li><span><a href=\"#Drop-unnecessary-columns\" data-toc-modified-id=\"Drop-unnecessary-columns-2.4.2\"><span class=\"toc-item-num\">2.4.2&nbsp;&nbsp;</span>Drop unnecessary columns</a></span></li><li><span><a href=\"#Add-seasonality-column\" data-toc-modified-id=\"Add-seasonality-column-2.4.3\"><span class=\"toc-item-num\">2.4.3&nbsp;&nbsp;</span>Add seasonality column</a></span></li><li><span><a href=\"#Add-lead-time-vector\" data-toc-modified-id=\"Add-lead-time-vector-2.4.4\"><span class=\"toc-item-num\">2.4.4&nbsp;&nbsp;</span>Add lead time vector</a></span></li><li><span><a href=\"#Convert-time-format\" data-toc-modified-id=\"Convert-time-format-2.4.5\"><span class=\"toc-item-num\">2.4.5&nbsp;&nbsp;</span>Convert time format</a></span></li><li><span><a href=\"#Update-discount-rate\" data-toc-modified-id=\"Update-discount-rate-2.4.6\"><span class=\"toc-item-num\">2.4.6&nbsp;&nbsp;</span>Update discount rate</a></span></li><li><span><a href=\"#Calculate-reservation-fee\" data-toc-modified-id=\"Calculate-reservation-fee-2.4.7\"><span class=\"toc-item-num\">2.4.7&nbsp;&nbsp;</span>Calculate reservation fee</a></span></li><li><span><a href=\"#Remove-exception\" data-toc-modified-id=\"Remove-exception-2.4.8\"><span class=\"toc-item-num\">2.4.8&nbsp;&nbsp;</span>Remove exception</a></span></li><li><span><a href=\"#Save-preprocessing-dataset\" data-toc-modified-id=\"Save-preprocessing-dataset-2.4.9\"><span class=\"toc-item-num\">2.4.9&nbsp;&nbsp;</span>Save preprocessing dataset</a></span></li></ul></li><li><span><a href=\"#Current-Reservation-Dataset\" data-toc-modified-id=\"Current-Reservation-Dataset-2.5\"><span class=\"toc-item-num\">2.5&nbsp;&nbsp;</span>Current Reservation Dataset</a></span><ul class=\"toc-item\"><li><span><a href=\"#Load-dataset\" data-toc-modified-id=\"Load-dataset-2.5.1\"><span class=\"toc-item-num\">2.5.1&nbsp;&nbsp;</span>Load dataset</a></span></li><li><span><a href=\"#Rename-columns\" data-toc-modified-id=\"Rename-columns-2.5.2\"><span class=\"toc-item-num\">2.5.2&nbsp;&nbsp;</span>Rename columns</a></span></li><li><span><a href=\"#Drop-columns\" data-toc-modified-id=\"Drop-columns-2.5.3\"><span class=\"toc-item-num\">2.5.3&nbsp;&nbsp;</span>Drop columns</a></span></li><li><span><a href=\"#Chnage-date-types\" data-toc-modified-id=\"Chnage-date-types-2.5.4\"><span class=\"toc-item-num\">2.5.4&nbsp;&nbsp;</span>Chnage date types</a></span></li><li><span><a href=\"#Filter-data\" data-toc-modified-id=\"Filter-data-2.5.5\"><span class=\"toc-item-num\">2.5.5&nbsp;&nbsp;</span>Filter data</a></span></li><li><span><a href=\"#Group-data\" data-toc-modified-id=\"Group-data-2.5.6\"><span class=\"toc-item-num\">2.5.6&nbsp;&nbsp;</span>Group data</a></span></li><li><span><a href=\"#Make-dataset-dictionary\" data-toc-modified-id=\"Make-dataset-dictionary-2.5.7\"><span class=\"toc-item-num\">2.5.7&nbsp;&nbsp;</span>Make dataset dictionary</a></span></li><li><span><a href=\"#Save-dictionary\" data-toc-modified-id=\"Save-dictionary-2.5.8\"><span class=\"toc-item-num\">2.5.8&nbsp;&nbsp;</span>Save dictionary</a></span></li></ul></li><li><span><a href=\"#Cum.-Reservation-Counts-on-model\" data-toc-modified-id=\"Cum.-Reservation-Counts-on-model-2.6\"><span class=\"toc-item-num\">2.6&nbsp;&nbsp;</span>Cum. Reservation Counts on model</a></span><ul class=\"toc-item\"><li><span><a href=\"#Load-dataset\" data-toc-modified-id=\"Load-dataset-2.6.1\"><span class=\"toc-item-num\">2.6.1&nbsp;&nbsp;</span>Load dataset</a></span></li><li><span><a href=\"#Rename-columns\" data-toc-modified-id=\"Rename-columns-2.6.2\"><span class=\"toc-item-num\">2.6.2&nbsp;&nbsp;</span>Rename columns</a></span></li><li><span><a href=\"#Change-data-types\" data-toc-modified-id=\"Change-data-types-2.6.3\"><span class=\"toc-item-num\">2.6.3&nbsp;&nbsp;</span>Change data types</a></span></li><li><span><a href=\"#Filter-periods\" data-toc-modified-id=\"Filter-periods-2.6.4\"><span class=\"toc-item-num\">2.6.4&nbsp;&nbsp;</span>Filter periods</a></span></li><li><span><a href=\"#Merge-data\" data-toc-modified-id=\"Merge-data-2.6.5\"><span class=\"toc-item-num\">2.6.5&nbsp;&nbsp;</span>Merge data</a></span></li><li><span><a href=\"#Filter-car-grades-(1.6)\" data-toc-modified-id=\"Filter-car-grades-(1.6)-2.6.6\"><span class=\"toc-item-num\">2.6.6&nbsp;&nbsp;</span>Filter car grades (1.6)</a></span></li><li><span><a href=\"#Group-data\" data-toc-modified-id=\"Group-data-2.6.7\"><span class=\"toc-item-num\">2.6.7&nbsp;&nbsp;</span>Group data</a></span></li><li><span><a href=\"#Change-data-types\" data-toc-modified-id=\"Change-data-types-2.6.8\"><span class=\"toc-item-num\">2.6.8&nbsp;&nbsp;</span>Change data types</a></span></li><li><span><a href=\"#Make-additional-columns\" data-toc-modified-id=\"Make-additional-columns-2.6.9\"><span class=\"toc-item-num\">2.6.9&nbsp;&nbsp;</span>Make additional columns</a></span></li><li><span><a href=\"#Save-reservation-history-dataset\" data-toc-modified-id=\"Save-reservation-history-dataset-2.6.10\"><span class=\"toc-item-num\">2.6.10&nbsp;&nbsp;</span>Save reservation history dataset</a></span></li><li><span><a href=\"#Drop-unnecessary-columns\" data-toc-modified-id=\"Drop-unnecessary-columns-2.6.11\"><span class=\"toc-item-num\">2.6.11&nbsp;&nbsp;</span>Drop unnecessary columns</a></span></li><li><span><a href=\"#Group-by-rent-and-reservation-day\" data-toc-modified-id=\"Group-by-rent-and-reservation-day-2.6.12\"><span class=\"toc-item-num\">2.6.12&nbsp;&nbsp;</span>Group by rent and reservation day</a></span></li><li><span><a href=\"#Merge\" data-toc-modified-id=\"Merge-2.6.13\"><span class=\"toc-item-num\">2.6.13&nbsp;&nbsp;</span>Merge</a></span></li><li><span><a href=\"#Re-group\" data-toc-modified-id=\"Re-group-2.6.14\"><span class=\"toc-item-num\">2.6.14&nbsp;&nbsp;</span>Re-group</a></span></li><li><span><a href=\"#Divde-into-each-car-model\" data-toc-modified-id=\"Divde-into-each-car-model-2.6.15\"><span class=\"toc-item-num\">2.6.15&nbsp;&nbsp;</span>Divde into each car model</a></span></li><li><span><a href=\"#Save-dataset\" data-toc-modified-id=\"Save-dataset-2.6.16\"><span class=\"toc-item-num\">2.6.16&nbsp;&nbsp;</span>Save dataset</a></span></li></ul></li></ul></li><li><span><a href=\"#Model\" data-toc-modified-id=\"Model-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Model</a></span><ul class=\"toc-item\"><li><span><a href=\"#Model-1:-Predict-Jeju-Visitors\" data-toc-modified-id=\"Model-1:-Predict-Jeju-Visitors-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>Model 1: Predict Jeju Visitors</a></span><ul class=\"toc-item\"><li><span><a href=\"#Load-dataset\" data-toc-modified-id=\"Load-dataset-3.1.1\"><span class=\"toc-item-num\">3.1.1&nbsp;&nbsp;</span>Load dataset</a></span></li><li><span><a href=\"#Preprocessing\" data-toc-modified-id=\"Preprocessing-3.1.2\"><span class=\"toc-item-num\">3.1.2&nbsp;&nbsp;</span>Preprocessing</a></span><ul class=\"toc-item\"><li><span><a href=\"#Filter-the-periods\" data-toc-modified-id=\"Filter-the-periods-3.1.2.1\"><span class=\"toc-item-num\">3.1.2.1&nbsp;&nbsp;</span>Filter the periods</a></span></li><li><span><a href=\"#Split-the-dataset\" data-toc-modified-id=\"Split-the-dataset-3.1.2.2\"><span class=\"toc-item-num\">3.1.2.2&nbsp;&nbsp;</span>Split the dataset</a></span></li></ul></li><li><span><a href=\"#Define-Models-&amp;-Validation-Method\" data-toc-modified-id=\"Define-Models-&amp;-Validation-Method-3.1.3\"><span class=\"toc-item-num\">3.1.3&nbsp;&nbsp;</span>Define Models &amp; Validation Method</a></span><ul class=\"toc-item\"><li><span><a href=\"#Define-time-series-models\" data-toc-modified-id=\"Define-time-series-models-3.1.3.1\"><span class=\"toc-item-num\">3.1.3.1&nbsp;&nbsp;</span>Define time series models</a></span></li><li><span><a href=\"#Validation:-Walk-forward-validation\" data-toc-modified-id=\"Validation:-Walk-forward-validation-3.1.3.2\"><span class=\"toc-item-num\">3.1.3.2&nbsp;&nbsp;</span>Validation: Walk forward validation</a></span></li></ul></li><li><span><a href=\"#Evaluation:-Walk-Forward-Validation\" data-toc-modified-id=\"Evaluation:-Walk-Forward-Validation-3.1.4\"><span class=\"toc-item-num\">3.1.4&nbsp;&nbsp;</span>Evaluation: Walk Forward Validation</a></span><ul class=\"toc-item\"><li><span><a href=\"#Set-paramter-grid\" data-toc-modified-id=\"Set-paramter-grid-3.1.4.1\"><span class=\"toc-item-num\">3.1.4.1&nbsp;&nbsp;</span>Set paramter grid</a></span></li><li><span><a href=\"#Walk-forward-validation\" data-toc-modified-id=\"Walk-forward-validation-3.1.4.2\"><span class=\"toc-item-num\">3.1.4.2&nbsp;&nbsp;</span>Walk forward validation</a></span></li><li><span><a href=\"#Save-best-parameter-grid\" data-toc-modified-id=\"Save-best-parameter-grid-3.1.4.3\"><span class=\"toc-item-num\">3.1.4.3&nbsp;&nbsp;</span>Save best parameter grid</a></span></li></ul></li><li><span><a href=\"#Load-paramters-and-fit-the-model\" data-toc-modified-id=\"Load-paramters-and-fit-the-model-3.1.5\"><span class=\"toc-item-num\">3.1.5&nbsp;&nbsp;</span>Load paramters and fit the model</a></span><ul class=\"toc-item\"><li><span><a href=\"#Load-best-parameters\" data-toc-modified-id=\"Load-best-parameters-3.1.5.1\"><span class=\"toc-item-num\">3.1.5.1&nbsp;&nbsp;</span>Load best parameters</a></span></li><li><span><a href=\"#Fit-the-model\" data-toc-modified-id=\"Fit-the-model-3.1.5.2\"><span class=\"toc-item-num\">3.1.5.2&nbsp;&nbsp;</span>Fit the model</a></span></li></ul></li><li><span><a href=\"#Prediction\" data-toc-modified-id=\"Prediction-3.1.6\"><span class=\"toc-item-num\">3.1.6&nbsp;&nbsp;</span>Prediction</a></span><ul class=\"toc-item\"><li><span><a href=\"#Predict-domestic-and-foreign\" data-toc-modified-id=\"Predict-domestic-and-foreign-3.1.6.1\"><span class=\"toc-item-num\">3.1.6.1&nbsp;&nbsp;</span>Predict domestic and foreign</a></span></li><li><span><a href=\"#Get-rate-of-change-on-jeju-visitors\" data-toc-modified-id=\"Get-rate-of-change-on-jeju-visitors-3.1.6.2\"><span class=\"toc-item-num\">3.1.6.2&nbsp;&nbsp;</span>Get rate of change on jeju visitors</a></span></li></ul></li></ul></li><li><span><a href=\"#Model-2:-Predict-Cum.-Res.-Counts\" data-toc-modified-id=\"Model-2:-Predict-Cum.-Res.-Counts-3.2\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>Model 2: Predict Cum. Res. Counts</a></span><ul class=\"toc-item\"><li><span><a href=\"#Make-training-dataset\" data-toc-modified-id=\"Make-training-dataset-3.2.1\"><span class=\"toc-item-num\">3.2.1&nbsp;&nbsp;</span>Make training dataset</a></span><ul class=\"toc-item\"><li><span><a href=\"#Load-dataset\" data-toc-modified-id=\"Load-dataset-3.2.1.1\"><span class=\"toc-item-num\">3.2.1.1&nbsp;&nbsp;</span>Load dataset</a></span></li><li><span><a href=\"#Divide-into-input-and-output-dataset\" data-toc-modified-id=\"Divide-into-input-and-output-dataset-3.2.1.2\"><span class=\"toc-item-num\">3.2.1.2&nbsp;&nbsp;</span>Divide into input and output dataset</a></span></li><li><span><a href=\"#Split-train-and-test-dataset\" data-toc-modified-id=\"Split-train-and-test-dataset-3.2.1.3\"><span class=\"toc-item-num\">3.2.1.3&nbsp;&nbsp;</span>Split train and test dataset</a></span></li></ul></li><li><span><a href=\"#Pre-test\" data-toc-modified-id=\"Pre-test-3.2.2\"><span class=\"toc-item-num\">3.2.2&nbsp;&nbsp;</span>Pre-test</a></span><ul class=\"toc-item\"><li><span><a href=\"#Pre-test-parameters\" data-toc-modified-id=\"Pre-test-parameters-3.2.2.1\"><span class=\"toc-item-num\">3.2.2.1&nbsp;&nbsp;</span>Pre-test parameters</a></span></li><li><span><a href=\"#Model:-SVM\" data-toc-modified-id=\"Model:-SVM-3.2.2.2\"><span class=\"toc-item-num\">3.2.2.2&nbsp;&nbsp;</span>Model: SVM</a></span></li><li><span><a href=\"#Model:-Extra-trees\" data-toc-modified-id=\"Model:-Extra-trees-3.2.2.3\"><span class=\"toc-item-num\">3.2.2.3&nbsp;&nbsp;</span>Model: Extra-trees</a></span></li><li><span><a href=\"#Model:-XGboost\" data-toc-modified-id=\"Model:-XGboost-3.2.2.4\"><span class=\"toc-item-num\">3.2.2.4&nbsp;&nbsp;</span>Model: XGboost</a></span></li><li><span><a href=\"#Compare-results\" data-toc-modified-id=\"Compare-results-3.2.2.5\"><span class=\"toc-item-num\">3.2.2.5&nbsp;&nbsp;</span>Compare results</a></span></li></ul></li><li><span><a href=\"#Training:-Extra-trees-Model\" data-toc-modified-id=\"Training:-Extra-trees-Model-3.2.3\"><span class=\"toc-item-num\">3.2.3&nbsp;&nbsp;</span>Training: Extra-trees Model</a></span><ul class=\"toc-item\"><li><span><a href=\"#Set-model-&amp;-paramter-grid\" data-toc-modified-id=\"Set-model-&amp;-paramter-grid-3.2.3.1\"><span class=\"toc-item-num\">3.2.3.1&nbsp;&nbsp;</span>Set model &amp; paramter grid</a></span></li><li><span><a href=\"#Save-best-parameter-grid\" data-toc-modified-id=\"Save-best-parameter-grid-3.2.3.2\"><span class=\"toc-item-num\">3.2.3.2&nbsp;&nbsp;</span>Save best parameter grid</a></span></li></ul></li><li><span><a href=\"#Load-paramters-and-fit-the-model\" data-toc-modified-id=\"Load-paramters-and-fit-the-model-3.2.4\"><span class=\"toc-item-num\">3.2.4&nbsp;&nbsp;</span>Load paramters and fit the model</a></span><ul class=\"toc-item\"><li><span><a href=\"#Load-best-hyperparameters\" data-toc-modified-id=\"Load-best-hyperparameters-3.2.4.1\"><span class=\"toc-item-num\">3.2.4.1&nbsp;&nbsp;</span>Load best hyperparameters</a></span></li><li><span><a href=\"#Fit-the-model\" data-toc-modified-id=\"Fit-the-model-3.2.4.2\"><span class=\"toc-item-num\">3.2.4.2&nbsp;&nbsp;</span>Fit the model</a></span></li></ul></li><li><span><a href=\"#Prediction\" data-toc-modified-id=\"Prediction-3.2.5\"><span class=\"toc-item-num\">3.2.5&nbsp;&nbsp;</span>Prediction</a></span><ul class=\"toc-item\"><li><span><a href=\"#Set-initial-variables\" data-toc-modified-id=\"Set-initial-variables-3.2.5.1\"><span class=\"toc-item-num\">3.2.5.1&nbsp;&nbsp;</span>Set initial variables</a></span></li><li><span><a href=\"#Make-dataframe-of-initial-setting\" data-toc-modified-id=\"Make-dataframe-of-initial-setting-3.2.5.2\"><span class=\"toc-item-num\">3.2.5.2&nbsp;&nbsp;</span>Make dataframe of initial setting</a></span></li><li><span><a href=\"#Predict-cum.-reservation-counts\" data-toc-modified-id=\"Predict-cum.-reservation-counts-3.2.5.3\"><span class=\"toc-item-num\">3.2.5.3&nbsp;&nbsp;</span>Predict cum. reservation counts</a></span></li><li><span><a href=\"#Save-result-dataset\" data-toc-modified-id=\"Save-result-dataset-3.2.5.4\"><span class=\"toc-item-num\">3.2.5.4&nbsp;&nbsp;</span>Save result dataset</a></span></li></ul></li></ul></li><li><span><a href=\"#Model-3:-Recommend-Best-Price\" data-toc-modified-id=\"Model-3:-Recommend-Best-Price-3.3\"><span class=\"toc-item-num\">3.3&nbsp;&nbsp;</span>Model 3: Recommend Best Price</a></span><ul class=\"toc-item\"><li><span><a href=\"#Define-Discount-Recommendation-Function\" data-toc-modified-id=\"Define-Discount-Recommendation-Function-3.3.1\"><span class=\"toc-item-num\">3.3.1&nbsp;&nbsp;</span>Define Discount Recommendation Function</a></span></li><li><span><a href=\"#Load-dataset\" data-toc-modified-id=\"Load-dataset-3.3.2\"><span class=\"toc-item-num\">3.3.2&nbsp;&nbsp;</span>Load dataset</a></span></li><li><span><a href=\"#Change-data-type\" data-toc-modified-id=\"Change-data-type-3.3.3\"><span class=\"toc-item-num\">3.3.3&nbsp;&nbsp;</span>Change data type</a></span></li><li><span><a href=\"#Calculate-each-data\" data-toc-modified-id=\"Calculate-each-data-3.3.4\"><span class=\"toc-item-num\">3.3.4&nbsp;&nbsp;</span>Calculate each data</a></span></li><li><span><a href=\"#Save-Recommendation-Results\" data-toc-modified-id=\"Save-Recommendation-Results-3.3.5\"><span class=\"toc-item-num\">3.3.5&nbsp;&nbsp;</span>Save Recommendation Results</a></span></li></ul></li></ul></li><li><span><a href=\"#Additional-Analysis\" data-toc-modified-id=\"Additional-Analysis-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Additional Analysis</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import math\n",
    "import time\n",
    "import copy\n",
    "import datetime as dt\n",
    "import pickle\n",
    "from itertools import product\n",
    "from collections import defaultdict\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Plotting\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.font_manager as fm\n",
    "import seaborn as sns\n",
    "font_path = r'C:\\Windows\\Fonts\\NanumBarunGothic.ttf'\n",
    "font_name = fm.FontProperties(fname=font_path).get_name()\n",
    "matplotlib.rc('font', family=font_name)\n",
    "plt.style.use('seaborn')\n",
    "\n",
    "# Data Preprocessing\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler    # Scaling\n",
    "from sklearn.model_selection import train_test_split    # Split train and test dataset\n",
    "\n",
    "# Evaluation\n",
    "from sklearn.metrics import accuracy_score, mean_squared_error\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV, KFold\n",
    "\n",
    "# Time Series Models\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from statsmodels.tsa.ar_model import AutoReg\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "from statsmodels.tsa.holtwinters import ExponentialSmoothing\n",
    "\n",
    "# Machine Learning Models\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.linear_model import LogisticRegression \n",
    "from sklearn.linear_model import SGDRegressor\n",
    "from sklearn import svm    # Support Vector Machine\n",
    "from sklearn.ensemble import ExtraTreesRegressor    # Extra-trees Regressor\n",
    "import xgboost as xgb    # XGboost\n",
    "import lightgbm as lgb   # Light gradient boosting machine\n",
    "\n",
    "# Customized Exponential Model\n",
    "from scipy.optimize import curve_fit\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define File Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Folder root path\n",
    "root_path = os.path.join('..', 'input')\n",
    "save_path = os.path.join('..', 'result')\n",
    "\n",
    "# Reservation history dataset path (yearly dataset)\n",
    "res_hx_18_path = os.path.join(root_path, 'res_18_discount.csv')\n",
    "res_hx_19_path = os.path.join(root_path, 'res_19_discount.csv')\n",
    "res_hx_20_path = os.path.join(root_path, 'res_20_discount.csv')\n",
    "\n",
    "# Reservation history discount dataset path\n",
    "res_hx_discount_18_path = os.path.join(root_path, 'res_result_18.csv')\n",
    "res_hx_discount_19_path = os.path.join(root_path, 'res_result_19.csv')\n",
    "\n",
    "# Specific day reservation dataset path\n",
    "res_curr_path = os.path.join(root_path, 'res_201111.csv')\n",
    "\n",
    "# Jeju visitor dataset path\n",
    "jeju_visit_path = os.path.join(root_path, 'jeju_visit_daily.csv')\n",
    "\n",
    "# Sort of Discount \n",
    "discount_type_path = os.path.join(root_path, 'discount_type.csv')\n",
    "discount_season_path = os.path.join(root_path, 'discount_season.csv')\n",
    "discount_policy_path = os.path.join(root_path, 'discount_policy.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Jeju Visitors History"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "jeju_visit = pd.read_csv(jeju_visit_path, delimiter='\\t', \n",
    "                         dtype={'date': int, 'domestic': int, 'foreign': int, 'total': int})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Rename columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "jeju_remap_cols = {'domestic': 'visit_dom', 'foreign': 'visit_for', 'total': 'visit_tot'}\n",
    "jeju_visit = jeju_visit.rename(columns=jeju_remap_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check NAs & Fill values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of NAs in jeju visitors: 0\n"
     ]
    }
   ],
   "source": [
    "# Chkeck NA values in reservation history\n",
    "na_cnt_jeju = jeju_visit.isna().sum().sum()\n",
    "print(f'Number of NAs in jeju visitors: {na_cnt_jeju}')\n",
    "\n",
    "# Fill values\n",
    "jeju_visit = jeju_visit.fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Chnage Data Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "jeju_visit['rent_day'] = pd.to_datetime(jeju_visit['date'], format='%Y%m%d')\n",
    "\n",
    "jeju_visit['visit_dom'] = jeju_visit['visit_dom'].astype(int)\n",
    "jeju_visit['visit_for'] = jeju_visit['visit_for'].astype(int)\n",
    "jeju_visit['visit_tot'] = jeju_visit['visit_tot'].astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Drop columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "jeju_visit = jeju_visit.drop(columns=['date'], axis=1, errors='ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reorder dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "jeju_visit = jeju_visit.sort_values(by=['rent_day'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "jeju_visit['visit_tot'] = jeju_visit['visit_dom'] + jeju_visit['visit_for']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reservation History"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reservation dataset\n",
    "res_hx_data_type = {'계약번호': int, '예약경로명': str, '고객구분명': str, '총 청구액(VAT포함)': int,\n",
    "                        '예약모델명': str, '차급': str, '대여일': str, '대여시간': str, '반납일': str,\n",
    "                        '반납시간': str, '차량대여요금(VAT포함)': int, 'CDW요금구분명': str, 'CDW요금': int, \n",
    "                        '총대여료(VAT포함)': int, '적용할인율(%) ': float, '예약일자': str}\n",
    "\n",
    "res_hx_18 = pd.read_csv(res_hx_18_path, delimiter='\\t', dtype=res_hx_data_type)\n",
    "res_hx_19 = pd.read_csv(res_hx_19_path, delimiter='\\t', dtype=res_hx_data_type)\n",
    "res_hx_20 = pd.read_csv(res_hx_20_path, delimiter='\\t', dtype=res_hx_data_type)\n",
    "\n",
    "# Reservation Discount dataset\n",
    "res_discount_data_type = {'예약경로': int,'예약경로명': str, '계약번호': int,  '고객': int, \n",
    "                          '고객구분': float, '고객구분명': str, '총 청구액(VAT포함)': int, \n",
    "                          '총 수납금액(VAT포함)': int, '총 잔액(VAT포함)': int, '예약모델': str, \n",
    "                          '예약모델명': str, '차급': str, '대여일': str, '대여시간': str, \n",
    "                          '반납일': str,'반납시간': str, '대여기간(일)': int, '대여기간(시간)': int, \n",
    "                          '실반납일시': int, '실대여기간(일)': int, '실대여기간(시간)': int,\n",
    "                          '차량대여요금(VAT포함)': int, 'CDW가입여부': str, 'CDW요금구분': float, \n",
    "                          'CDW요금구분명': str, 'CDW요금': str, '회원등급': str, '차종': str, \n",
    "                          '구매목적': str, '내부매출액': int, '수납': str, '예약일자': str, \n",
    "                          '할인유형': str, '할인유형명': str, '적용할인명': str}\n",
    "\n",
    "res_hx_18_discount = pd.read_csv(res_hx_discount_18_path, delimiter='\\t', \n",
    "                                 dtype=res_discount_data_type)\n",
    "res_hx_19_discount = pd.read_csv(res_hx_discount_19_path, delimiter='\\t',\n",
    "                                 dtype=res_discount_data_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Merge dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge reservation histroy dataset\n",
    "res_hx = pd.concat([res_hx_18, res_hx_19], axis=0, ignore_index=True)\n",
    "\n",
    "# Merge reservation discount history dataset\n",
    "res_hx_discount = pd.concat([res_hx_18_discount, res_hx_19_discount], axis=0, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Rename columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reservation history\n",
    "res_hx_remap_cols = {'계약번호': 'res_num', '예약경로명': 'res_route_nm', '고객구분명': 'cust_kind_nm',\n",
    "                     '총 청구액(VAT포함)': 'tot_fee', '예약모델명': 'res_model_nm', '차급': 'car_grd', \n",
    "                     '대여일': 'rent_day', '대여시간': 'rent_time', '반납일': 'return_day',\n",
    "                     '반납시간': 'return_time', '차량대여요금(VAT포함)': 'car_rent_fee', \n",
    "                     'CDW요금구분명': 'cdw_fee_kind_nm', 'CDW요금': 'cdw_fee', '회원등급': 'member_grd',\n",
    "                     '차종': 'car_kind', '예약일자': 'res_day', '할인유형': 'discount_type', \n",
    "                     '총대여료(VAT포함)': 'fin_fee', '적용할인율(%)': 'fin_discount',\n",
    "                     '할인유형명': 'discount_type_nm', '적용할인명': 'applyed_discount'}\n",
    "\n",
    "res_hx = res_hx.rename(columns=res_hx_remap_cols)\n",
    "res_hx_20 = res_hx_20.rename(columns=res_hx_remap_cols)\n",
    "\n",
    "# Reservation discount history\n",
    "res_hx_discount_remap_cols = {'계약번호': 'res_num', '할인유형': 'discount_type', \n",
    "                           '할인유형명': 'discount_type_nm', '적용할인명': 'applyed_discount'}\n",
    "\n",
    "res_hx_discount = res_hx_discount.rename(columns=res_hx_discount_remap_cols)\n",
    "res_res_hx_2020 = res_hx_20.rename(columns=res_hx_discount_remap_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exception\n",
    "res_hx_20 = res_hx_20[pd.to_datetime(res_hx_20['res_day'], format='%Y-%m-%d') >= dt.datetime(2020,1,1)]\n",
    "res_hx_20_discount = res_hx_20[['res_num', 'discount_type_nm', 'applyed_discount']]\n",
    "res_hx_20 = res_hx_20.drop(columns=['member_grd', 'discount_type_nm', 'applyed_discount'], axis=1)\n",
    "res_hx = res_hx.drop(columns=['cust_kind_nm'], axis=1, errors='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_hx = pd.concat([res_hx, res_hx_20], axis=0, ignore_index=True)\n",
    "res_hx_discount = pd.concat([res_hx_discount, res_hx_20_discount], axis=0, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check NAs & Fill values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of NAs in reservation history: 39949\n"
     ]
    }
   ],
   "source": [
    "# Chkeck NA values in reservation history\n",
    "na_cnt_res = res_hx.isna().sum().sum()\n",
    "print(f'Number of NAs in reservation history: {na_cnt_res}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Chnage data types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_hx['res_day'] = pd.to_datetime(res_hx['res_day'], format='%Y-%m-%d')\n",
    "res_hx['rent_day'] = pd.to_datetime(res_hx['rent_day'], format='%Y-%m-%d')\n",
    "res_hx['return_day'] = pd.to_datetime(res_hx['return_day'], format='%Y-%m-%d')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filter dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select 1.6 Grade cars\n",
    "res_hx = res_hx[res_hx['res_model_nm'].isin(['K3', 'THE NEW K3 (G)', 'ALL NEW K3 (G)',\n",
    "                                             '아반떼 AD (G) F/L', '올 뉴 아반떼 (G)', '아반떼 AD (G)',\n",
    "                                             '쏘울 (G)', '쏘울 부스터 (G)', '더 올 뉴 벨로스터 (G)'])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Group data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Car Model group\n",
    "# SOUL 모델은 VELOSTER 모델에 포함해서 분석 (실적 데이터가 적음)\n",
    "conditions = [\n",
    "    res_hx['res_model_nm'].isin(['K3', 'THE NEW K3 (G)', 'ALL NEW K3 (G)']),\n",
    "    res_hx['res_model_nm'].isin(['아반떼 AD (G)', '아반떼 AD (G) F/L', '올 뉴 아반떼 (G)']),\n",
    "    res_hx['res_model_nm'].isin(['더 올 뉴 벨로스터 (G)', '쏘울 (G)', '쏘울 부스터 (G)'])    \n",
    "]\n",
    "values = ['K3', 'AVANTE', 'VELOSTER']\n",
    "\n",
    "res_hx['res_model_grp'] = np.select(conditions, values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Drop columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reservation history dataset\n",
    "res_hx_drop_cols = ['res_route_nm', 'car_grd', 'res_model_nm', 'cdw_fee_kind_nm', 'tot_fee']\n",
    "res_hx = res_hx.drop(columns=res_hx_drop_cols, axis=1, errors='ignore')\n",
    "\n",
    "# Reservation discount history dataset\n",
    "res_hx_discount = res_hx_discount[['res_num', 'discount_type_nm', 'applyed_discount']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reorder dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_hx = res_hx.sort_values(by=['rent_day', 'res_day'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add necessary columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Caculate lead time of reservation \n",
    "res_hx['lead_time'] = res_hx['rent_day'] - res_hx['res_day']\n",
    "res_hx['lead_time'] = res_hx['lead_time'].apply(lambda x: x.days)\n",
    "\n",
    "# Conver rent/return day and time to datetime (YYYYMMDD HHMISS)\n",
    "rent_datetime = res_hx['rent_day'].apply(lambda x: dt.datetime.strftime(x, '%Y-%m-%d')) + ' ' + res_hx['rent_time']\n",
    "return_datetime = res_hx['return_day'].apply(lambda x: dt.datetime.strftime(x, '%Y-%m-%d')) + ' ' + res_hx['return_time']\n",
    "res_hx['rent_datetime'] = pd.to_datetime(rent_datetime, format='%Y-%m-%d %H:%M:%S')\n",
    "res_hx['return_datetime'] = pd.to_datetime(return_datetime, format='%Y-%m-%d %H:%M:%S')\n",
    "res_hx['rent_period'] = res_hx['return_datetime'] - res_hx['rent_datetime']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make reservation counts on each day\n",
    "res_cnt_on_rent_day = res_hx[['rent_day', 'res_num']].groupby(by=['rent_day']).count()\n",
    "res_cnt_on_rent_day = res_cnt_on_rent_day.rename(columns={'res_num': 'res_cnt'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rechange data type of reservation\n",
    "res_hx['res_day'] = pd.to_datetime(res_hx['res_day'], format='%Y-%m-%d')\n",
    "res_hx['rent_day'] = pd.to_datetime(res_hx['rent_day'], format='%Y-%m-%d')\n",
    "res_hx['return_day'] = pd.to_datetime(res_hx['return_day'], format='%Y-%m-%d')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discount History"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Discount dataset\n",
    "discount_type = pd.read_csv(discount_type_path, delimiter='\\t', dtype={'kind': str, 'discount': int})\n",
    "discount_season = pd.read_csv(discount_season_path, delimiter='\\t',\n",
    "                                 dtype={'YYYYMMDD': int, 'M3': float, 'M2': float, 'M1': float, 'W4': float,\n",
    "                                        'W3': float, 'W2': float, 'W1': float, 'weekend': float, 'season': float, \n",
    "                                        'event': float})\n",
    "discount_policy = pd.read_csv(discount_policy_path, delimiter='\\t', \n",
    "                  dtype={'date': int, '20160701': float, '20160901': float, '20170101': float,\n",
    "                        '20180101': float, '20180209': float, '20180312': float, '20180314': float,\n",
    "                        '20180327': float, '20180417': float, '20180508': float, '20180628': float,\n",
    "                        '20180720': float, '20180726': float, '20180808': float, '20180817': float,\n",
    "                        '20180903': float, '20180910': float, '20180914': float, '20181010': float,\n",
    "                        '20181029': float, '20181129': float, '20181218': float, '20190111': float,\n",
    "                        '20190122': float, '20190225': float, '20190313': float, '20190322': float,\n",
    "                        '20190405': float, '20190409': float, '20190419': float, '20190510': float,\n",
    "                        '20190521': float, '20190528': float, '20190612': float, '20190701': float,\n",
    "                        '20190718': float, '20190723': float, '20190816': float, '20190823': float,\n",
    "                        '20190909': float, '20191029': float})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check NAs & Fill values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of NAs in discount type: 0\n",
      "Number of NAs in discount season: 3774\n",
      "Number of NAs in discount policy: 62155\n"
     ]
    }
   ],
   "source": [
    "# Check NAs\n",
    "# discount type\n",
    "na_cnt_discount = discount_type.isna().sum().sum()\n",
    "# Discount Lead Time\n",
    "na_cnt_discount_season = discount_season.isna().sum().sum()\n",
    "# Discount Policy\n",
    "na_cnt_discount_policy = discount_policy.isna().sum().sum()\n",
    "\n",
    "print(f'Number of NAs in discount type: {na_cnt_discount}')\n",
    "print(f'Number of NAs in discount season: {na_cnt_discount_season}')\n",
    "print(f'Number of NAs in discount policy: {na_cnt_discount_policy}')\n",
    "\n",
    "# Fiil values\n",
    "# Discount Poilicy\n",
    "discount_policy['date'] = pd.to_datetime(discount_policy['date'], format='%Y%m%d')\n",
    "discount_policy = discount_policy.set_index('date')\n",
    "discount_policy = discount_policy.fillna(method='ffill', axis=1)\n",
    "discount_policy = discount_policy.fillna(method='bfill', axis=1)\n",
    "discount_policy = discount_policy.reset_index()\n",
    "\n",
    "# Discount Season\n",
    "discount_season = discount_season.fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Chnage Data Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "discount_season['rent_day'] = pd.to_datetime(discount_season['YYYYMMDD'], format='%Y%m%d')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filter dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Except 2017 year\n",
    "discount_season = discount_season[discount_season['YYYYMMDD'] >= 20180101]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Drop columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "discount_season = discount_season[['rent_day', 'weekend', 'season', 'event']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reorder dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "discount_season = discount_season.sort_values(by=['rent_day'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Apply discount policy history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Discount Policy\n",
    "discount_idx_to_lead_time = {idx: day for idx, day in enumerate(discount_policy.columns)}\n",
    "discount_lead_time_to_idx = {day: idx for idx, day in enumerate(discount_policy.columns)}\n",
    "\n",
    "res_day_to_idx = {}\n",
    "res_col = discount_policy.columns[1:]\n",
    "for i in range(len(res_col)-1):\n",
    "    date_range_temp = pd.date_range(start=res_col[i], end=res_col[i+1])\n",
    "    date_range_temp = date_range_temp[:-1]\n",
    "    for date in date_range_temp:\n",
    "        res_day_to_idx[date] = i\n",
    "        \n",
    "date_range_last = pd.date_range(start=res_col[-1], end=discount_policy['date'].iloc[-1].strftime('%Y%m%d'))\n",
    "for date in date_range_last:\n",
    "    res_day_to_idx[date] = len(res_col) - 1\n",
    "    \n",
    "rent_day_to_idx = {date: idx for idx, date in enumerate(discount_policy['date'])}\n",
    "discount_idx_col = [(rent_day_to_idx[idx], res_day_to_idx[col]) for idx, col in zip(res_hx['rent_day'], res_hx['res_day'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_hx['res_discount'] = [discount_policy.iloc[idx, col] for idx, col in discount_idx_col]\n",
    "res_hx['diff_discount'] = res_hx['fin_discount'] - res_hx['res_discount']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Correct naming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Discount type\n",
    "discount_type['discount'] = discount_type['discount'] - 60    # 비회원 기준 할인율\n",
    "discount_type_dict = {kind: discount for kind, discount in zip(discount_type['kind'], discount_type['discount'])}\n",
    "\n",
    "# Correct naming on 2018/2019 years \n",
    "discount_type_dict[\"인터넷일반회원\"] = discount_type_dict[\"일반회원\"]\n",
    "discount_type_dict[\"인터넷골드회원\"] = discount_type_dict[\"골드회원\"]\n",
    "discount_type_dict[\"인터넷더블골드회원\"] = discount_type_dict[\"더블골드회원\"]\n",
    "discount_type_dict[\"예약어플(골드회원)\"] = discount_type_dict[\"골드회원\"]\n",
    "discount_type_dict[\"예약어플(더블골드회원)\"] = discount_type_dict[\"더블골드회원\"]\n",
    "discount_type_dict['비씨카드(주)'] = discount_type_dict['BC카드']\n",
    "discount_type_dict[\"신한Top's club\"] = discount_type_dict[\"신한 Top's club\"]\n",
    "discount_type_dict[\"현대카드오토케어\"] = discount_type_dict[\"현대카드 오토케어\"]\n",
    "discount_type_dict[\"롯데그룹 임직원\"] = discount_type_dict[\"롯데그룹임직원\"]\n",
    "discount_type_dict[\"금호그룹 임직원\"] = discount_type_dict[\"금호그룹임직원\"]\n",
    "discount_type_dict[\"KT그룹 임직원\"] = discount_type_dict[\"KT그룹임직원\"]\n",
    "discount_type_dict[\"VIP거래처(제주)\"] = discount_type_dict[\"VIP거래처\"]\n",
    "discount_type_dict[\"에어부산(FLY.FUN)\"] = discount_type_dict[\"에어부산(FLY & FUN)\"]\n",
    "discount_type_dict[\"삼성전자 서비스\"] = discount_type_dict[\"삼성전자서비스\"]\n",
    "discount_type_dict[\"하나카드 VIP 컨시어지\"] = discount_type_dict[\"하나카드VIP컨시어지\"]\n",
    "discount_type_dict[\"BC-kt 제휴카드\"] = discount_type_dict[\"BC-kt제휴카드\"]\n",
    "discount_type_dict[\"인천공항홈페이지\"] = discount_type_dict[\"인천공항 홈페이지\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Integrate History Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Merge all of  history dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reservation + Reservation Counts on each day\n",
    "res_hx = pd.merge(res_hx, res_cnt_on_rent_day, how='left', on='rent_day', \n",
    "                  left_index=True, right_index=False)\n",
    "\n",
    "# Reservaion + Discount season\n",
    "res_hx = pd.merge(res_hx, discount_season, how='left', on='rent_day', \n",
    "                  left_index=True, right_index=False)\n",
    "\n",
    "# Reservation + Reservation discount\n",
    "res_hx = pd.merge(res_hx, res_hx_discount, how='left', on='res_num', \n",
    "               left_index=True, right_index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Drop unnecessary columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_hx = res_hx.drop(columns=['rent_time', 'return_time', 'rent_datetime', 'return_datetime'],\n",
    "                    axis=1, errors='ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add seasonality column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "conditions = [\n",
    "    (res_hx['season'] != 0),\n",
    "    (res_hx['event'] == 1),\n",
    "    ((res_hx['weekend'] == 1) & (res_hx['weekend'] == 1) & (res_hx['weekend'] == 1)),\n",
    "    (res_hx['season'] + res_hx['event'] + res_hx['weekend'] == 0)]\n",
    "values =[4, 3, 2, 1]\n",
    "\n",
    "res_hx['seasonality'] = np.select(conditions, values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add lead time vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "conditions = [\n",
    "    ((res_hx['lead_time'] >= 7 * 4) & (res_hx['lead_time'] < 7 * 5)),\n",
    "    ((res_hx['lead_time'] >= 7 * 5) & (res_hx['lead_time'] < 7 * 6)),\n",
    "    ((res_hx['lead_time'] >= 7 * 6) & (res_hx['lead_time'] < 7 * 7)),\n",
    "    ((res_hx['lead_time'] >= 7 * 7) & (res_hx['lead_time'] < 7 * 8)),\n",
    "    ((res_hx['lead_time'] >= 7 * 8) & (res_hx['lead_time'] < 7 * 9)),\n",
    "    ((res_hx['lead_time'] >= 7 * 9) & (res_hx['lead_time'] < 7 * 10)),\n",
    "    ((res_hx['lead_time'] >= 7 * 10) & (res_hx['lead_time'] < 7 * 11)),\n",
    "    ((res_hx['lead_time'] >= 7 * 11) & (res_hx['lead_time'] < 7 * 12)),\n",
    "    (res_hx['lead_time'] >= 7 * 12)]\n",
    "values = np.arange(28, 28 + 9, 1)\n",
    "\n",
    "res_hx['lead_time_vec'] = np.select(conditions, values)\n",
    "res_hx['lead_time_vec'] = np.where(res_hx['lead_time_vec'] <= 28, \n",
    "                                   res_hx['lead_time'].values,\n",
    "                                   res_hx['lead_time_vec'].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert time format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_hx['rent_period'] = res_hx['rent_period'].apply(lambda x: x.days* 24 + x.seconds//3600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Update discount rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_hx['discount_add'] = res_hx['applyed_discount'].apply(lambda x: discount_type_dict.get(x, 0))\n",
    "res_hx['res_discount'] += res_hx['discount_add']\n",
    "res_hx = res_hx.drop(columns=['discount_add'], errors='ignore')\n",
    "\n",
    "# 할인율 상한선 적용: 90%\n",
    "res_hx['res_discount'] = np.where(res_hx['res_discount'] > 90, 90, res_hx['res_discount'])    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate reservation fee"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_hx['res_fee'] = res_hx['fin_fee'] - res_hx['cdw_fee']\n",
    "res_hx['res_fee'] = res_hx['res_fee'] / (1 - res_hx['fin_discount']/100)\n",
    "res_hx['res_fee'] = res_hx['res_fee'] * (1 - res_hx['res_discount']/100) + res_hx['cdw_fee']\n",
    "res_hx['res_fee'] = res_hx['res_fee'].apply(lambda x: round(x, 0))\n",
    "res_hx = res_hx.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_hx.loc[res_hx['res_fee'] == np.inf, 'res_fee'] = ((res_hx['fin_fee'] - res_hx['cdw_fee']) * res_hx['res_discount']) + res_hx['cdw_fee']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove exception"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 예약 가격이 없는 경우 제외\n",
    "res_hx = res_hx[res_hx['res_fee'].notnull()]\n",
    "\n",
    "# 쿠폰할인 제외\n",
    "res_hx = res_hx[res_hx['discount_type_nm'] != '쿠폰할인']\n",
    "\n",
    "# 임의할인: 할인율 100% 제외\n",
    "res_hx = res_hx[res_hx['fin_fee'] != 0] \n",
    "\n",
    "# 업체할인: 비씨카드 이외 할인 제외\n",
    "res_hx = res_hx[(res_hx['discount_type_nm'] != '업체할인') | (res_hx['applyed_discount'] == '비씨카드(주)')] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save preprocessing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seasonality = res_hx[['rent_day', 'seasonality']].drop_duplicates().reset_index(drop=True)\n",
    "seasonality.to_csv(os.path.join(save_path, 'data', 'seasonality.csv'), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Current Reservation Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recent reservation dataset\n",
    "res_data_type = {'예약경로': int,'예약경로명': str, '계약번호': int,  '고객구분': int, \n",
    "                 '고객구분명': str, '총 청구액(VAT포함)': str, '예약모델': str, '예약모델명': str, \n",
    "                 '차급': str, '대여일': str, '대여시간': str, '반납일': str,'반납시간': str, \n",
    "                 '대여기간(일)': int, '대여기간(시간)': int, 'CDW요금': str, '할인유형': str,\n",
    "                 '할인유형명': str, '적용할인명': str, '회원등급': str, '구매목적': str,\n",
    "                 '생성일': str,   '차종': str}\n",
    "\n",
    "res_curr = pd.read_csv(res_201111_path, delimiter='\\t', dtype=res_data_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Rename columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reservation history\n",
    "res_remap_cols ={\n",
    "     '예약경로': 'res_route', '예약경로명': 'res_route_nm', '계약번호': 'res_num', \n",
    "     '고객구분': 'cust_kind', '고객구분명': 'cust_kind_nm', '총 청구액(VAT포함)': 'tot_fee', \n",
    "     '예약모델': 'res_model', '예약모델명': 'res_model_nm', '차급': 'car_grd', \n",
    "     '대여일': 'rent_day', '대여시간': 'rent_time', '반납일': 'return_day', '반납시간': 'return_time',\n",
    "     '대여기간(일)': 'rent_period_day', '대여기간(시간)': 'rent_period_time',\n",
    "     'CDW요금': 'cdw_fee', '할인유형': 'discount_type', '할인유형명': 'discount_type_nm',\n",
    "     '적용할인명': 'applyed_discount', '적용할인율(%)': 'discount_rate', '회원등급': 'member_grd',\n",
    "     '구매목적': 'sale_purpose','생성일': 'res_day', '차종': 'car_kind'}\n",
    "res_curr = res_curr.rename(columns=res_remap_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "res_curr.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Drop columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_drop_col = ['res_route', 'res_route_nm', 'cust_kind', 'cust_kind_nm', 'tot_fee',\n",
    "                'res_model', 'car_grd', 'rent_time', 'return_day', 'return_time', 'rent_period_day', \n",
    "                'rent_period_time', 'cdw_fee', 'discount_type', 'discount_type_nm', 'sale_purpose', \n",
    "                'applyed_discount', 'discount_rate', 'member_grd', 'sale_purpose', 'car_kind']\n",
    "res_curr = res_curr.drop(columns=res_drop_col, errors='ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Chnage date types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_curr['rent_day'] = pd.to_datetime(res_curr['rent_day'], format='%Y-%m-%d')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filter data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter only 1.6 grade car group\n",
    "res_curr = res_curr[res_curr['res_model_nm'].isin([\n",
    "    'K3', 'THE NEW K3 (G)', 'ALL NEW K3 (G)',\n",
    "    '아반떼 AD (G)', '아반떼 AD (G) F/L', '올 뉴 아반떼 (G)',\n",
    "    '더 올 뉴 벨로스터 (G)', '쏘울 (G)', '쏘울 부스터 (G)'\n",
    "])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Group data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Car Model group\n",
    "# SOUL 모델은 VELOSTER 모델에 포함해서 분석 (실적 데이터가 적음)\n",
    "conditions = [\n",
    "    res_curr['res_model_nm'].isin(['K3', 'THE NEW K3 (G)', 'ALL NEW K3 (G)']),\n",
    "    res_curr['res_model_nm'].isin(['아반떼 AD (G)', '아반떼 AD (G) F/L', '올 뉴 아반떼 (G)']),\n",
    "    res_curr['res_model_nm'].isin(['더 올 뉴 벨로스터 (G)', '쏘울 (G)', '쏘울 부스터 (G)'])    \n",
    "]\n",
    "values = ['K3', 'AVANTE', 'VELOSTER']\n",
    "\n",
    "res_curr['res_model_grp'] = np.select(conditions, values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_curr = res_curr.drop(columns=['res_model_nm'], errors='ignore')\n",
    "res_curr = res_curr.sort_values(by=['rent_day', 'res_day'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_cnt = res_curr.groupby(by=['rent_day', 'res_model_grp', 'res_day']).count()\n",
    "res_cum_cnt = res_cnt.groupby(by=['rent_day', 'res_model_grp']).cumsum()\n",
    "res_cum_cnt = res_cum_cnt.rename(columns={'res_num': 'res_cum_cnt'})\n",
    "# res_cum_cnt = res_cum_cnt.reset_index(level=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Make dataset dictionary "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_curr_rent_day = res_cum_cnt.index.get_level_values('rent_day').unique()\n",
    "res_cum_cnt_dict = {rent_day: res_cum_cnt.loc[rent_day] for rent_day in res_curr_rent_day}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = os.path.join('..', 'result', 'data', 'reservation')\n",
    "f = open(os.path.join(save_path, 'res_cum_cnt_dict.pickle'), 'wb')\n",
    "pickle.dump(res_cum_cnt_dict, f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cum. Reservation Counts on model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reservation history dataset\n",
    "res_hx_17_19_path = os.path.join('..', 'input', 'res_hx_17_19.csv')\n",
    "res_hx_20_path = os.path.join('..', 'input', 'res_hx_20.csv')\n",
    "res_hx_data_type = {'계약번호': int, '예약경로명': str, '예약모델명': str, \n",
    "                    '대여일': str, '대여시간': str, '반납일': str, '반납시간': str, \n",
    "                    '차량대여요금(VAT포함)': int, 'CDW요금': int, '총대여료(VAT포함)': int, \n",
    "                    '적용할인율(%) ': int, '생성일': str}\n",
    "\n",
    "res_hx_17_19 = pd.read_csv(res_hx_17_19_path, delimiter='\\t', dtype=res_hx_data_type)\n",
    "res_hx_20 = pd.read_csv(res_hx_20_path, delimiter='\\t', dtype=res_hx_data_type)\n",
    "\n",
    "# Seasonality setting dataset\n",
    "seasonality = pd.read_csv(os.path.join('..', 'result', 'data', 'seasonality.csv'))\n",
    "seasonality['rent_day'] = pd.to_datetime(seasonality['rent_day'], format='%Y-%m-%d')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Rename columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reservation history\n",
    "res_hx_remap_cols = {\n",
    "    '계약번호': 'res_num', '예약경로명': 'res_route_nm', '예약모델명': 'res_model_nm', \n",
    "    '대여일': 'rent_day', '대여시간': 'rent_time', '반납일': 'return_day', '반납시간': 'return_time', \n",
    "    '차량대여요금(VAT포함)': 'car_rent_fee', 'CDW요금': 'cdw_fee', '총대여료(VAT포함)': 'tot_fee', \n",
    "    '적용할인율(%)': 'discount', '생성일':'res_day'}\n",
    "\n",
    "res_hx_17_19 = res_hx_17_19.rename(columns=res_hx_remap_cols)\n",
    "res_hx_20 = res_hx_20.rename(columns=res_hx_remap_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Change data types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_hx_17_19['rent_day'] = pd.to_datetime(res_hx_17_19['rent_day'], format='%Y-%m-%d') \n",
    "res_hx_20['rent_day'] = pd.to_datetime(res_hx_20['rent_day'], format='%Y-%m-%d')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filter periods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Except 2017 year\n",
    "res_hx_18_19 = res_hx_17_19[(res_hx_17_19['rent_day'] >= dt.datetime(2018, 1, 1)) &\n",
    "                            (res_hx_17_19['rent_day'] < dt.datetime(2020, 1, 1))]\n",
    "res_hx_20 = res_hx_20[res_hx_20['rent_day'] >= dt.datetime(2020,1,1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Merge data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_hx = pd.concat([res_hx_18_19, res_hx_20])\n",
    "res_hx = res_hx.sort_values(by=['rent_day', 'res_day'])\n",
    "res_hx = res_hx.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_hx = pd.merge(res_hx, seasonality, how='left', on='rent_day', left_index=True, right_index=False)\n",
    "res_hx = res_hx.reset_index(drop=True)\n",
    "res_hx['seasonality'] = res_hx['seasonality'].fillna(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filter car grades (1.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select 1.6 Grade cars\n",
    "filter_cars = ['K3', 'THE NEW K3 (G)', 'ALL NEW K3 (G)',\n",
    "               '아반떼 AD (G)', '아반떼 AD (G) F/L', '올 뉴 아반떼 (G)',\n",
    "               '쏘울 (G)', '쏘울 부스터 (G)', '더 올 뉴 벨로스터 (G)']\n",
    "res_hx = res_hx[res_hx['res_model_nm'].isin(filter_cars)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Group data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Car Model group\n",
    "# SOUL 모델은 VELOSTER 모델에 포함해서 분석 (실적 데이터가 적음)\n",
    "avante = ['아반떼 AD (G)', '아반떼 AD (G) F/L', '올 뉴 아반떼 (G)']\n",
    "k3 = ['K3', 'THE NEW K3 (G)', 'ALL NEW K3 (G)']\n",
    "veloster_soul = ['더 올 뉴 벨로스터 (G)', '쏘울 (G)', '쏘울 부스터 (G)']\n",
    "\n",
    "conditions = [\n",
    "    res_hx['res_model_nm'].isin(avante),\n",
    "    res_hx['res_model_nm'].isin(k3),\n",
    "    res_hx['res_model_nm'].isin(veloster_soul)]\n",
    "\n",
    "values = ['AVANTE', 'K3', 'VELOSTER']\n",
    "res_hx['res_model_grp'] = np.select(conditions, values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Change data types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_hx['res_day'] = pd.to_datetime(res_hx['res_day'], format='%Y-%m-%d')\n",
    "res_hx['discount'] = res_hx['discount'].astype(int)\n",
    "res_hx['seasonality'] = res_hx['seasonality'].astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Make additional columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add lead time\n",
    "res_hx['lead_time'] = res_hx['rent_day'] - res_hx['res_day']\n",
    "res_hx['lead_time'] = res_hx['lead_time'].apply(lambda x: x.days)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add lead time vector\n",
    "conditions = [\n",
    "    ((res_hx['lead_time'] >= 7 * 4) & (res_hx['lead_time'] < 7 * 5)),\n",
    "    ((res_hx['lead_time'] >= 7 * 5) & (res_hx['lead_time'] < 7 * 6)),\n",
    "    ((res_hx['lead_time'] >= 7 * 6) & (res_hx['lead_time'] < 7 * 7)),\n",
    "    ((res_hx['lead_time'] >= 7 * 7) & (res_hx['lead_time'] < 7 * 8)),\n",
    "    ((res_hx['lead_time'] >= 7 * 8) & (res_hx['lead_time'] < 7 * 9)),\n",
    "    ((res_hx['lead_time'] >= 7 * 9) & (res_hx['lead_time'] < 7 * 10)),\n",
    "    ((res_hx['lead_time'] >= 7 * 10) & (res_hx['lead_time'] < 7 * 11)),\n",
    "    ((res_hx['lead_time'] >= 7 * 11) & (res_hx['lead_time'] < 7 * 12)),\n",
    "    (res_hx['lead_time'] >= 7 * 12)]\n",
    "\n",
    "values = np.arange(28, 28 + 9, 1)\n",
    "\n",
    "res_hx['lead_time_vec'] = np.select(conditions, values)\n",
    "res_hx['lead_time_vec'] = np.where(res_hx['lead_time_vec'] <= 28, \n",
    "                                   res_hx['lead_time'].values,\n",
    "                                   res_hx['lead_time_vec'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_hx['lead_time'] = res_hx['lead_time'] * -1\n",
    "res_hx['lead_time_vec'] = res_hx['lead_time_vec'] * -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save reservation history dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_hx = res_hx.sort_values(by=['rent_day', 'res_day'])\n",
    "res_hx.to_csv(os.path.join('..', 'result', 'data', 'model_2', 'res_history_18_20.csv'), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Drop unnecessary columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "hx_drop_col = ['res_route_nm', 'res_model_nm', 'rent_time', 'return_day', 'return_time', \n",
    "               'car_rent_fee', 'cdw_fee', 'tot_fee']\n",
    "res_hx = res_hx.drop(columns=hx_drop_col, errors='ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Group by rent and reservation day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_cnt = res_hx.groupby(by=['rent_day', 'res_model_grp', 'res_day']).count()['lead_time_vec']\n",
    "res_cum_cnt = res_cnt.groupby(by=['rent_day', 'res_model_grp']).cumsum()\n",
    "\n",
    "res_dscnt = res_hx.groupby(by=['rent_day', 'res_model_grp', 'res_day']).sum()['discount']\n",
    "res_cum_dscnt = res_dscnt.groupby(by=['rent_day', 'res_model_grp']).cumsum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_cum = pd.DataFrame({'cum_dscnt': res_cum_dscnt,\n",
    "                        'cum_cnt': res_cum_cnt},\n",
    "                        index=res_cum_dscnt.index)\n",
    "res_cum['cum_dscnt_mean'] = res_cum['cum_dscnt'] / res_cum['cum_cnt']\n",
    "res_cum['cum_dscnt_mean'] = res_cum['cum_dscnt_mean'].apply(lambda x: round(x, 2))\n",
    "res_cum = res_cum.reset_index(level=(0, 1, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_hx_car = pd.merge(res_hx, res_cum, how='left', on=['rent_day', 'res_model_grp', 'res_day'],\n",
    "                      left_index=True, right_index=False)\n",
    "res_hx_car = res_hx_car.sort_values(by=['rent_day', 'res_model_grp', 'res_day'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop unnecessary columns\n",
    "res_hx_car = res_hx_car.drop(columns=['res_num', 'res_day', 'lead_time', 'rent_day'\n",
    "                                      'discount', 'cum_dscnt'], errors='ignore')\n",
    "res_hx_car = res_hx_car.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Re-group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_hx_car_grp = res_hx_car.groupby(by=['res_model_grp', 'seasonality', 'lead_time_vec']).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Divde into each car model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_2_av = res_hx_car_grp.loc['AVANTE']\n",
    "model_2_k3 = res_hx_car_grp.loc['K3']\n",
    "model_2_vl = res_hx_car_grp.loc['VELOSTER']\n",
    "\n",
    "model_2_av = model_2_av.reset_index(level=(0, 1))\n",
    "model_2_k3 = model_2_k3.reset_index(level=(0, 1))\n",
    "model_2_vl = model_2_vl.reset_index(level=(0, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = os.path.join('..', 'result', 'data', 'model_2')\n",
    "model_2_av.to_csv(os.path.join(save_path, 'model_2_av.csv'), index=False)\n",
    "model_2_k3.to_csv(os.path.join(save_path, 'model_2_k3.csv'), index=False)\n",
    "model_2_vl.to_csv(os.path.join(save_path, 'model_2_vl.csv'), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 1: Predict Jeju Visitors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "jeju_visit = pd.read_csv(os.path.join('..', 'input', 'jeju_visit_daily.csv'), delimiter='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Filter the periods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interval: 1 year\n",
    "start_date = 20191101\n",
    "end_date = 20201031\n",
    "\n",
    "jeju_visit = jeju_visit[(jeju_visit['date'] >= start_date) & (jeju_visit['date'] <= end_date)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Split the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "# String date convert to datetime\n",
    "jeju_visit['date'] = pd.to_datetime(jeju_visit['date'], format='%Y%m%d')\n",
    "jeju_visit = jeju_visit.set_index('date')\n",
    "\n",
    "# Split dataset to domestic and foreign\n",
    "jeju_dom = jeju_visit[['domestic']]\n",
    "jeju_for = jeju_visit[['foreign']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define Models & Validation Method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Define time series models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ar_model(data, config: dict, pred_steps=0):\n",
    "    \"\"\"\n",
    "    :param data: times series data\n",
    "    :param lags: the number of lags\n",
    "    :param trend: the trend to include in the model\n",
    "            n: No Trend\n",
    "            c: Constant Only\n",
    "            t: Time Trend Only\n",
    "            ct: Constant and time trend\n",
    "    :param seasonal: Flag indicating whether to include seasonal dummies in the model    \n",
    "    :param pred_step: prediction steps\n",
    "    \"\"\"\n",
    "    model = AutoReg(endog=data, lags=config['lags'], trend=config['trend'])\n",
    "    model_fit = model.fit()\n",
    "    yhat = model_fit.predict(start=len(data), end=len(data) + pred_steps-1)\n",
    "    \n",
    "    return yhat\n",
    "    \n",
    "def arima_model(data, config: dict, pred_steps=0):\n",
    "    \"\"\"\n",
    "    :param data: time series data\n",
    "    :param order: (p, d, q)\n",
    "            p: Trend autoregression order\n",
    "            d: Trend difference order\n",
    "            q: Trend moving average order\n",
    "    :param trend: the trend to include in the model\n",
    "            n: No Trend\n",
    "            c: Constant\n",
    "            t: Trend\n",
    "            ct: Constant and Trend\n",
    "    :param freq: frequency of the time series (‘B’, ‘D’, ‘W’, ‘M’, ‘A’, ‘Q)\n",
    "    \"\"\"\n",
    "    model = ARIMA(data, order=config['order'], trend=config['trend'])\n",
    "    model_fit = model.fit()\n",
    "    yhat = model_fit.predict(start=len(data), end=len(data) + pred_steps-1)\n",
    "    \n",
    "    return yhat\n",
    "    \n",
    "def holt_winters_model(data, config: dict, pred_steps=0):\n",
    "    \"\"\"\n",
    "    :param data: time series data\n",
    "    :param trend - type of trend component\n",
    "        ('add', 'mul', 'additive', 'multiplicative')\n",
    "    :param damped_trend - should the trend component be damped\n",
    "    :param seasonal - Type of seasonal component\n",
    "            ('add', 'mul', 'additive', 'multiplicative', None)\n",
    "    :param seasonal_periods - The number of periods in a complete seasonal cycle\n",
    "    :param pred_step:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    model = ExponentialSmoothing(data, trend=config['trend'], damped_trend=config['damped_trend'])\n",
    "    model_fit = model.fit()\n",
    "    yhat = model_fit.predict(start=len(data), end=len(data) + pred_steps-1)\n",
    "    \n",
    "    return yhat\n",
    "\n",
    "time_series_model = {'ar': ar_model,\n",
    "                     'arima': arima_model,\n",
    "                     'hw': holt_winters_model}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Validation: Walk forward validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "def walk_forward_validation(model: str, data, n_test, config):\n",
    "    # split dataset\n",
    "    train, test = data[:-n_test], data[-n_test:]\n",
    "    \n",
    "    # calculate \n",
    "    yhat = time_series_model[model](data=train, config=config, pred_steps=n_test)\n",
    "    \n",
    "    # add actual observation to history for the next loop\n",
    "    error = math.sqrt(mean_squared_error(test.values, yhat))\n",
    "    \n",
    "    return error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_series_param_tune(model: str, data, n_test, param_grid: dict):\n",
    "    err_list = list()\n",
    "    for params in list(product(*list(param_grid.values()))):\n",
    "        config = dict()\n",
    "        config = {key: val for key, val in zip(list(param_grid.keys()), params)}\n",
    "        err = walk_forward_validation(model, data, n_test=n_test, config=config)\n",
    "        err_list.append((list(config.items()), round(err, 2)))\n",
    "        \n",
    "    err_list_sorted = sorted(err_list, key=lambda err: err[1])    \n",
    "        \n",
    "    return err_list_sorted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation: Walk Forward Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Set paramter grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paramter Grid\n",
    "ar_param_grid = {\n",
    "    'lags': list(np.arange(1,15,1)),\n",
    "    'trend': ['c', 't', 'ct'] \n",
    "}\n",
    "\n",
    "arima_param_grid = {}\n",
    "\n",
    "hw_param_grid = {\n",
    "    'trend': ['add', 'additive'],\n",
    "    'damped_trend': [True, False] \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Range\n",
    "n_test = 28"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Walk forward validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AR: Best paramters: [('lags', 7), ('trend', 't')], result: 6692.77\n",
      "HW: Best paramters: [('trend', 'add'), ('damped_trend', True)], result: 14461.68\n"
     ]
    }
   ],
   "source": [
    "err_dom_ar = time_series_param_tune(model='ar', data=jeju_dom, \n",
    "                                    n_test=n_test, param_grid=ar_param_grid)\n",
    "print(f'AR: Best paramters: {err_dom_ar[0][0]}, result: {err_dom_ar[0][1]}')\n",
    "\n",
    "err_dom_hw = time_series_param_tune(model='hw', data=jeju_dom, \n",
    "                                    n_test=n_test, param_grid=hw_param_grid)\n",
    "print(f'HW: Best paramters: {err_dom_hw[0][0]}, result: {err_dom_hw[0][1]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AR: Best paramters: [('lags', 1), ('trend', 't')], result: 27.83\n",
      "HW: Best paramters: [('trend', 'add'), ('damped_trend', True)], result: 148.46\n"
     ]
    }
   ],
   "source": [
    "err_for_ar = time_series_param_tune(model='ar', data=jeju_for, \n",
    "                                    n_test=n_test, param_grid=ar_param_grid)\n",
    "print(f'AR: Best paramters: {err_for_ar[0][0]}, result: {err_for_ar[0][1]}')\n",
    "\n",
    "err_for_hw = time_series_param_tune(model='hw', data=jeju_for, \n",
    "                                    n_test=n_test, param_grid=hw_param_grid)\n",
    "print(f'HW: Best paramters: {err_for_hw[0][0]}, result: {err_for_hw[0][1]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Save best parameter grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = os.path.join('..', 'result', 'model', 'model_1')\n",
    "\n",
    "m1_dom_ar_best_params = {key:val for key, val in err_dom_ar[0][0]}\n",
    "f = open(os.path.join(save_path, 'm1_dom_ar_best_params.pickle'), 'wb')\n",
    "pickle.dump(m1_dom_ar_best_params, f)\n",
    "f.close()\n",
    "\n",
    "m1_for_ar_best_params = {key:val for key, val in err_for_ar[0][0]}\n",
    "f = open(os.path.join(save_path, 'm1_for_ar_best_params.pickle'), 'wb')\n",
    "pickle.dump(m1_for_ar_best_params, f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load paramters and fit the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Load best parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = os.path.join('..', 'result', 'model', 'model_1')\n",
    "f = open(os.path.join(save_path, 'm1_dom_ar_best_params.pickle'), 'rb')\n",
    "m1_dom_ar_best_params = pickle.load(f)\n",
    "f.close()\n",
    "\n",
    "f = open(os.path.join(save_path, 'm1_dom_ar_best_params.pickle'), 'rb')\n",
    "m1_for_ar_best_params = pickle.load(f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Fit the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction periods(3 month)\n",
    "pred_steps = 88 + 31"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Domestic\n",
    "model_dom = AutoReg(endog=jeju_dom, \n",
    "                    lags=m1_dom_ar_best_params['lags'],\n",
    "                    trend=m1_dom_ar_best_params['trend'])\n",
    "model_dom_fit = model_dom.fit()\n",
    "pred_dom = model_dom_fit.predict(start=len(jeju_dom), end=len(jeju_dom) + pred_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Foreign\n",
    "model_for = AutoReg(endog=jeju_for, \n",
    "                    lags=m1_for_ar_best_params['lags'],\n",
    "                    trend=m1_for_ar_best_params['trend'])\n",
    "model_for_fit = model_for.fit()\n",
    "pred_for = model_for_fit.predict(start=len(jeju_for), end=len(jeju_for) + pred_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Predict domestic and foreign"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [],
   "source": [
    "visit_19_12 = jeju_visit.loc[dt.datetime(2019,12,1):dt.datetime(2019,12,31)]['total']\n",
    "visit_20_01 = jeju_visit.loc[dt.datetime(2020,1,1):dt.datetime(2020,1,31)]['total']\n",
    "visit_20_02 = jeju_visit.loc[dt.datetime(2020,2,1):dt.datetime(2020,2,28)]['total']\n",
    "\n",
    "pred_tot = pred_dom + pred_for\n",
    "\n",
    "pred_20_12 = pred_tot.loc[dt.datetime(2020,12,1):dt.datetime(2020,12,31)]\n",
    "pred_21_01 = pred_tot.loc[dt.datetime(2021,1,1):dt.datetime(2021,1,31)]\n",
    "pred_21_02 = pred_tot.loc[dt.datetime(2021,2,1):dt.datetime(2021,2,28)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Get rate of change on jeju visitors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [],
   "source": [
    "chg_rate_20_12 = round((pred_20_12.mean() / visit_19_12.mean() -1), 2)\n",
    "chg_rate_21_01 = round((pred_21_01.mean() / visit_20_01.mean() -1), 2)\n",
    "chg_rate_21_02 = round((pred_21_02.mean() / visit_20_02.mean() -1), 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 2: Predict Cum. Res. Counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Make training dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_path = os.path.join('..', 'result', 'data', 'model_2')\n",
    "\n",
    "# Load each model\n",
    "model_2_av = pd.read_csv(os.path.join(root_path, 'model_2_av.csv')) \n",
    "model_2_k3 = pd.read_csv(os.path.join(root_path, 'model_2_k3.csv')) \n",
    "model_2_vl = pd.read_csv(os.path.join(root_path, 'model_2_vl.csv')) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Divide into input and output dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Avante\n",
    "m2_av_x = model_2_av.drop(columns=['cum_cnt'])\n",
    "m2_av_y = model_2_av['cum_cnt']\n",
    "\n",
    "# K3\n",
    "m2_k3_x = model_2_k3.drop(columns=['cum_cnt'])\n",
    "m2_k3_y = model_2_k3['cum_cnt']\n",
    "\n",
    "# Veloster\n",
    "m2_vl_x = model_2_k3.drop(columns=['cum_cnt'])\n",
    "m2_vl_y = model_2_k3['cum_cnt']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Split train and test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_state = 2020\n",
    "\n",
    "m2_av_x_train, m2_av_x_test, m2_av_y_train, m2_av_y_test = train_test_split(m2_av_x, m2_av_y, \n",
    "                                                                            test_size=0.2, \n",
    "                                                                            random_state=random_state, \n",
    "                                                                            shuffle=True)\n",
    "m2_k3_x_train, m2_k3_x_test, m2_k3_y_train, m2_k3_y_test = train_test_split(m2_k3_x, m2_k3_y, \n",
    "                                                                            test_size=0.2, \n",
    "                                                                            random_state=random_state, \n",
    "                                                                            shuffle=True)\n",
    "m2_vl_x_train, m2_vl_x_test, m2_vl_y_train, m2_vl_y_test = train_test_split(m2_vl_x, m2_vl_y, \n",
    "                                                                            test_size=0.2, \n",
    "                                                                            random_state=random_state, \n",
    "                                                                            shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pre-test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Pre-test parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparamter of Support Vector Machine \n",
    "param_svm = {\n",
    "    'kernel': 'rbf',    # 'linear', 'poly', 'rbf'\n",
    "    'tol':1e-3, \n",
    "    'max_iter': -1\n",
    "}\n",
    "\n",
    "# Hyperparamter of Extra-tree model\n",
    "param_extra = {\n",
    "    'n_estimators': 100,\n",
    "    'criterion': 'mse',\n",
    "    'max_depth': None,\n",
    "    'min_samples_split': 2,\n",
    "    'max_features': 'auto',\n",
    "    'random_state': 2020\n",
    "}\n",
    "\n",
    "# Hyperparamter of XGboost model\n",
    "xgb_param = {'max_depth': 6, 'eta': 1, 'objective': 'reg:squarederror'}\n",
    "xgb_param['nthread'] = 4\n",
    "xgb_param['eval_metric'] = 'rmse'    # 'rmse', 'auc', 'logloss', 'map'\n",
    "num_round = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Model: SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model\n",
    "regr_svm = svm.SVR(kernel=param_svm['kernel'],\n",
    "                   tol=param_svm['tol'], \n",
    "                   max_iter=param_svm['max_iter'])\n",
    "# Fit model\n",
    "regr_svm.fit(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_svm = regr_svm.predict(X_test_scaled)    # Prediction on test dataset\n",
    "rmse_svm = mean_squared_error(y_test, pred_svm)    # Calculate RMSE\n",
    "pred_svm_df = pd.DataFrame({'real': y_test['cum_cnt'].values, 'prediction': pred_svm})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Model: Extra-trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model\n",
    "regr_extr = ExtraTreesRegressor(n_estimators=param_extra['n_estimators'], \n",
    "                                criterion=param_extra['criterion'],\n",
    "                                max_depth=param_extra['max_depth'],\n",
    "                                min_samples_split=param_extra['min_samples_split'],\n",
    "                                max_features=param_extra['max_features'],\n",
    "                                random_state=param_extra['random_state'])\n",
    "# Fit model\n",
    "regr_extr.fit(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_extr = regr_extr.predict(X_test_scaled)    # Prediction on test dataset\n",
    "rmse_extr = mean_squared_error(y_test, pred_extr)    # Calculate RMSE\n",
    "pred_extr_df = pd.DataFrame({'real':  y_test['cum_cnt'].values, 'prediction': pred_extr})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Model: XGboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make dataset\n",
    "dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "dtest = xgb.DMatrix(X_test, label=y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regr_xgb = xgb.train(xgb_param, dtrain, num_round)\n",
    "regr_xgb.save_model(os.path.join(save_path, 'model', 'xgboost.model'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_xgb = regr_xgb.predict(dtest)\n",
    "rmse_xgb = mean_squared_error(y_test, pred_xgb)\n",
    "pred_xgv_df = pd.DataFrame({'real':  y_test['cum_cnt'].values, 'prediction': pred_xgb}, index=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Compare results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse_svm, rmse_extr, rmse_xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_svm_df = pred_svm_df.sort_values(by=['real'])\n",
    "pred_svm_df = pred_svm_df.reset_index(drop=True)\n",
    "\n",
    "pred_extr_df = pred_extr_df.sort_values(by=['real'])\n",
    "pred_extr_df = pred_extr_df.reset_index(drop=True)\n",
    "\n",
    "pred_xgv_df = pred_xgv_df.sort_values(by=['real'])\n",
    "pred_xgv_df = pred_xgv_df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot SVM model result\n",
    "fig, axes = plt.subplots(1, 1)\n",
    "pred_svm_df['real'].plot.line(figsize=(15,6), linewidth=1.1, \n",
    "                              color='firebrick', label='Real', ax=axes)\n",
    "pred_svm_df['prediction'].plot.line(figsize=(15,6),linewidth=0.9, alpha=0.4,\n",
    "                                     color='coral', label='Prediction', ax=axes)\n",
    "axes.legend()\n",
    "plt.title('Result: SVM')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Extra-trees model result\n",
    "fig, axes = plt.subplots(1, 1)\n",
    "pred_extr_df['real'].plot.line(figsize=(15,6), linewidth=1.1, \n",
    "                              color='firebrick',  label='Real', ax=axes)\n",
    "pred_extr_df['prediction'].plot.line(figsize=(15,6),linewidth=0.9, alpha=0.4,\n",
    "                                     color='coral', label='Prediction', ax=axes)\n",
    "axes.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot XGboost model result\n",
    "fig, axes = plt.subplots(1, 1)\n",
    "pred_xgv_df['real'].plot.line(figsize=(15,6), linewidth=1.1, \n",
    "                              color='firebrick', label='Real', ax=axes)\n",
    "pred_xgv_df['prediction'].plot.line(figsize=(15,6),linewidth=0.9, alpha=0.4,\n",
    "                                     color='coral', label='Prediction', ax=axes)\n",
    "axes.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training: Extra-trees Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Set model & paramter grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Training models\n",
    "regressors = {}\n",
    "regressors.update({\"Extra Trees Regressor\": ExtraTreesRegressor()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameter grid\n",
    "param_grids = {}\n",
    "param_grids.update({\"Extra Trees Regressor\": {\n",
    "    'n_estimators': list(np.arange(100, 500, 100)),\n",
    "    'criterion': ['mse'],\n",
    "    'min_samples_split': list(np.arange(2, 6, 1)),   # minimum number of samples required to split inner node \n",
    "    'min_samples_leaf': list(np.arange(1, 6, 1)),    # have the effect of smoothing the model\n",
    "    'max_features': ['auto']\n",
    "}})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Grid search cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_best_hyper_param(x, y, regr, scoring='neg_root_mean_squared_error'):\n",
    "    # Select regressor algorithm\n",
    "    regressor = regressors[regr]\n",
    "    \n",
    "    # Define parameter grid\n",
    "    param_grid = param_grids[regr]\n",
    "    \n",
    "    # Initialize Grid Search object\n",
    "    gscv = GridSearchCV(estimator=regressor, param_grid=param_grid,\n",
    "                        scoring=scoring, n_jobs=1, cv=5, verbose=1)\n",
    "    \n",
    "    # Fit gscv\n",
    "    print(f'Tuning {regr}')\n",
    "    gscv.fit(x, y)\n",
    "    \n",
    "    # Get best paramters and score\n",
    "    best_params = gscv.best_params_\n",
    "    best_score = gscv.best_score_\n",
    "    \n",
    "    # Update regressor paramters\n",
    "    regressor.set_params(**best_params)\n",
    "    \n",
    "    return regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best paramter grid for Avante\n",
    "m2_av_extr_best = get_best_hyper_param(x=m2_av_x_train, y=m2_av_y_train.to_numpy(), \n",
    "                                       regr='Extra Trees Regressor')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best paramter grid for K3\n",
    "m2_k3_extr_best = get_best_hyper_param(x=m2_k3_x_train, y=m2_k3_y_train.to_numpy(), \n",
    "                                       regr='Extra Trees Regressor')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best paramter grid for Veloster\n",
    "m2_vl_extr_best = get_best_hyper_param(x=m2_vl_x_train, y=m2_vl_y_train.to_numpy(), \n",
    "                                       regr='Extra Trees Regressor')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Save best parameter grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save best paramters of extra-trees regressor\n",
    "\n",
    "# Avante\n",
    "m2_av_extr_best_params = m2_av_extr_best.get_params()\n",
    "f = open(os.path.join(save_path, 'model', 'model_2', \"m2_av_extr_best_params.pickle\"), \"wb\")\n",
    "pickle.dump(m2_av_extr_best_params, f)\n",
    "f.close()\n",
    "\n",
    "# K3\n",
    "m2_k3_extr_best_params= m2_k3_extr_best.get_params()\n",
    "f = open(os.path.join(save_path, 'model', 'model_2', \"m2_k3_extr_best_params.pickle\"), \"wb\")\n",
    "pickle.dump(m2_k3_extr_best_params, f)\n",
    "f.close()\n",
    "\n",
    "# Veloster\n",
    "m2_vl_extr_best_params= m2_vl_extr_best.get_params()\n",
    "f = open(os.path.join(save_path, 'model', 'model_2', \"m2_vl_extr_best_params.pickle\"), \"wb\")\n",
    "pickle.dump(m2_vl_extr_best_params, f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load paramters and fit the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Load best hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Avante\n",
    "load_path = os.path.join('..', 'result', 'model', 'model_2')\n",
    "f = open(os.path.join(load_path, \"m2_av_extr_best_params.pickle\"), \"rb\")\n",
    "m2_av_extr_best_params = pickle.load(f)\n",
    "f.close()\n",
    "\n",
    "# K3\n",
    "f = open(os.path.join(load_path, \"m2_k3_extr_best_params.pickle\"), \"rb\")\n",
    "m2_k3_extr_best_params = pickle.load(f)\n",
    "f.close()\n",
    "\n",
    "# Veloster\n",
    "f = open(os.path.join(load_path, \"m2_vl_extr_best_params.pickle\"), \"rb\")\n",
    "m2_vl_extr_best_params = pickle.load(f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Fit the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ExtraTreesRegressor(n_estimators=300)"
      ]
     },
     "execution_count": 322,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Avante\n",
    "extr_av = ExtraTreesRegressor(**m2_av_extr_best_params)\n",
    "extr_av.fit(m2_av_x, m2_av_y)\n",
    "\n",
    "# K3\n",
    "extr_k3 = ExtraTreesRegressor(**m2_k3_extr_best_params)\n",
    "extr_k3.fit(m2_k3_x, m2_k3_y)\n",
    "\n",
    "# Veloster\n",
    "extr_vl = ExtraTreesRegressor(**m2_vl_extr_best_params)\n",
    "extr_vl.fit(m2_vl_x, m2_vl_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Set initial variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting\n",
    "load_path = os.path.join('..', 'result', 'data')\n",
    "seasonality = pd.read_csv(os.path.join(load_path, 'seasonality.csv'))\n",
    "seasonality['rent_day'] = pd.to_datetime(seasonality['rent_day'], format='%Y-%m-%d')\n",
    "day_to_season = {day: season for day, season in zip(seasonality['rent_day'], \n",
    "                                                    seasonality['seasonality'])}\n",
    "# Initial discount rate for each season\n",
    "season_init_discount = {1: 70,   # Weekdays\n",
    "                        2: 60,   # Weekends\n",
    "                        3: 45,   # Holidays\n",
    "                        4: 10}    # Peak Season\n",
    "\n",
    "# Target rent day\n",
    "predict_day = '2020.9.27'\n",
    "\n",
    "# Lead Time Setting\n",
    "lead_time = np.arange(-83, 1, 1)\n",
    "lead_time_vec = np.arange(-36, 1, 1)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Make dataframe of initial setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get season value and initial discount rate\n",
    "predict_datetime = dt.datetime(*list(map(int, predict_day.split('.'))))    # Convert to datetime\n",
    "season = day_to_season[predict_datetime]    # Get seasonality on the day\n",
    "init_discount = season_init_discount[season]\n",
    "\n",
    "# Initial capacity of each car model\n",
    "init_capa_av = 40\n",
    "init_capa_k3 = 35\n",
    "init_capa_vl = 35\n",
    "\n",
    "# Inintial settting to dataframe\n",
    "pred_input = pd.DataFrame({'season': season, 'lead_time': lead_time_vec, 'discount': init_discount})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Predict cum. reservation counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction\n",
    "extr_pred_av = extr_av.predict(pred_input)    # Avante\n",
    "extr_pred_k3 = extr_k3.predict(pred_input)    # K3\n",
    "extr_pred_vl = extr_vl.predict(pred_input)    # Veloster\n",
    "\n",
    "# Correct decimal point\n",
    "vfun = np.vectorize(lambda x: round(x, 2))\n",
    "extr_pred_av = vfun(extr_pred_av)\n",
    "extr_pred_k3 = vfun(extr_pred_k3)\n",
    "extr_pred_vl = vfun(extr_pred_vl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [],
   "source": [
    "lt_to_lt_vec = {-1 * i : (((i//7) + 24) * -1 if i > 28 else i * -1) for i in range(0, 84, 1)}\n",
    "\n",
    "lt_vec_to_av = {lt_vec: av for lt_vec, av in zip(lead_time_vec, extr_pred_av)}\n",
    "lt_vec_to_k3 = {lt_vec: k3 for lt_vec, k3 in zip(lead_time_vec, extr_pred_k3)}\n",
    "lt_vec_to_vl = {lt_vec: vl for lt_vec, vl in zip(lead_time_vec, extr_pred_vl)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make result dataframe\n",
    "exp_res = pd.DataFrame({\n",
    "        'date': [predict_datetime - dt.timedelta(days=int(i*-1)) for i in lead_time],\n",
    "        'lead_time': lead_time,\n",
    "        'curr_discount': init_discount,\n",
    "        'capa_av': init_capa_av,\n",
    "        'exp_cum_av': [lt_vec_to_av[lt_to_lt_vec[i]] for i in lead_time],\n",
    "        'exp_res_rate_av': [lt_vec_to_av[lt_to_lt_vec[i]]/init_capa_av for i in lead_time],\n",
    "        'capa_k3': init_capa_k3,\n",
    "        'exp_cum_k3': [lt_vec_to_k3[lt_to_lt_vec[i]] for i in lead_time],\n",
    "        'exp_res_rate_k3': [lt_vec_to_k3[lt_to_lt_vec[i]]/init_capa_k3 for i in lead_time],\n",
    "        'capa_vl': init_capa_vl,\n",
    "        'exp_cum_vl': [lt_vec_to_vl[lt_to_lt_vec[i]] for i in lead_time],\n",
    "        'exp_res_rate_vl': [lt_vec_to_vl[lt_to_lt_vec[i]]/init_capa_vl for i in lead_time]\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Save result dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path =  os.path.join('..', 'result', 'data', 'recommend')\n",
    "# exp_res['date'] = exp_res['date'].apply(lambda x: x.strftime('%Y.%m.%d'))\n",
    "exp_res.to_csv(os.path.join(save_path, 'exp_cum_res_cnt(' + predict_day + ').csv'), index=False)\n",
    "exp_res.T.to_csv(os.path.join(save_path, 'exp_cum_res_cnt_T(' + predict_day + ').csv'), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 3: Recommend Best Price"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define Discount Recommendation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [],
   "source": [
    "# theta2 : Season 및 lead time 별 할인율 조정 전략에 관한 기준 예약률 산출\n",
    "# 과거 season 및 lead time 별 예약률 트렌드 데이터를 기준으로 산출\n",
    "def calc_tehta2(curr_exp_cnt: int, exp_res_cnt: int, exp_capa: int):\n",
    "    return round((curr_exp_cnt + exp_res_cnt) / exp_capa, 2)\n",
    "\n",
    "def dscnt_rec_fun(r: float, exp_res_rate: float, d: float):\n",
    "    \"\"\"\n",
    "    Customized Exponential function\n",
    "    r: Current Reservation Rate\n",
    "    d: Exp Demand Change Rate \n",
    "    ex_res_rate: Exp. Reservation Rate\n",
    "    \"\"\"\n",
    "    # Hyperparamters (need to tune)\n",
    "    theta1 = 1.2       # ratio of increasing / decreasing magnitude : discount\n",
    "    theta2 = 1         # ratio of increasing / decreasing magnitude : demand\n",
    "    phi_low = 1.7    # 1 < phi_low < phi_high < 2\n",
    "    phi_high = 1.4   # 1 < phi_low < phi_high < 2\n",
    "\n",
    "    if d > 0:\n",
    "        y = 1 - theta1 * (r ** (phi_high ** (-1 * (r * theta2 * d))) - exp_res_rate)\n",
    "    else:\n",
    "        y = 1 - theta1 * (r ** (phi_low ** (-1 * ((1 - r) * theta2 * d))) - exp_res_rate)\n",
    "        \n",
    "    return y\n",
    "\n",
    "def calc_sug_disc_rate():\n",
    "    pass##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "PermissionError",
     "evalue": "[Errno 13] Permission denied: '..\\\\result\\\\data\\\\recommend'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPermissionError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-344-70476eb38f29>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mpredict_day\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'2020.9.27'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mload_path\u001b[0m \u001b[1;33m=\u001b[0m  \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'..'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'result'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'data'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'recommend'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mexp_res\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mload_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'exp_cum_res_cnt('\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mpredict_day\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m').csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\python37\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[0;32m    684\u001b[0m     )\n\u001b[0;32m    685\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 686\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    687\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    688\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python37\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    450\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    451\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 452\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    453\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    454\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python37\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    944\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    945\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 946\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    947\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    948\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python37\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1187\u001b[0m                     \u001b[1;34m'are \"c\", \"python\", or \"python-fwf\")'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1188\u001b[0m                 )\n\u001b[1;32m-> 1189\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mklass\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1190\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1191\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_failover_to_python\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python37\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, **kwds)\u001b[0m\n\u001b[0;32m   2385\u001b[0m             \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencoding\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2386\u001b[0m             \u001b[0mcompression\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompression\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2387\u001b[1;33m             \u001b[0mmemory_map\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmemory_map\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2388\u001b[0m         )\n\u001b[0;32m   2389\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhandles\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python37\\lib\\site-packages\\pandas\\io\\common.py\u001b[0m in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors)\u001b[0m\n\u001b[0;32m    494\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mis_text\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    495\u001b[0m             \u001b[1;31m# No explicit encoding\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 496\u001b[1;33m             \u001b[0mf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath_or_buf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"replace\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnewline\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    497\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    498\u001b[0m             \u001b[1;31m# Binary mode\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mPermissionError\u001b[0m: [Errno 13] Permission denied: '..\\\\result\\\\data\\\\recommend'"
     ]
    }
   ],
   "source": [
    "predict_day = '2020.9.27'\n",
    "\n",
    "# Load Exp. Cum. reservation counts\n",
    "load_path =  os.path.join('..', 'result', 'data', 'recommend')\n",
    "exp_res = pd.read_csv(load_path, 'exp_cum_res_cnt(' + predict_day + ').csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load current reservation counts\n",
    "load_path = os.path.join('..', 'result', 'data', 'reservation')\n",
    "f = open(os.path.join(load_path, 'res_cum_cnt_dict.pickle'), 'rb')\n",
    "res_cum_cnt_dict = pickle.load(f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Change data type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_res['date'] = pd.to_datetime(exp_res['date'], format='%Y.%m.%d')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate each data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_dmd_change = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_res['rec_rate_av'] = exp_res['exp_res_rate_av'].apply(lambda x: dscnt_rec_fun(r=x, d=0, exp_res_rate=x))\n",
    "exp_res['rec_rate_k3'] = exp_res['exp_res_rate_k3'].apply(lambda x: dscnt_rec_fun(r=x, d=0, exp_res_rate=x))\n",
    "exp_res['rec_rate_vl'] = exp_res['exp_res_rate_vl'].apply(lambda x: dscnt_rec_fun(r=x, d=0, exp_res_rate=x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_res['rec_dscnt_av'] = exp_res['curr_discount'] * exp_res['rec_rate_av']\n",
    "exp_res['rec_dscnt_k3'] = exp_res['curr_discount'] * exp_res['rec_rate_k3']\n",
    "exp_res['rec_dscnt_vl'] = exp_res['curr_discount'] * exp_res['rec_rate_vl']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_res = exp_res.drop(columns=['rec_rate_av', 'rec_rate_k3', 'rec_rate_vl'], errors='ignore')\n",
    "exp_res = exp_res[['date', 'lead_time', 'curr_discount', \n",
    "                   'capa_av', 'exp_cum_av', 'exp_res_rate_av', 'rec_dscnt_av', \n",
    "                   'capa_k3', 'exp_cum_k3', 'exp_res_rate_k3', 'rec_dscnt_k3', \n",
    "                   'capa_vl', 'exp_cum_vl', 'exp_res_rate_vl', 'rec_dscnt_vl', \n",
    "                  ]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save Recommendation Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path =  os.path.join('..', 'result', 'data', 'recommend')\n",
    "exp_res.to_csv(os.path.join(save_path, 'dscnt_rec(' + predict_day + ').csv'), header=False)\n",
    "exp_res.T.to_csv(os.path.join(save_path, 'dscnt_rec_T(' + predict_day + ').csv'), header=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "349.091px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
