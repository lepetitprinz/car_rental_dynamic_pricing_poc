{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Definition\" data-toc-modified-id=\"Definition-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Definition</a></span><ul class=\"toc-item\"><li><span><a href=\"#Import-Library\" data-toc-modified-id=\"Import-Library-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Import Library</a></span></li><li><span><a href=\"#Define-File-Path\" data-toc-modified-id=\"Define-File-Path-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>Define File Path</a></span></li></ul></li><li><span><a href=\"#Data-Preprocessing\" data-toc-modified-id=\"Data-Preprocessing-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Data Preprocessing</a></span><ul class=\"toc-item\"><li><span><a href=\"#Jeju-Visitors-History\" data-toc-modified-id=\"Jeju-Visitors-History-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Jeju Visitors History</a></span><ul class=\"toc-item\"><li><span><a href=\"#Load-dataset\" data-toc-modified-id=\"Load-dataset-2.1.1\"><span class=\"toc-item-num\">2.1.1&nbsp;&nbsp;</span>Load dataset</a></span></li><li><span><a href=\"#Rename-columns\" data-toc-modified-id=\"Rename-columns-2.1.2\"><span class=\"toc-item-num\">2.1.2&nbsp;&nbsp;</span>Rename columns</a></span></li><li><span><a href=\"#Check-NAs-&amp;-Fill-values\" data-toc-modified-id=\"Check-NAs-&amp;-Fill-values-2.1.3\"><span class=\"toc-item-num\">2.1.3&nbsp;&nbsp;</span>Check NAs &amp; Fill values</a></span></li><li><span><a href=\"#Chnage-Data-Types\" data-toc-modified-id=\"Chnage-Data-Types-2.1.4\"><span class=\"toc-item-num\">2.1.4&nbsp;&nbsp;</span>Chnage Data Types</a></span></li><li><span><a href=\"#Drop-columns\" data-toc-modified-id=\"Drop-columns-2.1.5\"><span class=\"toc-item-num\">2.1.5&nbsp;&nbsp;</span>Drop columns</a></span></li><li><span><a href=\"#Reorder-dataset\" data-toc-modified-id=\"Reorder-dataset-2.1.6\"><span class=\"toc-item-num\">2.1.6&nbsp;&nbsp;</span>Reorder dataset</a></span></li></ul></li><li><span><a href=\"#Reservation-History\" data-toc-modified-id=\"Reservation-History-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>Reservation History</a></span><ul class=\"toc-item\"><li><span><a href=\"#Load-dataset\" data-toc-modified-id=\"Load-dataset-2.2.1\"><span class=\"toc-item-num\">2.2.1&nbsp;&nbsp;</span>Load dataset</a></span></li><li><span><a href=\"#Merge-dataset\" data-toc-modified-id=\"Merge-dataset-2.2.2\"><span class=\"toc-item-num\">2.2.2&nbsp;&nbsp;</span>Merge dataset</a></span></li><li><span><a href=\"#Rename-columns\" data-toc-modified-id=\"Rename-columns-2.2.3\"><span class=\"toc-item-num\">2.2.3&nbsp;&nbsp;</span>Rename columns</a></span></li><li><span><a href=\"#Check-NAs-&amp;-Fill-values\" data-toc-modified-id=\"Check-NAs-&amp;-Fill-values-2.2.4\"><span class=\"toc-item-num\">2.2.4&nbsp;&nbsp;</span>Check NAs &amp; Fill values</a></span></li><li><span><a href=\"#Chnage-data-types\" data-toc-modified-id=\"Chnage-data-types-2.2.5\"><span class=\"toc-item-num\">2.2.5&nbsp;&nbsp;</span>Chnage data types</a></span></li><li><span><a href=\"#Filter-dataset\" data-toc-modified-id=\"Filter-dataset-2.2.6\"><span class=\"toc-item-num\">2.2.6&nbsp;&nbsp;</span>Filter dataset</a></span></li><li><span><a href=\"#Group-data\" data-toc-modified-id=\"Group-data-2.2.7\"><span class=\"toc-item-num\">2.2.7&nbsp;&nbsp;</span>Group data</a></span></li><li><span><a href=\"#Drop-columns\" data-toc-modified-id=\"Drop-columns-2.2.8\"><span class=\"toc-item-num\">2.2.8&nbsp;&nbsp;</span>Drop columns</a></span></li><li><span><a href=\"#Reorder-dataset\" data-toc-modified-id=\"Reorder-dataset-2.2.9\"><span class=\"toc-item-num\">2.2.9&nbsp;&nbsp;</span>Reorder dataset</a></span></li><li><span><a href=\"#Add-necessary-columns\" data-toc-modified-id=\"Add-necessary-columns-2.2.10\"><span class=\"toc-item-num\">2.2.10&nbsp;&nbsp;</span>Add necessary columns</a></span></li></ul></li><li><span><a href=\"#Discount-History\" data-toc-modified-id=\"Discount-History-2.3\"><span class=\"toc-item-num\">2.3&nbsp;&nbsp;</span>Discount History</a></span><ul class=\"toc-item\"><li><span><a href=\"#Load-dataset\" data-toc-modified-id=\"Load-dataset-2.3.1\"><span class=\"toc-item-num\">2.3.1&nbsp;&nbsp;</span>Load dataset</a></span></li><li><span><a href=\"#Check-NAs-&amp;-Fill-values\" data-toc-modified-id=\"Check-NAs-&amp;-Fill-values-2.3.2\"><span class=\"toc-item-num\">2.3.2&nbsp;&nbsp;</span>Check NAs &amp; Fill values</a></span></li><li><span><a href=\"#Chnage-Data-Types\" data-toc-modified-id=\"Chnage-Data-Types-2.3.3\"><span class=\"toc-item-num\">2.3.3&nbsp;&nbsp;</span>Chnage Data Types</a></span></li><li><span><a href=\"#Filter-dataset\" data-toc-modified-id=\"Filter-dataset-2.3.4\"><span class=\"toc-item-num\">2.3.4&nbsp;&nbsp;</span>Filter dataset</a></span></li><li><span><a href=\"#Drop-columns\" data-toc-modified-id=\"Drop-columns-2.3.5\"><span class=\"toc-item-num\">2.3.5&nbsp;&nbsp;</span>Drop columns</a></span></li><li><span><a href=\"#Reorder-dataset\" data-toc-modified-id=\"Reorder-dataset-2.3.6\"><span class=\"toc-item-num\">2.3.6&nbsp;&nbsp;</span>Reorder dataset</a></span></li><li><span><a href=\"#Apply-discount-policy-history\" data-toc-modified-id=\"Apply-discount-policy-history-2.3.7\"><span class=\"toc-item-num\">2.3.7&nbsp;&nbsp;</span>Apply discount policy history</a></span></li><li><span><a href=\"#Correct-naming\" data-toc-modified-id=\"Correct-naming-2.3.8\"><span class=\"toc-item-num\">2.3.8&nbsp;&nbsp;</span>Correct naming</a></span></li></ul></li><li><span><a href=\"#Integrate-History-Dataset\" data-toc-modified-id=\"Integrate-History-Dataset-2.4\"><span class=\"toc-item-num\">2.4&nbsp;&nbsp;</span>Integrate History Dataset</a></span><ul class=\"toc-item\"><li><span><a href=\"#Merge-all-of--history-dataset\" data-toc-modified-id=\"Merge-all-of--history-dataset-2.4.1\"><span class=\"toc-item-num\">2.4.1&nbsp;&nbsp;</span>Merge all of  history dataset</a></span></li><li><span><a href=\"#Drop-unnecessary-columns\" data-toc-modified-id=\"Drop-unnecessary-columns-2.4.2\"><span class=\"toc-item-num\">2.4.2&nbsp;&nbsp;</span>Drop unnecessary columns</a></span></li><li><span><a href=\"#Add-seasonality-column\" data-toc-modified-id=\"Add-seasonality-column-2.4.3\"><span class=\"toc-item-num\">2.4.3&nbsp;&nbsp;</span>Add seasonality column</a></span></li><li><span><a href=\"#Add-lead-time-vector\" data-toc-modified-id=\"Add-lead-time-vector-2.4.4\"><span class=\"toc-item-num\">2.4.4&nbsp;&nbsp;</span>Add lead time vector</a></span></li><li><span><a href=\"#Convert-time-format\" data-toc-modified-id=\"Convert-time-format-2.4.5\"><span class=\"toc-item-num\">2.4.5&nbsp;&nbsp;</span>Convert time format</a></span></li><li><span><a href=\"#Update-discount-rate\" data-toc-modified-id=\"Update-discount-rate-2.4.6\"><span class=\"toc-item-num\">2.4.6&nbsp;&nbsp;</span>Update discount rate</a></span></li><li><span><a href=\"#Calculate-reservation-fee\" data-toc-modified-id=\"Calculate-reservation-fee-2.4.7\"><span class=\"toc-item-num\">2.4.7&nbsp;&nbsp;</span>Calculate reservation fee</a></span></li><li><span><a href=\"#Remove-exception\" data-toc-modified-id=\"Remove-exception-2.4.8\"><span class=\"toc-item-num\">2.4.8&nbsp;&nbsp;</span>Remove exception</a></span></li><li><span><a href=\"#Save-preprocessing-dataset\" data-toc-modified-id=\"Save-preprocessing-dataset-2.4.9\"><span class=\"toc-item-num\">2.4.9&nbsp;&nbsp;</span>Save preprocessing dataset</a></span></li></ul></li><li><span><a href=\"#History-dataset:-Res.-count\" data-toc-modified-id=\"History-dataset:-Res.-count-2.5\"><span class=\"toc-item-num\">2.5&nbsp;&nbsp;</span>History dataset: Res. count</a></span><ul class=\"toc-item\"><li><span><a href=\"#Load-dataset\" data-toc-modified-id=\"Load-dataset-2.5.1\"><span class=\"toc-item-num\">2.5.1&nbsp;&nbsp;</span>Load dataset</a></span></li><li><span><a href=\"#Rename-columns\" data-toc-modified-id=\"Rename-columns-2.5.2\"><span class=\"toc-item-num\">2.5.2&nbsp;&nbsp;</span>Rename columns</a></span></li><li><span><a href=\"#Change-data-types\" data-toc-modified-id=\"Change-data-types-2.5.3\"><span class=\"toc-item-num\">2.5.3&nbsp;&nbsp;</span>Change data types</a></span></li><li><span><a href=\"#Filter-periods\" data-toc-modified-id=\"Filter-periods-2.5.4\"><span class=\"toc-item-num\">2.5.4&nbsp;&nbsp;</span>Filter periods</a></span></li><li><span><a href=\"#Merge-dataset\" data-toc-modified-id=\"Merge-dataset-2.5.5\"><span class=\"toc-item-num\">2.5.5&nbsp;&nbsp;</span>Merge dataset</a></span></li><li><span><a href=\"#Filter-car-grades-(1.6)\" data-toc-modified-id=\"Filter-car-grades-(1.6)-2.5.6\"><span class=\"toc-item-num\">2.5.6&nbsp;&nbsp;</span>Filter car grades (1.6)</a></span></li><li><span><a href=\"#Group-data\" data-toc-modified-id=\"Group-data-2.5.7\"><span class=\"toc-item-num\">2.5.7&nbsp;&nbsp;</span>Group data</a></span></li><li><span><a href=\"#Change-data-types\" data-toc-modified-id=\"Change-data-types-2.5.8\"><span class=\"toc-item-num\">2.5.8&nbsp;&nbsp;</span>Change data types</a></span></li><li><span><a href=\"#Make-additional-columns\" data-toc-modified-id=\"Make-additional-columns-2.5.9\"><span class=\"toc-item-num\">2.5.9&nbsp;&nbsp;</span>Make additional columns</a></span></li><li><span><a href=\"#Save-reservation-history-dataset\" data-toc-modified-id=\"Save-reservation-history-dataset-2.5.10\"><span class=\"toc-item-num\">2.5.10&nbsp;&nbsp;</span>Save reservation history dataset</a></span></li><li><span><a href=\"#Drop-unnecessary-columns\" data-toc-modified-id=\"Drop-unnecessary-columns-2.5.11\"><span class=\"toc-item-num\">2.5.11&nbsp;&nbsp;</span>Drop unnecessary columns</a></span></li><li><span><a href=\"#Group-by-rent-and-reservation-day\" data-toc-modified-id=\"Group-by-rent-and-reservation-day-2.5.12\"><span class=\"toc-item-num\">2.5.12&nbsp;&nbsp;</span>Group by rent and reservation day</a></span></li><li><span><a href=\"#Merge\" data-toc-modified-id=\"Merge-2.5.13\"><span class=\"toc-item-num\">2.5.13&nbsp;&nbsp;</span>Merge</a></span></li><li><span><a href=\"#Re-group\" data-toc-modified-id=\"Re-group-2.5.14\"><span class=\"toc-item-num\">2.5.14&nbsp;&nbsp;</span>Re-group</a></span></li><li><span><a href=\"#Divde-into-each-car-model\" data-toc-modified-id=\"Divde-into-each-car-model-2.5.15\"><span class=\"toc-item-num\">2.5.15&nbsp;&nbsp;</span>Divde into each car model</a></span></li><li><span><a href=\"#Save-dataset\" data-toc-modified-id=\"Save-dataset-2.5.16\"><span class=\"toc-item-num\">2.5.16&nbsp;&nbsp;</span>Save dataset</a></span></li></ul></li><li><span><a href=\"#History-dataset:-Res.-Utilization\" data-toc-modified-id=\"History-dataset:-Res.-Utilization-2.6\"><span class=\"toc-item-num\">2.6&nbsp;&nbsp;</span>History dataset: Res. Utilization</a></span><ul class=\"toc-item\"><li><span><a href=\"#Load-Dataset\" data-toc-modified-id=\"Load-Dataset-2.6.1\"><span class=\"toc-item-num\">2.6.1&nbsp;&nbsp;</span>Load Dataset</a></span></li><li><span><a href=\"#Rename-columns\" data-toc-modified-id=\"Rename-columns-2.6.2\"><span class=\"toc-item-num\">2.6.2&nbsp;&nbsp;</span>Rename columns</a></span></li><li><span><a href=\"#Change-data-types\" data-toc-modified-id=\"Change-data-types-2.6.3\"><span class=\"toc-item-num\">2.6.3&nbsp;&nbsp;</span>Change data types</a></span></li><li><span><a href=\"#Filter-periods\" data-toc-modified-id=\"Filter-periods-2.6.4\"><span class=\"toc-item-num\">2.6.4&nbsp;&nbsp;</span>Filter periods</a></span></li><li><span><a href=\"#Merge-dataset\" data-toc-modified-id=\"Merge-dataset-2.6.5\"><span class=\"toc-item-num\">2.6.5&nbsp;&nbsp;</span>Merge dataset</a></span></li><li><span><a href=\"#Delete-variables-from-memory\" data-toc-modified-id=\"Delete-variables-from-memory-2.6.6\"><span class=\"toc-item-num\">2.6.6&nbsp;&nbsp;</span>Delete variables from memory</a></span></li><li><span><a href=\"#Filter-car-grades\" data-toc-modified-id=\"Filter-car-grades-2.6.7\"><span class=\"toc-item-num\">2.6.7&nbsp;&nbsp;</span>Filter car grades</a></span></li><li><span><a href=\"#Group-car-models\" data-toc-modified-id=\"Group-car-models-2.6.8\"><span class=\"toc-item-num\">2.6.8&nbsp;&nbsp;</span>Group car models</a></span></li><li><span><a href=\"#Change-data-types\" data-toc-modified-id=\"Change-data-types-2.6.9\"><span class=\"toc-item-num\">2.6.9&nbsp;&nbsp;</span>Change data types</a></span></li><li><span><a href=\"#Drop-unnecessary-columns\" data-toc-modified-id=\"Drop-unnecessary-columns-2.6.10\"><span class=\"toc-item-num\">2.6.10&nbsp;&nbsp;</span>Drop unnecessary columns</a></span></li><li><span><a href=\"#Make-utilization-rate-dataset\" data-toc-modified-id=\"Make-utilization-rate-dataset-2.6.11\"><span class=\"toc-item-num\">2.6.11&nbsp;&nbsp;</span>Make utilization rate dataset</a></span></li><li><span><a href=\"#Group-by-rent-&amp;-reservation-day\" data-toc-modified-id=\"Group-by-rent-&amp;-reservation-day-2.6.12\"><span class=\"toc-item-num\">2.6.12&nbsp;&nbsp;</span>Group by rent &amp; reservation day</a></span></li><li><span><a href=\"#Convert-to-dataframe\" data-toc-modified-id=\"Convert-to-dataframe-2.6.13\"><span class=\"toc-item-num\">2.6.13&nbsp;&nbsp;</span>Convert to dataframe</a></span></li><li><span><a href=\"#Add-capacity-of-model-column\" data-toc-modified-id=\"Add-capacity-of-model-column-2.6.14\"><span class=\"toc-item-num\">2.6.14&nbsp;&nbsp;</span>Add capacity of model column</a></span></li><li><span><a href=\"#Add-additional-columns\" data-toc-modified-id=\"Add-additional-columns-2.6.15\"><span class=\"toc-item-num\">2.6.15&nbsp;&nbsp;</span>Add additional columns</a></span></li><li><span><a href=\"#Re-group\" data-toc-modified-id=\"Re-group-2.6.16\"><span class=\"toc-item-num\">2.6.16&nbsp;&nbsp;</span>Re-group</a></span></li><li><span><a href=\"#Dived-into-each-car-model\" data-toc-modified-id=\"Dived-into-each-car-model-2.6.17\"><span class=\"toc-item-num\">2.6.17&nbsp;&nbsp;</span>Dived into each car model</a></span></li><li><span><a href=\"#Save-dataset\" data-toc-modified-id=\"Save-dataset-2.6.18\"><span class=\"toc-item-num\">2.6.18&nbsp;&nbsp;</span>Save dataset</a></span></li></ul></li><li><span><a href=\"#Recent-dataset:-Res.-Utilization\" data-toc-modified-id=\"Recent-dataset:-Res.-Utilization-2.7\"><span class=\"toc-item-num\">2.7&nbsp;&nbsp;</span>Recent dataset: Res. Utilization</a></span><ul class=\"toc-item\"><li><span><a href=\"#Load-dataset\" data-toc-modified-id=\"Load-dataset-2.7.1\"><span class=\"toc-item-num\">2.7.1&nbsp;&nbsp;</span>Load dataset</a></span></li><li><span><a href=\"#Rename-columns\" data-toc-modified-id=\"Rename-columns-2.7.2\"><span class=\"toc-item-num\">2.7.2&nbsp;&nbsp;</span>Rename columns</a></span></li><li><span><a href=\"#Drop-columns\" data-toc-modified-id=\"Drop-columns-2.7.3\"><span class=\"toc-item-num\">2.7.3&nbsp;&nbsp;</span>Drop columns</a></span></li><li><span><a href=\"#Filter-car-grades\" data-toc-modified-id=\"Filter-car-grades-2.7.4\"><span class=\"toc-item-num\">2.7.4&nbsp;&nbsp;</span>Filter car grades</a></span></li><li><span><a href=\"#Group-car-models\" data-toc-modified-id=\"Group-car-models-2.7.5\"><span class=\"toc-item-num\">2.7.5&nbsp;&nbsp;</span>Group car models</a></span></li><li><span><a href=\"#Change-data-types\" data-toc-modified-id=\"Change-data-types-2.7.6\"><span class=\"toc-item-num\">2.7.6&nbsp;&nbsp;</span>Change data types</a></span></li><li><span><a href=\"#Make-utilization-rate-dataset\" data-toc-modified-id=\"Make-utilization-rate-dataset-2.7.7\"><span class=\"toc-item-num\">2.7.7&nbsp;&nbsp;</span>Make utilization rate dataset</a></span></li><li><span><a href=\"#Group-by-rent-&amp;-reservation-day\" data-toc-modified-id=\"Group-by-rent-&amp;-reservation-day-2.7.8\"><span class=\"toc-item-num\">2.7.8&nbsp;&nbsp;</span>Group by rent &amp; reservation day</a></span></li><li><span><a href=\"#Convert-to-dataframe\" data-toc-modified-id=\"Convert-to-dataframe-2.7.9\"><span class=\"toc-item-num\">2.7.9&nbsp;&nbsp;</span>Convert to dataframe</a></span></li><li><span><a href=\"#Add-capacity-of-model-columns\" data-toc-modified-id=\"Add-capacity-of-model-columns-2.7.10\"><span class=\"toc-item-num\">2.7.10&nbsp;&nbsp;</span>Add capacity of model columns</a></span></li><li><span><a href=\"#Add-additional-columns\" data-toc-modified-id=\"Add-additional-columns-2.7.11\"><span class=\"toc-item-num\">2.7.11&nbsp;&nbsp;</span>Add additional columns</a></span></li><li><span><a href=\"#Drop-unncessary-columns\" data-toc-modified-id=\"Drop-unncessary-columns-2.7.12\"><span class=\"toc-item-num\">2.7.12&nbsp;&nbsp;</span>Drop unncessary columns</a></span></li><li><span><a href=\"#Change-decimal-points\" data-toc-modified-id=\"Change-decimal-points-2.7.13\"><span class=\"toc-item-num\">2.7.13&nbsp;&nbsp;</span>Change decimal points</a></span></li><li><span><a href=\"#Sava-result-dataset\" data-toc-modified-id=\"Sava-result-dataset-2.7.14\"><span class=\"toc-item-num\">2.7.14&nbsp;&nbsp;</span>Sava result dataset</a></span></li></ul></li><li><span><a href=\"#Recent-dataset:-Res.-Count\" data-toc-modified-id=\"Recent-dataset:-Res.-Count-2.8\"><span class=\"toc-item-num\">2.8&nbsp;&nbsp;</span>Recent dataset: Res. Count</a></span><ul class=\"toc-item\"><li><span><a href=\"#Load-dataset\" data-toc-modified-id=\"Load-dataset-2.8.1\"><span class=\"toc-item-num\">2.8.1&nbsp;&nbsp;</span>Load dataset</a></span></li><li><span><a href=\"#Rename-columns\" data-toc-modified-id=\"Rename-columns-2.8.2\"><span class=\"toc-item-num\">2.8.2&nbsp;&nbsp;</span>Rename columns</a></span></li><li><span><a href=\"#Drop-unnecessary-columns\" data-toc-modified-id=\"Drop-unnecessary-columns-2.8.3\"><span class=\"toc-item-num\">2.8.3&nbsp;&nbsp;</span>Drop unnecessary columns</a></span></li><li><span><a href=\"#Change-data-types\" data-toc-modified-id=\"Change-data-types-2.8.4\"><span class=\"toc-item-num\">2.8.4&nbsp;&nbsp;</span>Change data types</a></span></li><li><span><a href=\"#Filter-dataset\" data-toc-modified-id=\"Filter-dataset-2.8.5\"><span class=\"toc-item-num\">2.8.5&nbsp;&nbsp;</span>Filter dataset</a></span></li><li><span><a href=\"#Group-data\" data-toc-modified-id=\"Group-data-2.8.6\"><span class=\"toc-item-num\">2.8.6&nbsp;&nbsp;</span>Group data</a></span></li><li><span><a href=\"#Add-additional-columns\" data-toc-modified-id=\"Add-additional-columns-2.8.7\"><span class=\"toc-item-num\">2.8.7&nbsp;&nbsp;</span>Add additional columns</a></span></li><li><span><a href=\"#Save-result-dataset\" data-toc-modified-id=\"Save-result-dataset-2.8.8\"><span class=\"toc-item-num\">2.8.8&nbsp;&nbsp;</span>Save result dataset</a></span></li></ul></li></ul></li><li><span><a href=\"#Model\" data-toc-modified-id=\"Model-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Model</a></span><ul class=\"toc-item\"><li><span><a href=\"#Model-1:-Predict-Jeju-Visitors\" data-toc-modified-id=\"Model-1:-Predict-Jeju-Visitors-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>Model 1: Predict Jeju Visitors</a></span><ul class=\"toc-item\"><li><span><a href=\"#Load-dataset\" data-toc-modified-id=\"Load-dataset-3.1.1\"><span class=\"toc-item-num\">3.1.1&nbsp;&nbsp;</span>Load dataset</a></span></li><li><span><a href=\"#Preprocessing\" data-toc-modified-id=\"Preprocessing-3.1.2\"><span class=\"toc-item-num\">3.1.2&nbsp;&nbsp;</span>Preprocessing</a></span><ul class=\"toc-item\"><li><span><a href=\"#Filter-the-periods\" data-toc-modified-id=\"Filter-the-periods-3.1.2.1\"><span class=\"toc-item-num\">3.1.2.1&nbsp;&nbsp;</span>Filter the periods</a></span></li><li><span><a href=\"#Split-the-dataset\" data-toc-modified-id=\"Split-the-dataset-3.1.2.2\"><span class=\"toc-item-num\">3.1.2.2&nbsp;&nbsp;</span>Split the dataset</a></span></li></ul></li><li><span><a href=\"#Define-Models-&amp;-Validation-Method\" data-toc-modified-id=\"Define-Models-&amp;-Validation-Method-3.1.3\"><span class=\"toc-item-num\">3.1.3&nbsp;&nbsp;</span>Define Models &amp; Validation Method</a></span><ul class=\"toc-item\"><li><span><a href=\"#Define-time-series-models\" data-toc-modified-id=\"Define-time-series-models-3.1.3.1\"><span class=\"toc-item-num\">3.1.3.1&nbsp;&nbsp;</span>Define time series models</a></span></li><li><span><a href=\"#Validation:-Walk-forward-validation\" data-toc-modified-id=\"Validation:-Walk-forward-validation-3.1.3.2\"><span class=\"toc-item-num\">3.1.3.2&nbsp;&nbsp;</span>Validation: Walk forward validation</a></span></li></ul></li><li><span><a href=\"#Evaluation:-Walk-Forward-Validation\" data-toc-modified-id=\"Evaluation:-Walk-Forward-Validation-3.1.4\"><span class=\"toc-item-num\">3.1.4&nbsp;&nbsp;</span>Evaluation: Walk Forward Validation</a></span><ul class=\"toc-item\"><li><span><a href=\"#Set-paramter-grid\" data-toc-modified-id=\"Set-paramter-grid-3.1.4.1\"><span class=\"toc-item-num\">3.1.4.1&nbsp;&nbsp;</span>Set paramter grid</a></span></li><li><span><a href=\"#Walk-forward-validation\" data-toc-modified-id=\"Walk-forward-validation-3.1.4.2\"><span class=\"toc-item-num\">3.1.4.2&nbsp;&nbsp;</span>Walk forward validation</a></span></li><li><span><a href=\"#Save-best-parameter-grid\" data-toc-modified-id=\"Save-best-parameter-grid-3.1.4.3\"><span class=\"toc-item-num\">3.1.4.3&nbsp;&nbsp;</span>Save best parameter grid</a></span></li></ul></li><li><span><a href=\"#Load-paramters-and-fit-the-model\" data-toc-modified-id=\"Load-paramters-and-fit-the-model-3.1.5\"><span class=\"toc-item-num\">3.1.5&nbsp;&nbsp;</span>Load paramters and fit the model</a></span><ul class=\"toc-item\"><li><span><a href=\"#Load-best-parameters\" data-toc-modified-id=\"Load-best-parameters-3.1.5.1\"><span class=\"toc-item-num\">3.1.5.1&nbsp;&nbsp;</span>Load best parameters</a></span></li><li><span><a href=\"#Fit-the-model\" data-toc-modified-id=\"Fit-the-model-3.1.5.2\"><span class=\"toc-item-num\">3.1.5.2&nbsp;&nbsp;</span>Fit the model</a></span></li></ul></li><li><span><a href=\"#Prediction\" data-toc-modified-id=\"Prediction-3.1.6\"><span class=\"toc-item-num\">3.1.6&nbsp;&nbsp;</span>Prediction</a></span><ul class=\"toc-item\"><li><span><a href=\"#Predict-domestic-and-foreign\" data-toc-modified-id=\"Predict-domestic-and-foreign-3.1.6.1\"><span class=\"toc-item-num\">3.1.6.1&nbsp;&nbsp;</span>Predict domestic and foreign</a></span></li><li><span><a href=\"#Get-rate-of-change-on-jeju-visitors\" data-toc-modified-id=\"Get-rate-of-change-on-jeju-visitors-3.1.6.2\"><span class=\"toc-item-num\">3.1.6.2&nbsp;&nbsp;</span>Get rate of change on jeju visitors</a></span></li><li><span><a href=\"#Save-Prediction-result\" data-toc-modified-id=\"Save-Prediction-result-3.1.6.3\"><span class=\"toc-item-num\">3.1.6.3&nbsp;&nbsp;</span>Save Prediction result</a></span></li></ul></li></ul></li><li><span><a href=\"#Model-2:-Predict-Cum.-Res.-Counts-(Training)\" data-toc-modified-id=\"Model-2:-Predict-Cum.-Res.-Counts-(Training)-3.2\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>Model 2: Predict Cum. Res. Counts (Training)</a></span><ul class=\"toc-item\"><li><span><a href=\"#Make-training-dataset\" data-toc-modified-id=\"Make-training-dataset-3.2.1\"><span class=\"toc-item-num\">3.2.1&nbsp;&nbsp;</span>Make training dataset</a></span><ul class=\"toc-item\"><li><span><a href=\"#Load-dataset\" data-toc-modified-id=\"Load-dataset-3.2.1.1\"><span class=\"toc-item-num\">3.2.1.1&nbsp;&nbsp;</span>Load dataset</a></span></li><li><span><a href=\"#Divide-into-input-and-output\" data-toc-modified-id=\"Divide-into-input-and-output-3.2.1.2\"><span class=\"toc-item-num\">3.2.1.2&nbsp;&nbsp;</span>Divide into input and output</a></span></li><li><span><a href=\"#Split-train-and-test-dataset\" data-toc-modified-id=\"Split-train-and-test-dataset-3.2.1.3\"><span class=\"toc-item-num\">3.2.1.3&nbsp;&nbsp;</span>Split train and test dataset</a></span></li></ul></li><li><span><a href=\"#Pre-test\" data-toc-modified-id=\"Pre-test-3.2.2\"><span class=\"toc-item-num\">3.2.2&nbsp;&nbsp;</span>Pre-test</a></span><ul class=\"toc-item\"><li><span><a href=\"#Pre-test-parameters\" data-toc-modified-id=\"Pre-test-parameters-3.2.2.1\"><span class=\"toc-item-num\">3.2.2.1&nbsp;&nbsp;</span>Pre-test parameters</a></span></li><li><span><a href=\"#Model:-SVM\" data-toc-modified-id=\"Model:-SVM-3.2.2.2\"><span class=\"toc-item-num\">3.2.2.2&nbsp;&nbsp;</span>Model: SVM</a></span></li><li><span><a href=\"#Model:-Extra-trees\" data-toc-modified-id=\"Model:-Extra-trees-3.2.2.3\"><span class=\"toc-item-num\">3.2.2.3&nbsp;&nbsp;</span>Model: Extra-trees</a></span></li><li><span><a href=\"#Model:-XGboost\" data-toc-modified-id=\"Model:-XGboost-3.2.2.4\"><span class=\"toc-item-num\">3.2.2.4&nbsp;&nbsp;</span>Model: XGboost</a></span></li><li><span><a href=\"#Compare-results\" data-toc-modified-id=\"Compare-results-3.2.2.5\"><span class=\"toc-item-num\">3.2.2.5&nbsp;&nbsp;</span>Compare results</a></span></li></ul></li><li><span><a href=\"#Training:-Extra-trees-Model\" data-toc-modified-id=\"Training:-Extra-trees-Model-3.2.3\"><span class=\"toc-item-num\">3.2.3&nbsp;&nbsp;</span>Training: Extra-trees Model</a></span><ul class=\"toc-item\"><li><span><a href=\"#Set-model-&amp;-paramter-grid\" data-toc-modified-id=\"Set-model-&amp;-paramter-grid-3.2.3.1\"><span class=\"toc-item-num\">3.2.3.1&nbsp;&nbsp;</span>Set model &amp; paramter grid</a></span></li><li><span><a href=\"#Grid-search-cross-validation\" data-toc-modified-id=\"Grid-search-cross-validation-3.2.3.2\"><span class=\"toc-item-num\">3.2.3.2&nbsp;&nbsp;</span>Grid search cross validation</a></span></li><li><span><a href=\"#Save-best-parameter-grid\" data-toc-modified-id=\"Save-best-parameter-grid-3.2.3.3\"><span class=\"toc-item-num\">3.2.3.3&nbsp;&nbsp;</span>Save best parameter grid</a></span></li></ul></li></ul></li><li><span><a href=\"#Model-2:-Predict-Cum.-Res.-Counts-(Prediction)\" data-toc-modified-id=\"Model-2:-Predict-Cum.-Res.-Counts-(Prediction)-3.3\"><span class=\"toc-item-num\">3.3&nbsp;&nbsp;</span>Model 2: Predict Cum. Res. Counts (Prediction)</a></span><ul class=\"toc-item\"><li><span><a href=\"#Load-dataset-&amp;-parameters\" data-toc-modified-id=\"Load-dataset-&amp;-parameters-3.3.1\"><span class=\"toc-item-num\">3.3.1&nbsp;&nbsp;</span>Load dataset &amp; parameters</a></span><ul class=\"toc-item\"><li><span><a href=\"#Load-dataset\" data-toc-modified-id=\"Load-dataset-3.3.1.1\"><span class=\"toc-item-num\">3.3.1.1&nbsp;&nbsp;</span>Load dataset</a></span></li><li><span><a href=\"#Divide-into-input-and-output\" data-toc-modified-id=\"Divide-into-input-and-output-3.3.1.2\"><span class=\"toc-item-num\">3.3.1.2&nbsp;&nbsp;</span>Divide into input and output</a></span></li><li><span><a href=\"#Load-best-hyperparameters\" data-toc-modified-id=\"Load-best-hyperparameters-3.3.1.3\"><span class=\"toc-item-num\">3.3.1.3&nbsp;&nbsp;</span>Load best hyperparameters</a></span></li></ul></li><li><span><a href=\"#Fit-the-model\" data-toc-modified-id=\"Fit-the-model-3.3.2\"><span class=\"toc-item-num\">3.3.2&nbsp;&nbsp;</span>Fit the model</a></span></li><li><span><a href=\"#Prediction\" data-toc-modified-id=\"Prediction-3.3.3\"><span class=\"toc-item-num\">3.3.3&nbsp;&nbsp;</span>Prediction</a></span><ul class=\"toc-item\"><li><span><a href=\"#Set-initial-variables\" data-toc-modified-id=\"Set-initial-variables-3.3.3.1\"><span class=\"toc-item-num\">3.3.3.1&nbsp;&nbsp;</span>Set initial variables</a></span></li><li><span><a href=\"#Set-prediction-day\" data-toc-modified-id=\"Set-prediction-day-3.3.3.2\"><span class=\"toc-item-num\">3.3.3.2&nbsp;&nbsp;</span>Set prediction day</a></span></li><li><span><a href=\"#Make-dataframe-of-initial-setting\" data-toc-modified-id=\"Make-dataframe-of-initial-setting-3.3.3.3\"><span class=\"toc-item-num\">3.3.3.3&nbsp;&nbsp;</span>Make dataframe of initial setting</a></span></li><li><span><a href=\"#Predict-res.-count-and-utilization\" data-toc-modified-id=\"Predict-res.-count-and-utilization-3.3.3.4\"><span class=\"toc-item-num\">3.3.3.4&nbsp;&nbsp;</span>Predict res. count and utilization</a></span></li><li><span><a href=\"#Save-result-dataset\" data-toc-modified-id=\"Save-result-dataset-3.3.3.5\"><span class=\"toc-item-num\">3.3.3.5&nbsp;&nbsp;</span>Save result dataset</a></span></li></ul></li></ul></li><li><span><a href=\"#Model-3:-Recommend-Best-Price\" data-toc-modified-id=\"Model-3:-Recommend-Best-Price-3.4\"><span class=\"toc-item-num\">3.4&nbsp;&nbsp;</span>Model 3: Recommend Best Price</a></span><ul class=\"toc-item\"><li><span><a href=\"#Define-Discount-Recommendation-Function\" data-toc-modified-id=\"Define-Discount-Recommendation-Function-3.4.1\"><span class=\"toc-item-num\">3.4.1&nbsp;&nbsp;</span>Define Discount Recommendation Function</a></span></li><li><span><a href=\"#Load-dataset\" data-toc-modified-id=\"Load-dataset-3.4.2\"><span class=\"toc-item-num\">3.4.2&nbsp;&nbsp;</span>Load dataset</a></span></li><li><span><a href=\"#Rescaling\" data-toc-modified-id=\"Rescaling-3.4.3\"><span class=\"toc-item-num\">3.4.3&nbsp;&nbsp;</span>Rescaling</a></span></li><li><span><a href=\"#Split-by-each-model\" data-toc-modified-id=\"Split-by-each-model-3.4.4\"><span class=\"toc-item-num\">3.4.4&nbsp;&nbsp;</span>Split by each model</a></span></li><li><span><a href=\"#Drop-columns\" data-toc-modified-id=\"Drop-columns-3.4.5\"><span class=\"toc-item-num\">3.4.5&nbsp;&nbsp;</span>Drop columns</a></span></li><li><span><a href=\"#Rename-columns\" data-toc-modified-id=\"Rename-columns-3.4.6\"><span class=\"toc-item-num\">3.4.6&nbsp;&nbsp;</span>Rename columns</a></span></li><li><span><a href=\"#Merge-dataset\" data-toc-modified-id=\"Merge-dataset-3.4.7\"><span class=\"toc-item-num\">3.4.7&nbsp;&nbsp;</span>Merge dataset</a></span></li><li><span><a href=\"#Fill-NA-values\" data-toc-modified-id=\"Fill-NA-values-3.4.8\"><span class=\"toc-item-num\">3.4.8&nbsp;&nbsp;</span>Fill NA values</a></span></li><li><span><a href=\"#Get-pred.-demand-change\" data-toc-modified-id=\"Get-pred.-demand-change-3.4.9\"><span class=\"toc-item-num\">3.4.9&nbsp;&nbsp;</span>Get pred. demand change</a></span></li><li><span><a href=\"#Filter-datetime\" data-toc-modified-id=\"Filter-datetime-3.4.10\"><span class=\"toc-item-num\">3.4.10&nbsp;&nbsp;</span>Filter datetime</a></span></li><li><span><a href=\"#Recommend\" data-toc-modified-id=\"Recommend-3.4.11\"><span class=\"toc-item-num\">3.4.11&nbsp;&nbsp;</span>Recommend</a></span></li><li><span><a href=\"#Apply-recommend-change-of-discount-rate\" data-toc-modified-id=\"Apply-recommend-change-of-discount-rate-3.4.12\"><span class=\"toc-item-num\">3.4.12&nbsp;&nbsp;</span>Apply recommend change of discount rate</a></span></li><li><span><a href=\"#Apply-discount-Policy\" data-toc-modified-id=\"Apply-discount-Policy-3.4.13\"><span class=\"toc-item-num\">3.4.13&nbsp;&nbsp;</span>Apply discount Policy</a></span></li><li><span><a href=\"#Split\" data-toc-modified-id=\"Split-3.4.14\"><span class=\"toc-item-num\">3.4.14&nbsp;&nbsp;</span>Split</a></span></li><li><span><a href=\"#Rename-Columns\" data-toc-modified-id=\"Rename-Columns-3.4.15\"><span class=\"toc-item-num\">3.4.15&nbsp;&nbsp;</span>Rename Columns</a></span></li><li><span><a href=\"#Save-Recommendation-Results\" data-toc-modified-id=\"Save-Recommendation-Results-3.4.16\"><span class=\"toc-item-num\">3.4.16&nbsp;&nbsp;</span>Save Recommendation Results</a></span></li></ul></li></ul></li><li><span><a href=\"#Implementation\" data-toc-modified-id=\"Implementation-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Implementation</a></span><ul class=\"toc-item\"><li><span><a href=\"#Model-2-:-Prediction\" data-toc-modified-id=\"Model-2-:-Prediction-4.1\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;</span>Model 2 : Prediction</a></span></li><li><span><a href=\"#Model-3-Recommendation\" data-toc-modified-id=\"Model-3-Recommendation-4.2\"><span class=\"toc-item-num\">4.2&nbsp;&nbsp;</span>Model 3 Recommendation</a></span></li><li><span><a href=\"#Define-Function\" data-toc-modified-id=\"Define-Function-4.3\"><span class=\"toc-item-num\">4.3&nbsp;&nbsp;</span>Define Function</a></span></li><li><span><a href=\"#Default-setting\" data-toc-modified-id=\"Default-setting-4.4\"><span class=\"toc-item-num\">4.4&nbsp;&nbsp;</span>Default setting</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import math\n",
    "import time\n",
    "import timeit\n",
    "import copy\n",
    "import pickle\n",
    "import datetime as dt\n",
    "from itertools import product\n",
    "from collections import defaultdict\n",
    "from datetime import timedelta\n",
    "from dateutil.relativedelta import relativedelta\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Plotting\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.font_manager as fm\n",
    "import seaborn as sns\n",
    "font_path = r'C:\\Windows\\Fonts\\NanumBarunGothic.ttf'\n",
    "font_name = fm.FontProperties(fname=font_path).get_name()\n",
    "matplotlib.rc('font', family=font_name)\n",
    "plt.style.use('seaborn')\n",
    "\n",
    "# Data Preprocessing\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler    # Scaling\n",
    "from sklearn.model_selection import train_test_split    # Split train and test dataset\n",
    "\n",
    "# Evaluation\n",
    "from sklearn.metrics import accuracy_score, mean_squared_error\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV, KFold\n",
    "\n",
    "# Time Series Models\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from statsmodels.tsa.ar_model import AutoReg\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "from statsmodels.tsa.holtwinters import ExponentialSmoothing\n",
    "\n",
    "# Machine Learning Models\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.linear_model import LogisticRegression \n",
    "from sklearn.linear_model import SGDRegressor\n",
    "from sklearn import svm    # Support Vector Machine\n",
    "from sklearn.ensemble import ExtraTreesRegressor    # Extra-trees Regressor\n",
    "import xgboost as xgb    # XGboost\n",
    "import lightgbm as lgb   # Light gradient boosting machine\n",
    "\n",
    "# Customized Exponential Model\n",
    "from scipy.optimize import curve_fit\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define File Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Folder root path\n",
    "root_path = os.path.join('..', 'input')\n",
    "save_path = os.path.join('..', 'result')\n",
    "\n",
    "# Reservation history dataset path (yearly dataset)\n",
    "res_hx_18_path = os.path.join(root_path, 'res_18_discount.csv')\n",
    "res_hx_19_path = os.path.join(root_path, 'res_19_discount.csv')\n",
    "res_hx_20_path = os.path.join(root_path, 'res_20_discount.csv')\n",
    "\n",
    "# Reservation history discount dataset path\n",
    "res_hx_discount_18_path = os.path.join(root_path, 'res_result_18.csv')\n",
    "res_hx_discount_19_path = os.path.join(root_path, 'res_result_19.csv')\n",
    "\n",
    "# Jeju visitor dataset path\n",
    "jeju_visit_path = os.path.join(root_path, 'jeju_visit_daily.csv')\n",
    "\n",
    "# Sort of Discount \n",
    "discount_type_path = os.path.join(root_path, 'discount_type.csv')\n",
    "discount_season_path = os.path.join(root_path, 'discount_season.csv')\n",
    "discount_policy_path = os.path.join(root_path, 'discount_policy.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Jeju Visitors History"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jeju_visit = pd.read_csv(jeju_visit_path, delimiter='\\t', \n",
    "                         dtype={'date': int, 'domestic': int, 'foreign': int, 'total': int})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Rename columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jeju_remap_cols = {'domestic': 'visit_dom', 'foreign': 'visit_for', 'total': 'visit_tot'}\n",
    "jeju_visit = jeju_visit.rename(columns=jeju_remap_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check NAs & Fill values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chkeck NA values in reservation history\n",
    "na_cnt_jeju = jeju_visit.isna().sum().sum()\n",
    "print(f'Number of NAs in jeju visitors: {na_cnt_jeju}')\n",
    "\n",
    "# Fill values\n",
    "jeju_visit = jeju_visit.fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Chnage Data Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jeju_visit['rent_day'] = pd.to_datetime(jeju_visit['date'], format='%Y%m%d')\n",
    "\n",
    "jeju_visit['visit_dom'] = jeju_visit['visit_dom'].astype(int)\n",
    "jeju_visit['visit_for'] = jeju_visit['visit_for'].astype(int)\n",
    "jeju_visit['visit_tot'] = jeju_visit['visit_tot'].astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Drop columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jeju_visit = jeju_visit.drop(columns=['date'], axis=1, errors='ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reorder dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jeju_visit = jeju_visit.sort_values(by=['rent_day'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jeju_visit['visit_tot'] = jeju_visit['visit_dom'] + jeju_visit['visit_for']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reservation History"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reservation dataset\n",
    "res_hx_data_type = {'계약번호': int, '예약경로명': str, '고객구분명': str, '총 청구액(VAT포함)': int,\n",
    "                        '예약모델명': str, '차급': str, '대여일': str, '대여시간': str, '반납일': str,\n",
    "                        '반납시간': str, '차량대여요금(VAT포함)': int, 'CDW요금구분명': str, 'CDW요금': int, \n",
    "                        '총대여료(VAT포함)': int, '적용할인율(%) ': float, '예약일자': str}\n",
    "\n",
    "res_hx_18 = pd.read_csv(res_hx_18_path, delimiter='\\t', dtype=res_hx_data_type)\n",
    "res_hx_19 = pd.read_csv(res_hx_19_path, delimiter='\\t', dtype=res_hx_data_type)\n",
    "res_hx_20 = pd.read_csv(res_hx_20_path, delimiter='\\t', dtype=res_hx_data_type)\n",
    "\n",
    "# Reservation Discount dataset\n",
    "res_discount_data_type = {'예약경로': int,'예약경로명': str, '계약번호': int,  '고객': int, \n",
    "                          '고객구분': float, '고객구분명': str, '총 청구액(VAT포함)': int, \n",
    "                          '총 수납금액(VAT포함)': int, '총 잔액(VAT포함)': int, '예약모델': str, \n",
    "                          '예약모델명': str, '차급': str, '대여일': str, '대여시간': str, \n",
    "                          '반납일': str,'반납시간': str, '대여기간(일)': int, '대여기간(시간)': int, \n",
    "                          '실반납일시': int, '실대여기간(일)': int, '실대여기간(시간)': int,\n",
    "                          '차량대여요금(VAT포함)': int, 'CDW가입여부': str, 'CDW요금구분': float, \n",
    "                          'CDW요금구분명': str, 'CDW요금': str, '회원등급': str, '차종': str, \n",
    "                          '구매목적': str, '내부매출액': int, '수납': str, '예약일자': str, \n",
    "                          '할인유형': str, '할인유형명': str, '적용할인명': str}\n",
    "\n",
    "res_hx_18_discount = pd.read_csv(res_hx_discount_18_path, delimiter='\\t', \n",
    "                                 dtype=res_discount_data_type)\n",
    "res_hx_19_discount = pd.read_csv(res_hx_discount_19_path, delimiter='\\t',\n",
    "                                 dtype=res_discount_data_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Merge dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge reservation histroy dataset\n",
    "res_hx = pd.concat([res_hx_18, res_hx_19], axis=0, ignore_index=True)\n",
    "\n",
    "# Merge reservation discount history dataset\n",
    "res_hx_discount = pd.concat([res_hx_18_discount, res_hx_19_discount], axis=0, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Rename columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reservation history\n",
    "res_hx_remap_cols = {'계약번호': 'res_num', '예약경로명': 'res_route_nm', '고객구분명': 'cust_kind_nm',\n",
    "                     '총 청구액(VAT포함)': 'tot_fee', '예약모델명': 'res_model_nm', '차급': 'car_grd', \n",
    "                     '대여일': 'rent_day', '대여시간': 'rent_time', '반납일': 'return_day',\n",
    "                     '반납시간': 'return_time', '차량대여요금(VAT포함)': 'car_rent_fee', \n",
    "                     'CDW요금구분명': 'cdw_fee_kind_nm', 'CDW요금': 'cdw_fee', '회원등급': 'member_grd',\n",
    "                     '차종': 'car_kind', '예약일자': 'res_day', '할인유형': 'discount_type', \n",
    "                     '총대여료(VAT포함)': 'fin_fee', '적용할인율(%)': 'fin_discount',\n",
    "                     '할인유형명': 'discount_type_nm', '적용할인명': 'applyed_discount'}\n",
    "\n",
    "res_hx = res_hx.rename(columns=res_hx_remap_cols)\n",
    "res_hx_20 = res_hx_20.rename(columns=res_hx_remap_cols)\n",
    "\n",
    "# Reservation discount history\n",
    "res_hx_discount_remap_cols = {'계약번호': 'res_num', '할인유형': 'discount_type', \n",
    "                           '할인유형명': 'discount_type_nm', '적용할인명': 'applyed_discount'}\n",
    "\n",
    "res_hx_discount = res_hx_discount.rename(columns=res_hx_discount_remap_cols)\n",
    "res_res_hx_2020 = res_hx_20.rename(columns=res_hx_discount_remap_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exception\n",
    "res_hx_20 = res_hx_20[pd.to_datetime(res_hx_20['res_day'], format='%Y-%m-%d') >= dt.datetime(2020,1,1)]\n",
    "res_hx_20_discount = res_hx_20[['res_num', 'discount_type_nm', 'applyed_discount']]\n",
    "res_hx_20 = res_hx_20.drop(columns=['member_grd', 'discount_type_nm', 'applyed_discount'], axis=1)\n",
    "res_hx = res_hx.drop(columns=['cust_kind_nm'], axis=1, errors='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_hx = pd.concat([res_hx, res_hx_20], axis=0, ignore_index=True)\n",
    "res_hx_discount = pd.concat([res_hx_discount, res_hx_20_discount], axis=0, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check NAs & Fill values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Chkeck NA values in reservation history\n",
    "na_cnt_res = res_hx.isna().sum().sum()\n",
    "print(f'Number of NAs in reservation history: {na_cnt_res}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Chnage data types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_hx['res_day'] = pd.to_datetime(res_hx['res_day'], format='%Y-%m-%d')\n",
    "res_hx['rent_day'] = pd.to_datetime(res_hx['rent_day'], format='%Y-%m-%d')\n",
    "res_hx['return_day'] = pd.to_datetime(res_hx['return_day'], format='%Y-%m-%d')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filter dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select 1.6 Grade cars\n",
    "res_hx = res_hx[res_hx['res_model_nm'].isin(['K3', 'THE NEW K3 (G)', 'ALL NEW K3 (G)',\n",
    "                                             '아반떼 AD (G) F/L', '올 뉴 아반떼 (G)', '아반떼 AD (G)',\n",
    "                                             '쏘울 (G)', '쏘울 부스터 (G)', '더 올 뉴 벨로스터 (G)'])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Group data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Car Model group\n",
    "# SOUL 모델은 VELOSTER 모델에 포함해서 분석 (실적 데이터가 적음)\n",
    "conditions = [\n",
    "    res_hx['res_model_nm'].isin(['K3', 'THE NEW K3 (G)', 'ALL NEW K3 (G)']),\n",
    "    res_hx['res_model_nm'].isin(['아반떼 AD (G)', '아반떼 AD (G) F/L', '올 뉴 아반떼 (G)']),\n",
    "    res_hx['res_model_nm'].isin(['더 올 뉴 벨로스터 (G)', '쏘울 (G)', '쏘울 부스터 (G)'])    \n",
    "]\n",
    "values = ['K3', 'AVANTE', 'VELOSTER']\n",
    "\n",
    "res_hx['res_model_grp'] = np.select(conditions, values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Drop columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reservation history dataset\n",
    "res_hx_drop_cols = ['res_route_nm', 'car_grd', 'res_model_nm', 'cdw_fee_kind_nm', 'tot_fee']\n",
    "res_hx = res_hx.drop(columns=res_hx_drop_cols, axis=1, errors='ignore')\n",
    "\n",
    "# Reservation discount history dataset\n",
    "res_hx_discount = res_hx_discount[['res_num', 'discount_type_nm', 'applyed_discount']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reorder dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_hx = res_hx.sort_values(by=['rent_day', 'res_day'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add necessary columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Caculate lead time of reservation \n",
    "res_hx['lead_time'] = res_hx['rent_day'] - res_hx['res_day']\n",
    "res_hx['lead_time'] = res_hx['lead_time'].to_numpy().astype('timedelta64[D]') / np.timedelta64(1, 'D')\n",
    "\n",
    "# Conver rent/return day and time to datetime (YYYYMMDD HHMISS)\n",
    "rent_datetime = res_hx['rent_day'].apply(lambda x: dt.datetime.strftime(x, '%Y-%m-%d')) + ' ' + res_hx['rent_time']\n",
    "return_datetime = res_hx['return_day'].apply(lambda x: dt.datetime.strftime(x, '%Y-%m-%d')) + ' ' + res_hx['return_time']\n",
    "res_hx['rent_datetime'] = pd.to_datetime(rent_datetime, format='%Y-%m-%d %H:%M:%S')\n",
    "res_hx['return_datetime'] = pd.to_datetime(return_datetime, format='%Y-%m-%d %H:%M:%S')\n",
    "res_hx['rent_period'] = res_hx['return_datetime'] - res_hx['rent_datetime']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make reservation counts on each day\n",
    "res_cnt_on_rent_day = res_hx[['rent_day', 'res_num']].groupby(by=['rent_day']).count()\n",
    "res_cnt_on_rent_day = res_cnt_on_rent_day.rename(columns={'res_num': 'res_cnt'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rechange data type of reservation\n",
    "res_hx['res_day'] = pd.to_datetime(res_hx['res_day'], format='%Y-%m-%d')\n",
    "res_hx['rent_day'] = pd.to_datetime(res_hx['rent_day'], format='%Y-%m-%d')\n",
    "res_hx['return_day'] = pd.to_datetime(res_hx['return_day'], format='%Y-%m-%d')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discount History"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Discount dataset\n",
    "discount_type = pd.read_csv(discount_type_path, delimiter='\\t', dtype={'kind': str, 'discount': int})\n",
    "discount_season = pd.read_csv(discount_season_path, delimiter='\\t',\n",
    "                                 dtype={'YYYYMMDD': int, 'M3': float, 'M2': float, 'M1': float, 'W4': float,\n",
    "                                        'W3': float, 'W2': float, 'W1': float, 'weekend': float, 'season': float, \n",
    "                                        'event': float})\n",
    "discount_policy = pd.read_csv(discount_policy_path, delimiter='\\t', \n",
    "                  dtype={'date': int, '20160701': float, '20160901': float, '20170101': float,\n",
    "                        '20180101': float, '20180209': float, '20180312': float, '20180314': float,\n",
    "                        '20180327': float, '20180417': float, '20180508': float, '20180628': float,\n",
    "                        '20180720': float, '20180726': float, '20180808': float, '20180817': float,\n",
    "                        '20180903': float, '20180910': float, '20180914': float, '20181010': float,\n",
    "                        '20181029': float, '20181129': float, '20181218': float, '20190111': float,\n",
    "                        '20190122': float, '20190225': float, '20190313': float, '20190322': float,\n",
    "                        '20190405': float, '20190409': float, '20190419': float, '20190510': float,\n",
    "                        '20190521': float, '20190528': float, '20190612': float, '20190701': float,\n",
    "                        '20190718': float, '20190723': float, '20190816': float, '20190823': float,\n",
    "                        '20190909': float, '20191029': float})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check NAs & Fill values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check NAs\n",
    "# discount type\n",
    "na_cnt_discount = discount_type.isna().sum().sum()\n",
    "# Discount Lead Time\n",
    "na_cnt_discount_season = discount_season.isna().sum().sum()\n",
    "# Discount Policy\n",
    "na_cnt_discount_policy = discount_policy.isna().sum().sum()\n",
    "\n",
    "print(f'Number of NAs in discount type: {na_cnt_discount}')\n",
    "print(f'Number of NAs in discount season: {na_cnt_discount_season}')\n",
    "print(f'Number of NAs in discount policy: {na_cnt_discount_policy}')\n",
    "\n",
    "# Fiil values\n",
    "# Discount Poilicy\n",
    "discount_policy['date'] = pd.to_datetime(discount_policy['date'], format='%Y%m%d')\n",
    "discount_policy = discount_policy.set_index('date')\n",
    "discount_policy = discount_policy.fillna(method='ffill', axis=1)\n",
    "discount_policy = discount_policy.fillna(method='bfill', axis=1)\n",
    "discount_policy = discount_policy.reset_index()\n",
    "\n",
    "# Discount Season\n",
    "discount_season = discount_season.fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Chnage Data Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "discount_season['rent_day'] = pd.to_datetime(discount_season['YYYYMMDD'], format='%Y%m%d')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filter dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Except 2017 year\n",
    "discount_season = discount_season[discount_season['YYYYMMDD'] >= 20180101]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Drop columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "discount_season = discount_season[['rent_day', 'weekend', 'season', 'event']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reorder dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "discount_season = discount_season.sort_values(by=['rent_day'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Apply discount policy history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Discount Policy\n",
    "discount_idx_to_lead_time = {idx: day for idx, day in enumerate(discount_policy.columns)}\n",
    "discount_lead_time_to_idx = {day: idx for idx, day in enumerate(discount_policy.columns)}\n",
    "\n",
    "res_day_to_idx = {}\n",
    "res_col = discount_policy.columns[1:]\n",
    "for i in range(len(res_col)-1):\n",
    "    date_range_temp = pd.date_range(start=res_col[i], end=res_col[i+1])\n",
    "    date_range_temp = date_range_temp[:-1]\n",
    "    for date in date_range_temp:\n",
    "        res_day_to_idx[date] = i\n",
    "        \n",
    "date_range_last = pd.date_range(start=res_col[-1], end=discount_policy['date'].iloc[-1].strftime('%Y%m%d'))\n",
    "for date in date_range_last:\n",
    "    res_day_to_idx[date] = len(res_col) - 1\n",
    "    \n",
    "rent_day_to_idx = {date: idx for idx, date in enumerate(discount_policy['date'])}\n",
    "discount_idx_col = [(rent_day_to_idx[idx], res_day_to_idx[col]) for idx, col in zip(res_hx['rent_day'], res_hx['res_day'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_hx['res_discount'] = [discount_policy.iloc[idx, col] for idx, col in discount_idx_col]\n",
    "res_hx['diff_discount'] = res_hx['fin_discount'] - res_hx['res_discount']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Correct naming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Discount type\n",
    "discount_type['discount'] = discount_type['discount'] - 60    # 비회원 기준 할인율\n",
    "discount_type_dict = {kind: discount for kind, discount in zip(discount_type['kind'], discount_type['discount'])}\n",
    "\n",
    "# Correct naming on 2018/2019 years \n",
    "discount_type_dict[\"인터넷일반회원\"] = discount_type_dict[\"일반회원\"]\n",
    "discount_type_dict[\"인터넷골드회원\"] = discount_type_dict[\"골드회원\"]\n",
    "discount_type_dict[\"인터넷더블골드회원\"] = discount_type_dict[\"더블골드회원\"]\n",
    "discount_type_dict[\"예약어플(골드회원)\"] = discount_type_dict[\"골드회원\"]\n",
    "discount_type_dict[\"예약어플(더블골드회원)\"] = discount_type_dict[\"더블골드회원\"]\n",
    "discount_type_dict['비씨카드(주)'] = discount_type_dict['BC카드']\n",
    "discount_type_dict[\"신한Top's club\"] = discount_type_dict[\"신한 Top's club\"]\n",
    "discount_type_dict[\"현대카드오토케어\"] = discount_type_dict[\"현대카드 오토케어\"]\n",
    "discount_type_dict[\"롯데그룹 임직원\"] = discount_type_dict[\"롯데그룹임직원\"]\n",
    "discount_type_dict[\"금호그룹 임직원\"] = discount_type_dict[\"금호그룹임직원\"]\n",
    "discount_type_dict[\"KT그룹 임직원\"] = discount_type_dict[\"KT그룹임직원\"]\n",
    "discount_type_dict[\"VIP거래처(제주)\"] = discount_type_dict[\"VIP거래처\"]\n",
    "discount_type_dict[\"에어부산(FLY.FUN)\"] = discount_type_dict[\"에어부산(FLY & FUN)\"]\n",
    "discount_type_dict[\"삼성전자 서비스\"] = discount_type_dict[\"삼성전자서비스\"]\n",
    "discount_type_dict[\"하나카드 VIP 컨시어지\"] = discount_type_dict[\"하나카드VIP컨시어지\"]\n",
    "discount_type_dict[\"BC-kt 제휴카드\"] = discount_type_dict[\"BC-kt제휴카드\"]\n",
    "discount_type_dict[\"인천공항홈페이지\"] = discount_type_dict[\"인천공항 홈페이지\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Integrate History Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Merge all of  history dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reservation + Reservation Counts on each day\n",
    "res_hx = pd.merge(res_hx, res_cnt_on_rent_day, how='left', on='rent_day', \n",
    "                  left_index=True, right_index=False)\n",
    "\n",
    "# Reservaion + Discount season\n",
    "res_hx = pd.merge(res_hx, discount_season, how='left', on='rent_day', \n",
    "                  left_index=True, right_index=False)\n",
    "\n",
    "# Reservation + Reservation discount\n",
    "res_hx = pd.merge(res_hx, res_hx_discount, how='left', on='res_num', \n",
    "               left_index=True, right_index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Drop unnecessary columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_hx = res_hx.drop(columns=['rent_time', 'return_time', 'rent_datetime', 'return_datetime'],\n",
    "                    axis=1, errors='ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add seasonality column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conditions = [\n",
    "    (res_hx['season'] != 0),\n",
    "    (res_hx['event'] == 1),\n",
    "    ((res_hx['weekend'] == 1) & (res_hx['weekend'] == 1) & (res_hx['weekend'] == 1)),\n",
    "    (res_hx['season'] + res_hx['event'] + res_hx['weekend'] == 0)]\n",
    "values =[4, 3, 2, 1]\n",
    "\n",
    "res_hx['seasonality'] = np.select(conditions, values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add lead time vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conditions = [\n",
    "    ((res_hx['lead_time'] >= 7 * 4) & (res_hx['lead_time'] < 7 * 5)),\n",
    "    ((res_hx['lead_time'] >= 7 * 5) & (res_hx['lead_time'] < 7 * 6)),\n",
    "    ((res_hx['lead_time'] >= 7 * 6) & (res_hx['lead_time'] < 7 * 7)),\n",
    "    ((res_hx['lead_time'] >= 7 * 7) & (res_hx['lead_time'] < 7 * 8)),\n",
    "    ((res_hx['lead_time'] >= 7 * 8) & (res_hx['lead_time'] < 7 * 9)),\n",
    "    ((res_hx['lead_time'] >= 7 * 9) & (res_hx['lead_time'] < 7 * 10)),\n",
    "    ((res_hx['lead_time'] >= 7 * 10) & (res_hx['lead_time'] < 7 * 11)),\n",
    "    ((res_hx['lead_time'] >= 7 * 11) & (res_hx['lead_time'] < 7 * 12)),\n",
    "    (res_hx['lead_time'] >= 7 * 12)]\n",
    "values = np.arange(28, 28 + 9, 1)\n",
    "\n",
    "res_hx['lead_time_vec'] = np.select(conditions, values)\n",
    "res_hx['lead_time_vec'] = np.where(res_hx['lead_time_vec'] <= 28, \n",
    "                                   res_hx['lead_time'].values,\n",
    "                                   res_hx['lead_time_vec'].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert time format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_hx['rent_period'] = res_hx['rent_period'].apply(lambda x: x.days* 24 + x.seconds//3600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Update discount rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_hx['discount_add'] = res_hx['applyed_discount'].apply(lambda x: discount_type_dict.get(x, 0))\n",
    "res_hx['res_discount'] += res_hx['discount_add']\n",
    "res_hx = res_hx.drop(columns=['discount_add'], errors='ignore')\n",
    "\n",
    "# 할인율 상한선 적용: 90%\n",
    "res_hx['res_discount'] = np.where(res_hx['res_discount'] > 90, 90, res_hx['res_discount'])    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate reservation fee"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_hx['res_fee'] = res_hx['fin_fee'] - res_hx['cdw_fee']\n",
    "res_hx['res_fee'] = res_hx['res_fee'] / (1 - res_hx['fin_discount']/100)\n",
    "res_hx['res_fee'] = res_hx['res_fee'] * (1 - res_hx['res_discount']/100) + res_hx['cdw_fee']\n",
    "res_hx['res_fee'] = round(res_hx['res_fee'].to_numpy(), 0)\n",
    "res_hx = res_hx.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_hx.loc[res_hx['res_fee'] == np.inf, 'res_fee'] = ((res_hx['fin_fee'] - res_hx['cdw_fee']) * res_hx['res_discount']) + res_hx['cdw_fee']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove exception"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 예약 가격이 없는 경우 제외\n",
    "res_hx = res_hx[res_hx['res_fee'].notnull()]\n",
    "\n",
    "# 쿠폰할인 제외\n",
    "res_hx = res_hx[res_hx['discount_type_nm'] != '쿠폰할인']\n",
    "\n",
    "# 임의할인: 할인율 100% 제외\n",
    "res_hx = res_hx[res_hx['fin_fee'] != 0] \n",
    "\n",
    "# 업체할인: 비씨카드 이외 할인 제외\n",
    "res_hx = res_hx[(res_hx['discount_type_nm'] != '업체할인') | (res_hx['applyed_discount'] == '비씨카드(주)')] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save preprocessing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seasonality = res_hx[['rent_day', 'seasonality']].drop_duplicates().reset_index(drop=True)\n",
    "seasonality.to_csv(os.path.join(save_path, 'data', 'seasonality.csv'), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### History dataset: Res. count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reservation history dataset\n",
    "res_hx_17_19_path = os.path.join('..', 'input', 'res_hx_17_19.csv')\n",
    "res_hx_20_path = os.path.join('..', 'input', 'res_hx_20.csv')\n",
    "res_hx_data_type = {'계약번호': int, '예약경로명': str, '예약모델명': str, \n",
    "                    '대여일': str, '대여시간': str, '반납일': str, '반납시간': str, \n",
    "                    '차량대여요금(VAT포함)': int, 'CDW요금': int, '총대여료(VAT포함)': int, \n",
    "                    '적용할인율(%) ': int, '생성일': str}\n",
    "\n",
    "res_hx_17_19 = pd.read_csv(res_hx_17_19_path, delimiter='\\t', dtype=res_hx_data_type)\n",
    "res_hx_20 = pd.read_csv(res_hx_20_path, delimiter='\\t', dtype=res_hx_data_type)\n",
    "\n",
    "# Seasonality setting dataset\n",
    "seasonality = pd.read_csv(os.path.join('..', 'input', 'seasonality', 'seasonality_hx.csv'))\n",
    "seasonality['rent_day'] = pd.to_datetime(seasonality['rent_day'], format='%Y-%m-%d')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Rename columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reservation history\n",
    "res_hx_remap_cols = {\n",
    "    '계약번호': 'res_num', '예약경로명': 'res_route_nm', '예약모델명': 'res_model_nm', \n",
    "    '대여일': 'rent_day', '대여시간': 'rent_time', '반납일': 'return_day', '반납시간': 'return_time', \n",
    "    '차량대여요금(VAT포함)': 'car_rent_fee', 'CDW요금': 'cdw_fee', '총대여료(VAT포함)': 'tot_fee', \n",
    "    '적용할인율(%)': 'discount', '생성일':'res_day'}\n",
    "\n",
    "res_hx_17_19 = res_hx_17_19.rename(columns=res_hx_remap_cols)\n",
    "res_hx_20 = res_hx_20.rename(columns=res_hx_remap_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Change data types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_hx_17_19['rent_day'] = pd.to_datetime(res_hx_17_19['rent_day'], format='%Y-%m-%d') \n",
    "res_hx_20['rent_day'] = pd.to_datetime(res_hx_20['rent_day'], format='%Y-%m-%d')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filter periods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Except 2017 year\n",
    "res_hx_18_19 = res_hx_17_19[(res_hx_17_19['rent_day'] >= dt.datetime(2018, 1, 1)) &\n",
    "                            (res_hx_17_19['rent_day'] < dt.datetime(2020, 1, 1))]\n",
    "res_hx_20 = res_hx_20[res_hx_20['rent_day'] >= dt.datetime(2020,1,1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Merge dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_hx = pd.concat([res_hx_18_19, res_hx_20])\n",
    "res_hx = res_hx.sort_values(by=['rent_day', 'res_day'])\n",
    "res_hx = res_hx.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_hx = pd.merge(res_hx, seasonality, how='left', on='rent_day', left_index=True, right_index=False)\n",
    "res_hx = res_hx.reset_index(drop=True)\n",
    "res_hx['seasonality'] = res_hx['seasonality'].fillna(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filter car grades (1.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select 1.6 Grade cars\n",
    "filter_cars = ['K3', 'THE NEW K3 (G)', 'ALL NEW K3 (G)',\n",
    "               '아반떼 AD (G)', '아반떼 AD (G) F/L', '올 뉴 아반떼 (G)',\n",
    "               '쏘울 (G)', '쏘울 부스터 (G)', '더 올 뉴 벨로스터 (G)']\n",
    "res_hx = res_hx[res_hx['res_model_nm'].isin(filter_cars)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Group data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Car Model group\n",
    "# SOUL 모델은 VELOSTER 모델에 포함해서 분석 (실적 데이터가 적음)\n",
    "avante = ['아반떼 AD (G)', '아반떼 AD (G) F/L', '올 뉴 아반떼 (G)']\n",
    "k3 = ['K3', 'THE NEW K3 (G)', 'ALL NEW K3 (G)']\n",
    "veloster_soul = ['더 올 뉴 벨로스터 (G)', '쏘울 (G)', '쏘울 부스터 (G)']\n",
    "\n",
    "conditions = [\n",
    "    res_hx['res_model_nm'].isin(avante),\n",
    "    res_hx['res_model_nm'].isin(k3),\n",
    "    res_hx['res_model_nm'].isin(veloster_soul)]\n",
    "\n",
    "values = ['AVANTE', 'K3', 'VELOSTER']\n",
    "res_hx['res_model_grp'] = np.select(conditions, values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Change data types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_hx['res_day'] = pd.to_datetime(res_hx['res_day'], format='%Y-%m-%d')\n",
    "res_hx['discount'] = res_hx['discount'].astype(int)\n",
    "res_hx['seasonality'] = res_hx['seasonality'].astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Make additional columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add lead time\n",
    "res_hx['lead_time'] = res_hx['rent_day'] - res_hx['res_day']\n",
    "res_hx['lead_time'] = res_hx['lead_time'].to_numpy().astype('timedelta64[D]') / np.timedelta64(1, 'D')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add lead time vector\n",
    "conditions = [\n",
    "    ((res_hx['lead_time'] >= 7 * 4) & (res_hx['lead_time'] < 7 * 5)),\n",
    "    ((res_hx['lead_time'] >= 7 * 5) & (res_hx['lead_time'] < 7 * 6)),\n",
    "    ((res_hx['lead_time'] >= 7 * 6) & (res_hx['lead_time'] < 7 * 7)),\n",
    "    ((res_hx['lead_time'] >= 7 * 7) & (res_hx['lead_time'] < 7 * 8)),\n",
    "    ((res_hx['lead_time'] >= 7 * 8) & (res_hx['lead_time'] < 7 * 9)),\n",
    "    ((res_hx['lead_time'] >= 7 * 9) & (res_hx['lead_time'] < 7 * 10)),\n",
    "    ((res_hx['lead_time'] >= 7 * 10) & (res_hx['lead_time'] < 7 * 11)),\n",
    "    ((res_hx['lead_time'] >= 7 * 11) & (res_hx['lead_time'] < 7 * 12)),\n",
    "    (res_hx['lead_time'] >= 7 * 12)]\n",
    "\n",
    "values = np.arange(28, 28 + 9, 1)\n",
    "\n",
    "res_hx['lead_time_vec'] = np.select(conditions, values)\n",
    "res_hx['lead_time_vec'] = np.where(res_hx['lead_time_vec'] <= 28, \n",
    "                                   res_hx['lead_time'].values,\n",
    "                                   res_hx['lead_time_vec'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_hx['lead_time'] = res_hx['lead_time'] * -1\n",
    "res_hx['lead_time_vec'] = res_hx['lead_time_vec'] * -1\n",
    "res_hx['lead_time_vec'] = res_hx['lead_time_vec'].astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save reservation history dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_hx = res_hx.sort_values(by=['rent_day', 'res_day'])\n",
    "res_hx.to_csv(os.path.join('..', 'result', 'data', 'model_2', 'res_history_18_20.csv'), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Drop unnecessary columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hx_drop_col = ['res_route_nm', 'res_model_nm', 'rent_time', 'return_day', 'return_time', \n",
    "#                'car_rent_fee', 'cdw_fee', 'tot_fee']\n",
    "hx_drop_col = ['res_route_nm', 'res_model_nm', 'rent_time', 'return_day', 'return_time', \n",
    "               'car_rent_fee', 'cdw_fee', 'tot_fee']\n",
    "res_hx = res_hx.drop(columns=hx_drop_col, errors='ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Group by rent and reservation day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_cnt = res_hx.groupby(by=['rent_day', 'res_model_grp', 'res_day']).count()['lead_time_vec']\n",
    "res_cum_cnt = res_cnt.groupby(by=['rent_day', 'res_model_grp']).cumsum()\n",
    "\n",
    "res_dscnt = res_hx.groupby(by=['rent_day', 'res_model_grp', 'res_day']).sum()['discount']\n",
    "res_cum_dscnt = res_dscnt.groupby(by=['rent_day', 'res_model_grp']).cumsum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_cum = pd.DataFrame({'cum_dscnt': res_cum_dscnt,\n",
    "                        'cum_cnt': res_cum_cnt},\n",
    "                        index=res_cum_dscnt.index)\n",
    "res_cum['cum_dscnt_mean'] = res_cum['cum_dscnt'] / res_cum['cum_cnt']\n",
    "res_cum['cum_dscnt_mean'] = np.round(res_cum['cum_dscnt_mean'].to_numpy())\n",
    "res_cum = res_cum.reset_index(level=(0, 1, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_hx_car = pd.merge(res_hx, res_cum, how='left', on=['rent_day', 'res_model_grp', 'res_day'],\n",
    "                      left_index=True, right_index=False)\n",
    "res_hx_car = res_hx_car.sort_values(by=['rent_day', 'res_model_grp', 'res_day'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop unnecessary columns\n",
    "res_hx_car = res_hx_car.drop(columns=['res_num', 'res_day', 'lead_time', 'rent_day'\n",
    "                                      'discount', 'cum_dscnt'], errors='ignore')\n",
    "res_hx_car = res_hx_car.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Re-group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_hx_car_grp = res_hx_car.groupby(by=['res_model_grp', 'seasonality', 'lead_time_vec']).mean()\n",
    "res_hx_car_grp = res_hx_car_grp.drop(columns=['discount'], errors='ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Divde into each car model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_2_cnt_av = res_hx_car_grp.loc['AVANTE']\n",
    "model_2_cnt_k3 = res_hx_car_grp.loc['K3']\n",
    "model_2_cnt_vl = res_hx_car_grp.loc['VELOSTER']\n",
    "\n",
    "model_2_cnt_av = model_2_cnt_av.reset_index(level=(0, 1))\n",
    "model_2_cnt_k3 = model_2_cnt_k3.reset_index(level=(0, 1))\n",
    "model_2_cnt_vl = model_2_cnt_vl.reset_index(level=(0, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = os.path.join('..', 'result', 'data', 'model_2')\n",
    "model_2_cnt_av.to_csv(os.path.join(save_path, 'model_2_cnt_av.csv'), index=False)\n",
    "model_2_cnt_k3.to_csv(os.path.join(save_path, 'model_2_cnt_k3.csv'), index=False)\n",
    "model_2_cnt_vl.to_csv(os.path.join(save_path, 'model_2_cnt_vl.csv'), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### History dataset: Res. Utilization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reservation history dataset\n",
    "res_hx_17_19_path = os.path.join('..', 'input', 'res_hx_17_19.csv')\n",
    "res_hx_20_path = os.path.join('..', 'input', 'res_hx_20.csv')\n",
    "res_hx_data_type = {'계약번호': int, '예약경로명': str, '예약모델명': str, \n",
    "                    '대여일': str, '대여시간': str, '반납일': str, '반납시간': str, \n",
    "                    '차량대여요금(VAT포함)': int, 'CDW요금': int, '총대여료(VAT포함)': int, \n",
    "                    '적용할인율(%) ': int, '생성일': str}\n",
    "\n",
    "res_hx_17_19 = pd.read_csv(res_hx_17_19_path, delimiter='\\t', dtype=res_hx_data_type)\n",
    "res_hx_20 = pd.read_csv(res_hx_20_path, delimiter='\\t', dtype=res_hx_data_type)\n",
    "\n",
    "# Capacity history of car models\n",
    "capa_hx_path = os.path.join('..', 'input', 'capa', 'capa_hx.csv')\n",
    "capa_hx = pd.read_csv(capa_hx_path, delimiter='\\t', dtype={'date':str, 'model':str, 'capa':int})\n",
    "\n",
    "# Seasonality setting dataset\n",
    "seasonality = pd.read_csv(os.path.join('..', 'input', 'seasonality', 'seasonality_hx_bak.csv'))\n",
    "seasonality['rent_day'] = pd.to_datetime(seasonality['rent_day'], format='%Y-%m-%d')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Rename columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reservation history\n",
    "res_hx_remap_cols = {\n",
    "    '계약번호': 'res_num', '예약경로명': 'res_route_nm', '예약모델명': 'res_model_nm', \n",
    "    '대여일': 'rent_day', '대여시간': 'rent_time', '반납일': 'return_day', '반납시간': 'return_time', \n",
    "    '차량대여요금(VAT포함)': 'car_rent_fee', 'CDW요금': 'cdw_fee', '총대여료(VAT포함)': 'tot_fee', \n",
    "    '적용할인율(%)': 'discount', '생성일':'res_day'}\n",
    "\n",
    "res_hx_17_19 = res_hx_17_19.rename(columns=res_hx_remap_cols)\n",
    "res_hx_20 = res_hx_20.rename(columns=res_hx_remap_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Change data types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_hx_17_19['rent_day'] = pd.to_datetime(res_hx_17_19['rent_day'], format='%Y-%m-%d') \n",
    "res_hx_20['rent_day'] = pd.to_datetime(res_hx_20['rent_day'], format='%Y-%m-%d')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filter periods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Except 2017 year\n",
    "res_hx_18_19 = res_hx_17_19[(res_hx_17_19['rent_day'] >= dt.datetime(2018, 1, 1)) &\n",
    "                            (res_hx_17_19['rent_day'] < dt.datetime(2020, 1, 1))]\n",
    "res_hx_20 = res_hx_20[res_hx_20['rent_day'] >= dt.datetime(2020,1,1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Merge dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_hx = pd.concat([res_hx_18_19, res_hx_20])\n",
    "res_hx = res_hx.sort_values(by=['rent_day', 'res_day'])\n",
    "res_hx = res_hx.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_hx = pd.merge(res_hx, seasonality, how='left', on='rent_day', left_index=True, right_index=False)\n",
    "res_hx = res_hx.reset_index(drop=True)\n",
    "res_hx['seasonality'] = res_hx['seasonality'].fillna(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Delete variables from memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "del res_hx_17_19\n",
    "del res_hx_18_19\n",
    "del res_hx_20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filter car grades"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select 1.6 Grade cars\n",
    "filter_cars = ['K3', 'THE NEW K3 (G)', 'ALL NEW K3 (G)',\n",
    "               '아반떼 AD (G)', '아반떼 AD (G) F/L', '올 뉴 아반떼 (G)',\n",
    "               '쏘울 (G)', '쏘울 부스터 (G)', '더 올 뉴 벨로스터 (G)']\n",
    "res_hx = res_hx[res_hx['res_model_nm'].isin(filter_cars)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Group car models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Car Model group\n",
    "# SOUL 모델은 VELOSTER 모델에 포함해서 분석 (실적 데이터가 적음)\n",
    "avante = ['아반떼 AD (G)', '아반떼 AD (G) F/L', '올 뉴 아반떼 (G)']\n",
    "k3 = ['K3', 'THE NEW K3 (G)', 'ALL NEW K3 (G)']\n",
    "veloster_soul = ['더 올 뉴 벨로스터 (G)', '쏘울 (G)', '쏘울 부스터 (G)']\n",
    "\n",
    "conditions = [\n",
    "    res_hx['res_model_nm'].isin(avante),\n",
    "    res_hx['res_model_nm'].isin(k3),\n",
    "    res_hx['res_model_nm'].isin(veloster_soul)]\n",
    "\n",
    "values = ['AVANTE', 'K3', 'VELOSTER']\n",
    "res_hx['res_model_grp'] = np.select(conditions, values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Change data types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_hx['res_day'] = pd.to_datetime(res_hx['res_day'], format='%Y-%m-%d')\n",
    "res_hx['discount'] = res_hx['discount'].astype(int)\n",
    "res_hx['seasonality'] = res_hx['seasonality'].astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Drop unnecessary columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_col = ['res_route_nm', 'res_model_nm' ,'car_rent_fee', 'cdw_fee', 'tot_fee', 'res_num']\n",
    "res_hx = res_hx.drop(columns=drop_col, errors='ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Make utilization rate dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_util_cnt_df(df: pd.DataFrame):\n",
    "    res_hx_util = []\n",
    "    for rent_d, rent_t, return_d, return_t, res_day, discount, model_grp in zip(\n",
    "        df['rent_day'], df['rent_time'], df['return_day'], df['return_time'], df['res_day'],\n",
    "        df['discount'], df['res_model_grp']):\n",
    "        \n",
    "        day_hour = timedelta(hours=24)\n",
    "        six_hour = timedelta(hours=6)\n",
    "        date_range = pd.date_range(start=rent_d, end=return_d)    # days of rent periods\n",
    "        date_len = len(date_range)\n",
    "        f = list(map(int, rent_t.split(':'))) \n",
    "        l = list(map(int, return_t.split(':')))   \n",
    "        ft = timedelta(hours=f[0], minutes=f[1])    # time of rent day \n",
    "        lt = timedelta(hours=l[0], minutes=l[1])    # time of return day\n",
    "        \n",
    "        f_util = 1\n",
    "        l_util = 1 \n",
    "        if (day_hour - ft) < six_hour:\n",
    "            f_util = (day_hour - ft) / six_hour\n",
    "        if lt < six_hour:\n",
    "            l_util = lt / six_hour\n",
    "\n",
    "        if date_len > 2:\n",
    "            util = np.array(f_util)\n",
    "            util = np.append(util, np.ones(date_len-2))\n",
    "            util = np.append(util, l_util)\n",
    "            \n",
    "        elif date_len == 2:\n",
    "            util = np.array([f_util, l_util])\n",
    "            \n",
    "        else :\n",
    "            util = 1\n",
    "            if (lt - ft) < six_hour:\n",
    "                util = (lt - ft) / six_hour\n",
    "            util = np.array([util])    \n",
    "        \n",
    "        res_hx_util.extend(np.array([\n",
    "            date_range,\n",
    "            [res_day] * date_len,\n",
    "            util,\n",
    "            [discount] * date_len,\n",
    "            [model_grp] * date_len\n",
    "        ]).T)\n",
    "    \n",
    "    res_hx_util_df = pd.DataFrame(res_hx_util,\n",
    "                                 columns=['rent_day', 'res_day', 'util_rate', 'discount', 'model',])\n",
    "\n",
    "    return res_hx_util_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_util_cnt_df_BAK(df: pd.DataFrame):\n",
    "    res_hx_util = []\n",
    "    for rent_d, rent_t, return_d, return_t, res_day, discount, model_grp in zip(\n",
    "        df['rent_day'], df['rent_time'], df['return_day'], df['return_time'], df['res_day'],\n",
    "        df['discount'], df['res_model_grp']):\n",
    "        \n",
    "        date_range = pd.date_range(start=rent_d, end=return_d)    # days of rent periods\n",
    "        date_len = len(date_range)\n",
    "        f = list(map(int, rent_t.split(':'))) \n",
    "        l = list(map(int, return_t.split(':')))   \n",
    "        ft = timedelta(hours=f[0], minutes=f[1])    # time of rent day \n",
    "        lt = timedelta(hours=l[0], minutes=l[1])    # time of return day           \n",
    "        day_hour = timedelta(hours=24)\n",
    "        \n",
    "        if date_len > 2:\n",
    "            f_util = (day_hour- ft) / day_hour\n",
    "            l_util = lt / day_hour\n",
    "            util = np.array(f_util)\n",
    "            util = np.append(util, np.ones(date_len-2))\n",
    "            util = np.append(util, l_util)\n",
    "            \n",
    "        elif date_len == 2:\n",
    "            f_util = (day_hour - ft) / day_hour\n",
    "            l_util = lt / day_hour\n",
    "            util = np.array([f_util, l_util])\n",
    "        else :\n",
    "            util = (lt - ft) / day_hour\n",
    "            util = np.array([util])    \n",
    "        \n",
    "        res_hx_util.extend(np.array([\n",
    "            date_range,\n",
    "            [res_day] * date_len,\n",
    "            util,\n",
    "            [discount] * date_len,\n",
    "            [model_grp] * date_len\n",
    "        ]).T)\n",
    "    \n",
    "    res_hx_util_df = pd.DataFrame(res_hx_util,\n",
    "                                 columns=['rent_day', 'res_day', 'util_rate', 'discount', 'model',])\n",
    "\n",
    "    return res_hx_util_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make utilization rate dataset\n",
    "res_hx_util = make_util_cnt_df(df=res_hx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Group by rent & reservation day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_cnt = res_hx_util.groupby(by=['rent_day', 'model', 'res_day']).count()['discount']\n",
    "res_cum_cnt = res_cnt.groupby(by=['rent_day', 'model']).cumsum()\n",
    "\n",
    "res_util = res_hx_util.groupby(by=['rent_day', 'model', 'res_day']).sum()['util_rate']\n",
    "res_cum_util = res_util.groupby(by=['rent_day', 'model']).cumsum()\n",
    "\n",
    "res_dscnt = res_hx_util.groupby(by=['rent_day', 'model', 'res_day']).sum()['discount']\n",
    "res_cum_dscnt = res_dscnt.groupby(by=['rent_day', 'model']).cumsum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert to dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_cum_util = pd.DataFrame({'cum_util_time': res_cum_util,\n",
    "                             'cum_util_cnt': res_cum_cnt,\n",
    "                             'cum_dscnt': res_cum_dscnt},\n",
    "                             index=res_cum_dscnt.index)\n",
    "res_cum_util['cum_dscnt_mean'] = res_cum_util['cum_dscnt'] / res_cum_util['cum_util_cnt']\n",
    "res_cum_util['cum_dscnt_mean'] = np.round(res_cum_util['cum_dscnt_mean'].to_numpy(), 2)\n",
    "res_cum_util = res_cum_util.reset_index(level=(0, 1, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "hx_util = pd.merge(res_cum_util, seasonality, how='inner', on=['rent_day'],\n",
    "                      left_index=True, right_index=False)\n",
    "hx_util = hx_util.sort_values(by=['rent_day', 'model', 'res_day'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add capacity of model column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "hx_util['month'] = hx_util['rent_day'].dt.strftime('%Y%m')\n",
    "capa_hx_dict = {(month, model): capa for month, model, capa in zip(capa_hx['date'],\n",
    "                                                                   capa_hx['model'], capa_hx['capa'])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set capacity for each rent day & model\n",
    "def f(x):\n",
    "    return capa_hx_dict[(x[0], x[1])]\n",
    "hx_util['capa'] = hx_util[['month', 'model']].apply(f, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "hx_util['cum_util_time_rate'] = hx_util['cum_util_time'] / hx_util['capa']\n",
    "hx_util['cum_util_cnt_rate'] = hx_util['cum_util_cnt'] / hx_util['capa']\n",
    "hx_util = hx_util.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add additional columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add lead time\n",
    "hx_util['lead_time'] = hx_util['rent_day'] - hx_util['res_day']\n",
    "hx_util['lead_time'] = hx_util['lead_time'].to_numpy().astype('timedelta64[D]') / np.timedelta64(1, 'D')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add lead time vector\n",
    "conditions = [\n",
    "    ((hx_util['lead_time'] >= 7 * 4) & (hx_util['lead_time'] < 7 * 5)),\n",
    "    ((hx_util['lead_time'] >= 7 * 5) & (hx_util['lead_time'] < 7 * 6)),\n",
    "    ((hx_util['lead_time'] >= 7 * 6) & (hx_util['lead_time'] < 7 * 7)),\n",
    "    ((hx_util['lead_time'] >= 7 * 7) & (hx_util['lead_time'] < 7 * 8)),\n",
    "    ((hx_util['lead_time'] >= 7 * 8) & (hx_util['lead_time'] < 7 * 9)),\n",
    "    ((hx_util['lead_time'] >= 7 * 9) & (hx_util['lead_time'] < 7 * 10)),\n",
    "    ((hx_util['lead_time'] >= 7 * 10) & (hx_util['lead_time'] < 7 * 11)),\n",
    "    ((hx_util['lead_time'] >= 7 * 11) & (hx_util['lead_time'] < 7 * 12)),\n",
    "    (hx_util['lead_time'] >= 7 * 12)]\n",
    "\n",
    "values = np.arange(28, 28 + 9, 1)\n",
    "\n",
    "hx_util['lead_time_vec'] = np.select(conditions, values)\n",
    "hx_util['lead_time_vec'] = np.where(hx_util['lead_time_vec'] <= 28, \n",
    "                                   hx_util['lead_time'].values,\n",
    "                                   hx_util['lead_time_vec'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hx_util['lead_time'] = hx_util['lead_time'] * -1\n",
    "hx_util = hx_util.drop(columns=['lead_time', 'cum_dscnt'])\n",
    "hx_util['lead_time_vec'] = hx_util['lead_time_vec'] * -1\n",
    "hx_util['lead_time_vec'] = hx_util['lead_time_vec'].astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Re-group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "hx_uitl_grp = hx_util.groupby(by=['model', 'seasonality', 'lead_time_vec']).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dived into each car model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_2_uilt_av = hx_uitl_grp.loc['AVANTE']\n",
    "model_2_uilt_k3 = hx_uitl_grp.loc['K3']\n",
    "model_2_uilt_vl = hx_uitl_grp.loc['VELOSTER']\n",
    "\n",
    "model_2_uilt_av = model_2_uilt_av.reset_index(level=(0, 1))\n",
    "model_2_uilt_k3 = model_2_uilt_k3.reset_index(level=(0, 1))\n",
    "model_2_uilt_vl = model_2_uilt_vl.reset_index(level=(0, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = os.path.join('..', 'result', 'data', 'model_2')\n",
    "\n",
    "model_2_uilt_av.to_csv(os.path.join(save_path, 'model_2_util_av.csv'), index=False)\n",
    "model_2_uilt_k3.to_csv(os.path.join(save_path, 'model_2_util_k3.csv'), index=False)\n",
    "model_2_uilt_vl.to_csv(os.path.join(save_path, 'model_2_util_vl.csv'), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recent dataset: Res. Utilization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recent reservation dataset\n",
    "res_date = '201113'\n",
    "res_curr_path = os.path.join('..', 'input', 'reservation', 'res_201113.csv')\n",
    "\n",
    "res_data_type = {'예약경로': int,'예약경로명': str, '계약번호': int,  '고객구분': int, \n",
    "                 '고객구분명': str, '총 청구액(VAT포함)': str, '예약모델': str, '예약모델명': str, \n",
    "                 '차급': str, '대여일': str, '대여시간': str, '반납일': str,'반납시간': str, \n",
    "                 '대여기간(일)': int, '대여기간(시간)': int, 'CDW요금': str, '할인유형': str,\n",
    "                 '할인유형명': str, '적용할인명': str, '회원등급': str, '구매목적': str,\n",
    "                 '생성일': str,   '차종': str}\n",
    "\n",
    "res_curr = pd.read_csv(res_curr_path, delimiter='\\t', dtype=res_data_type)\n",
    "\n",
    "# Capacity history of car models\n",
    "capa_curr_path = os.path.join('..', 'input', 'capa', 'capa_curr.csv')\n",
    "capa_curr = pd.read_csv(capa_curr_path, delimiter='\\t', dtype={'date':str, 'model':str, 'capa':int})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Rename columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reservation history\n",
    "res_remap_cols ={\n",
    "     '예약경로': 'res_route', '예약경로명': 'res_route_nm', '계약번호': 'res_num', \n",
    "     '고객구분': 'cust_kind', '고객구분명': 'cust_kind_nm', '총 청구액(VAT포함)': 'tot_fee', \n",
    "     '예약모델': 'res_model', '예약모델명': 'res_model_nm', '차급': 'car_grd', \n",
    "     '대여일': 'rent_day', '대여시간': 'rent_time', '반납일': 'return_day', '반납시간': 'return_time',\n",
    "     '대여기간(일)': 'rent_period_day', '대여기간(시간)': 'rent_period_time',\n",
    "     'CDW요금': 'cdw_fee', '할인유형': 'discount_type', '할인유형명': 'discount_type_nm',\n",
    "     '적용할인명': 'applyed_discount', '적용할인율(%)': 'discount_rate', '회원등급': 'member_grd',\n",
    "     '구매목적': 'sale_purpose','생성일': 'res_day', '차종': 'car_kind'}\n",
    "res_curr = res_curr.rename(columns=res_remap_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Drop columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_drop_col = ['res_num', 'res_route', 'res_route_nm', 'cust_kind', 'cust_kind_nm', 'tot_fee',\n",
    "                'res_model', 'car_grd', 'rent_period_day', 'rent_period_time', 'cdw_fee', \n",
    "                'discount_type', 'discount_type_nm', 'sale_purpose', 'applyed_discount', \n",
    "                'discount_rate', 'member_grd', 'sale_purpose', 'car_kind']\n",
    "res_curr = res_curr.drop(columns=res_drop_col, errors='ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filter car grades"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select 1.6 Grade cars\n",
    "filter_cars = ['K3', 'THE NEW K3 (G)', 'ALL NEW K3 (G)',\n",
    "               '아반떼 AD (G)', '아반떼 AD (G) F/L', '올 뉴 아반떼 (G)',\n",
    "               '쏘울 (G)', '쏘울 부스터 (G)', '더 올 뉴 벨로스터 (G)']\n",
    "res_curr = res_curr[res_curr['res_model_nm'].isin(filter_cars)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Group car models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Car Model group\n",
    "# SOUL 모델은 VELOSTER 모델에 포함해서 분석 (실적 데이터가 적음)\n",
    "avante = ['아반떼 AD (G)', '아반떼 AD (G) F/L', '올 뉴 아반떼 (G)']\n",
    "k3 = ['K3', 'THE NEW K3 (G)', 'ALL NEW K3 (G)']\n",
    "veloster_soul = ['더 올 뉴 벨로스터 (G)', '쏘울 (G)', '쏘울 부스터 (G)']\n",
    "\n",
    "conditions = [\n",
    "    res_curr['res_model_nm'].isin(avante),\n",
    "    res_curr['res_model_nm'].isin(k3),\n",
    "    res_curr['res_model_nm'].isin(veloster_soul)]\n",
    "\n",
    "values = ['AVANTE', 'K3', 'VELOSTER']\n",
    "res_curr['res_model_grp'] = np.select(conditions, values)\n",
    "\n",
    "res_curr = res_curr.drop(columns=['res_model_nm'], errors='ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Change data types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_curr['res_day'] = pd.to_datetime(res_curr['res_day'], format='%Y-%m-%d')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Make utilization rate dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_util_cnt_df(df: pd.DataFrame):\n",
    "    res_util = []\n",
    "    for rent_d, rent_t, return_d, return_t, res_day, model_grp in zip(\n",
    "        df['rent_day'], df['rent_time'], df['return_day'], df['return_time'], df['res_day'],\n",
    "        df['res_model_grp']):\n",
    "        \n",
    "        day_hour = timedelta(hours=24)\n",
    "        six_hour = timedelta(hours=6)\n",
    "        date_range = pd.date_range(start=rent_d, end=return_d)    # days of rent periods\n",
    "        date_len = len(date_range)\n",
    "        f = list(map(int, rent_t.split(':'))) \n",
    "        l = list(map(int, return_t.split(':')))   \n",
    "        ft = timedelta(hours=f[0], minutes=f[1])    # time of rent day \n",
    "        lt = timedelta(hours=l[0], minutes=l[1])    # time of return day           \n",
    "        \n",
    "        f_util = 1\n",
    "        l_util = 1 \n",
    "        if (day_hour - ft) < six_hour:\n",
    "            f_util = (day_hour - ft) / six_hour\n",
    "        if lt < six_hour:\n",
    "            l_util = lt / six_hour\n",
    "        \n",
    "        if date_len > 2:\n",
    "            util = np.array(f_util)\n",
    "            util = np.append(util, np.ones(date_len-2))\n",
    "            util = np.append(util, l_util)\n",
    "            \n",
    "        elif date_len == 2:\n",
    "            util = np.array([f_util, l_util])\n",
    "        else :\n",
    "            util = 1\n",
    "            if (lt - ft) < six_hour:\n",
    "                util = (lt - ft) / six_hour\n",
    "            util = np.array([util])     \n",
    "        \n",
    "        res_util.extend(np.array([\n",
    "            date_range,\n",
    "            [res_day] * date_len,\n",
    "            util,\n",
    "            [model_grp] * date_len\n",
    "        ]).T)\n",
    "    \n",
    "    res_util_df = pd.DataFrame(res_util,\n",
    "                            columns=['rent_day', 'res_day', 'util_rate', 'model',])\n",
    "\n",
    "    return res_util_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_util_cnt_df_BAK(df: pd.DataFrame):\n",
    "    res_util = []\n",
    "    for rent_d, rent_t, return_d, return_t, res_day, model_grp in zip(\n",
    "        df['rent_day'], df['rent_time'], df['return_day'], df['return_time'], df['res_day'],\n",
    "        df['res_model_grp']):\n",
    "        \n",
    "        date_range = pd.date_range(start=rent_d, end=return_d)    # days of rent periods\n",
    "        date_len = len(date_range)\n",
    "        f = list(map(int, rent_t.split(':'))) \n",
    "        l = list(map(int, return_t.split(':')))   \n",
    "        ft = timedelta(hours=f[0], minutes=f[1])    # time of rent day \n",
    "        lt = timedelta(hours=l[0], minutes=l[1])    # time of return day           \n",
    "        day_hour = timedelta(hours=24)\n",
    "        \n",
    "        if date_len > 2:\n",
    "            f_util = (day_hour - ft) / day_hour\n",
    "            l_util = lt / day_hour\n",
    "            util = np.array(f_util)\n",
    "            util = np.append(util, np.ones(date_len-2))\n",
    "            util = np.append(util, l_util)\n",
    "            \n",
    "        elif date_len == 2:\n",
    "            f_util = (day_hour - ft) / day_hour\n",
    "            l_util = lt / day_hour\n",
    "            util = np.array([f_util, l_util])\n",
    "        else :\n",
    "            util = (lt - ft) / day_hour\n",
    "            util = np.array([util])    \n",
    "        \n",
    "        res_util.extend(np.array([\n",
    "            date_range,\n",
    "            [res_day] * date_len,\n",
    "            util,\n",
    "            [model_grp] * date_len\n",
    "        ]).T)\n",
    "    \n",
    "    res_util_df = pd.DataFrame(res_util,\n",
    "                            columns=['rent_day', 'res_day', 'util_rate', 'model',])\n",
    "\n",
    "    return res_util_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_curr_util = make_util_cnt_df(df=res_curr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Group by rent & reservation day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_curr_cnt = res_curr_util.groupby(by=['rent_day', 'model', 'res_day']).count()['util_rate']\n",
    "res_curr_cum_cnt = res_curr_cnt.groupby(by=['rent_day', 'model']).cumsum()\n",
    "\n",
    "res_curr_util = res_curr_util.groupby(by=['rent_day', 'model', 'res_day']).sum()['util_rate']\n",
    "res_curr_cum_util = res_curr_util.groupby(by=['rent_day', 'model']).cumsum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert to dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_util = pd.DataFrame({'cum_util_time': res_curr_cum_util,\n",
    "                          'cum_util_cnt': res_curr_cum_cnt},\n",
    "                          index=res_curr_cum_cnt.index)\n",
    "curr_util = curr_util.reset_index(level=(0, 1, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add capacity of model columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_util['month'] = curr_util['rent_day'].dt.strftime('%Y%m')\n",
    "capa_curr_dict = {(month, model): capa for month, model, capa in zip(capa_curr['date'],\n",
    "                                                                     capa_curr['model'], \n",
    "                                                                     capa_curr['capa'])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set capacity for each rent day & model\n",
    "def f_curr(x):\n",
    "    return capa_curr_dict[(x[0], x[1])]\n",
    "curr_util['capa'] = curr_util[['month', 'model']].apply(f_curr, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_util['cum_util_time_rate'] = curr_util['cum_util_time'] / curr_util['capa']\n",
    "curr_util['cum_util_cnt_rate'] = curr_util['cum_util_cnt'] / curr_util['capa']\n",
    "curr_util = curr_util.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add additional columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add lead time\n",
    "curr_util['lead_time'] = curr_util['rent_day'] - curr_util['res_day']\n",
    "curr_util['lead_time'] = curr_util['lead_time'].to_numpy().astype('timedelta64[D]') / np.timedelta64(1, 'D')\n",
    "curr_util['lead_time'] = curr_util['lead_time'] * -1\n",
    "curr_util['lead_time'] = curr_util['lead_time'].astype(int)\n",
    "\n",
    "# Calculate available capacity \n",
    "curr_util['avail_capa'] = curr_util['capa'] - curr_util['cum_util_cnt']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Drop unncessary columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_col = ['res_day', 'month', 'capa']\n",
    "curr_util = curr_util.drop(columns=drop_col, errors='ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Change decimal points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_util['cum_util_time'] = np.round(curr_util['cum_util_time'].to_numpy(), 2)\n",
    "curr_util['cum_util_time_rate'] = np.round(curr_util['cum_util_time_rate'].to_numpy(), 2)\n",
    "curr_util['cum_util_cnt_rate'] = np.round(curr_util['cum_util_cnt_rate'].to_numpy(), 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sava result dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "save_path = os.path.join('..', 'result', 'data', 'reservation')\n",
    "curr_util.to_csv(os.path.join(save_path, 'res_curr_util('+ res_date + ').csv'), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recent dataset: Res. Count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recent reservation dataset\n",
    "res_date = '201113'\n",
    "res_curr_path = os.path.join('..', 'input', 'reservation', 'res_201113.csv')\n",
    "\n",
    "res_data_type = {'예약경로': int,'예약경로명': str, '계약번호': int,  '고객구분': int, \n",
    "                 '고객구분명': str, '총 청구액(VAT포함)': str, '예약모델': str, '예약모델명': str, \n",
    "                 '차급': str, '대여일': str, '대여시간': str, '반납일': str,'반납시간': str, \n",
    "                 '대여기간(일)': int, '대여기간(시간)': int, 'CDW요금': str, '할인유형': str,\n",
    "                 '할인유형명': str, '적용할인명': str, '회원등급': str, '구매목적': str,\n",
    "                 '생성일': str,   '차종': str}\n",
    "\n",
    "res_curr = pd.read_csv(res_curr_path, delimiter='\\t', dtype=res_data_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Rename columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reservation history\n",
    "res_remap_cols ={\n",
    "     '예약경로': 'res_route', '예약경로명': 'res_route_nm', '계약번호': 'res_num', \n",
    "     '고객구분': 'cust_kind', '고객구분명': 'cust_kind_nm', '총 청구액(VAT포함)': 'tot_fee', \n",
    "     '예약모델': 'res_model', '예약모델명': 'res_model_nm', '차급': 'car_grd', \n",
    "     '대여일': 'rent_day', '대여시간': 'rent_time', '반납일': 'return_day', '반납시간': 'return_time',\n",
    "     '대여기간(일)': 'rent_period_day', '대여기간(시간)': 'rent_period_time',\n",
    "     'CDW요금': 'cdw_fee', '할인유형': 'discount_type', '할인유형명': 'discount_type_nm',\n",
    "     '적용할인명': 'applyed_discount', '적용할인율(%)': 'discount_rate', '회원등급': 'member_grd',\n",
    "     '구매목적': 'sale_purpose','생성일': 'res_day', '차종': 'car_kind'}\n",
    "res_curr = res_curr.rename(columns=res_remap_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Drop unnecessary columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_drop_col = ['res_route', 'res_route_nm', 'cust_kind', 'cust_kind_nm', 'tot_fee',\n",
    "                'res_model', 'car_grd', 'rent_time', 'return_day', 'return_time', 'rent_period_day', \n",
    "                'rent_period_time', 'cdw_fee', 'discount_type', 'discount_type_nm', 'sale_purpose', \n",
    "                'applyed_discount', 'discount_rate', 'member_grd', 'sale_purpose', 'car_kind']\n",
    "res_curr = res_curr.drop(columns=res_drop_col, errors='ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Change data types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_curr['rent_day'] = pd.to_datetime(res_curr['rent_day'], format='%Y-%m-%d')\n",
    "res_curr['res_day'] = pd.to_datetime(res_curr['res_day'], format='%Y-%m-%d')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filter dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter only 1.6 grade car group\n",
    "res_curr = res_curr[res_curr['res_model_nm'].isin([\n",
    "    'K3', 'THE NEW K3 (G)', 'ALL NEW K3 (G)',\n",
    "    '아반떼 AD (G)', '아반떼 AD (G) F/L', '올 뉴 아반떼 (G)',\n",
    "    '더 올 뉴 벨로스터 (G)', '쏘울 (G)', '쏘울 부스터 (G)'\n",
    "])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Group data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Car Model group\n",
    "# SOUL 모델은 VELOSTER 모델에 포함해서 분석 (실적 데이터가 적음)\n",
    "conditions = [\n",
    "    res_curr['res_model_nm'].isin(['K3', 'THE NEW K3 (G)', 'ALL NEW K3 (G)']),\n",
    "    res_curr['res_model_nm'].isin(['아반떼 AD (G)', '아반떼 AD (G) F/L', '올 뉴 아반떼 (G)']),\n",
    "    res_curr['res_model_nm'].isin(['더 올 뉴 벨로스터 (G)', '쏘울 (G)', '쏘울 부스터 (G)'])    \n",
    "]\n",
    "values = ['K3', 'AVANTE', 'VELOSTER']\n",
    "\n",
    "res_curr['res_model_grp'] = np.select(conditions, values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_curr = res_curr.drop(columns=['res_model_nm'], errors='ignore')\n",
    "res_curr = res_curr.sort_values(by=['rent_day', 'res_day'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_cnt = res_curr.groupby(by=['rent_day', 'res_model_grp', 'res_day']).count()\n",
    "curr_cnt = res_cnt.groupby(by=['rent_day', 'res_model_grp']).cumsum()\n",
    "curr_cnt = curr_cnt.rename(columns={'res_num': 'res_cum_cnt'})\n",
    "# res_cum_cnt = res_cum_cnt.reset_index(level=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add additional columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_cnt = curr_cnt.reset_index(level=(0, 1, 2))\n",
    "curr_cnt = curr_cnt.drop(columns=['return_day', 'return_time'], errors='ignore')\n",
    "curr_cnt = curr_cnt.rename(columns={'rent_time': 'res_cnt', \n",
    "                                          'res_model_grp': 'model'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add lead time\n",
    "curr_cnt['lead_time'] = curr_cnt['rent_day'] - curr_cnt['res_day']\n",
    "curr_cnt['lead_time'] = curr_cnt['lead_time'].to_numpy().astype('timedelta64[D]') / np.timedelta64(1, 'D')\n",
    "curr_cnt['lead_time'] = curr_cnt['lead_time'] * -1\n",
    "curr_cnt['lead_time'] = curr_cnt['lead_time'].astype(int)\n",
    "\n",
    "curr_cnt = curr_cnt.drop(columns=['res_day'], errors='ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save result dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = os.path.join('..', 'result', 'data', 'reservation')\n",
    "curr_cnt.to_csv(os.path.join(save_path, 'res_curr_cnt('+ res_date + ').csv'), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 1: Predict Jeju Visitors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jeju_visit = pd.read_csv(os.path.join('..', 'input', 'jeju_visit_daily.csv'), delimiter='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Filter the periods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interval: 1 year\n",
    "start_date = 20191101\n",
    "end_date = 20201031\n",
    "\n",
    "jeju_visit = jeju_visit[(jeju_visit['date'] >= start_date) & (jeju_visit['date'] <= end_date)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Split the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# String date convert to datetime\n",
    "jeju_visit['date'] = pd.to_datetime(jeju_visit['date'], format='%Y%m%d')\n",
    "jeju_visit = jeju_visit.set_index('date')\n",
    "\n",
    "# Split dataset to domestic and foreign\n",
    "jeju_dom = jeju_visit[['domestic']]\n",
    "jeju_for = jeju_visit[['foreign']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define Models & Validation Method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Define time series models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ar_model(data, config: dict, pred_steps=0):\n",
    "    \"\"\"\n",
    "    :param data: times series data\n",
    "    :param lags: the number of lags\n",
    "    :param trend: the trend to include in the model\n",
    "            n: No Trend\n",
    "            c: Constant Only\n",
    "            t: Time Trend Only\n",
    "            ct: Constant and time trend\n",
    "    :param seasonal: Flag indicating whether to include seasonal dummies in the model    \n",
    "    :param pred_step: prediction steps\n",
    "    \"\"\"\n",
    "    model = AutoReg(endog=data, lags=config['lags'], trend=config['trend'])\n",
    "    model_fit = model.fit()\n",
    "    yhat = model_fit.predict(start=len(data), end=len(data) + pred_steps-1)\n",
    "    \n",
    "    return yhat\n",
    "    \n",
    "def arima_model(data, config: dict, pred_steps=0):\n",
    "    \"\"\"\n",
    "    :param data: time series data\n",
    "    :param order: (p, d, q)\n",
    "            p: Trend autoregression order\n",
    "            d: Trend difference order\n",
    "            q: Trend moving average order\n",
    "    :param trend: the trend to include in the model\n",
    "            n: No Trend\n",
    "            c: Constant\n",
    "            t: Trend\n",
    "            ct: Constant and Trend\n",
    "    :param freq: frequency of the time series (‘B’, ‘D’, ‘W’, ‘M’, ‘A’, ‘Q)\n",
    "    \"\"\"\n",
    "    model = ARIMA(data, order=config['order'], trend=config['trend'])\n",
    "    model_fit = model.fit()\n",
    "    yhat = model_fit.predict(start=len(data), end=len(data) + pred_steps-1)\n",
    "    \n",
    "    return yhat\n",
    "    \n",
    "def holt_winters_model(data, config: dict, pred_steps=0):\n",
    "    \"\"\"\n",
    "    :param data: time series data\n",
    "    :param trend - type of trend component\n",
    "        ('add', 'mul', 'additive', 'multiplicative')\n",
    "    :param damped_trend - should the trend component be damped\n",
    "    :param seasonal - Type of seasonal component\n",
    "            ('add', 'mul', 'additive', 'multiplicative', None)\n",
    "    :param seasonal_periods - The number of periods in a complete seasonal cycle\n",
    "    :param pred_step:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    model = ExponentialSmoothing(data, trend=config['trend'], damped_trend=config['damped_trend'])\n",
    "    model_fit = model.fit()\n",
    "    yhat = model_fit.predict(start=len(data), end=len(data) + pred_steps-1)\n",
    "    \n",
    "    return yhat\n",
    "\n",
    "time_series_model = {'ar': ar_model,\n",
    "                     'arima': arima_model,\n",
    "                     'hw': holt_winters_model}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Validation: Walk forward validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def walk_forward_validation(model: str, data, n_test, config):\n",
    "    # split dataset\n",
    "    train, test = data[:-n_test], data[-n_test:]\n",
    "    \n",
    "    # calculate \n",
    "    yhat = time_series_model[model](data=train, config=config, pred_steps=n_test)\n",
    "    \n",
    "    # add actual observation to history for the next loop\n",
    "    error = math.sqrt(mean_squared_error(test.values, yhat))\n",
    "    \n",
    "    return error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_series_param_tune(model: str, data, n_test, param_grid: dict):\n",
    "    err_list = list()\n",
    "    for params in list(product(*list(param_grid.values()))):\n",
    "        config = dict()\n",
    "        config = {key: val for key, val in zip(list(param_grid.keys()), params)}\n",
    "        err = walk_forward_validation(model, data, n_test=n_test, config=config)\n",
    "        err_list.append((list(config.items()), round(err, 2)))\n",
    "        \n",
    "    err_list_sorted = sorted(err_list, key=lambda err: err[1])    \n",
    "        \n",
    "    return err_list_sorted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation: Walk Forward Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Set paramter grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paramter Grid\n",
    "ar_param_grid = {\n",
    "    'lags': list(np.arange(1,15,1)),\n",
    "    'trend': ['c', 't', 'ct'] \n",
    "}\n",
    "\n",
    "arima_param_grid = {}\n",
    "\n",
    "hw_param_grid = {\n",
    "    'trend': ['add', 'additive'],\n",
    "    'damped_trend': [True, False] \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Range\n",
    "n_test = 28"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Walk forward validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "err_dom_ar = time_series_param_tune(model='ar', data=jeju_dom, \n",
    "                                    n_test=n_test, param_grid=ar_param_grid)\n",
    "print(f'AR: Best paramters: {err_dom_ar[0][0]}, result: {err_dom_ar[0][1]}')\n",
    "\n",
    "err_dom_hw = time_series_param_tune(model='hw', data=jeju_dom, \n",
    "                                    n_test=n_test, param_grid=hw_param_grid)\n",
    "print(f'HW: Best paramters: {err_dom_hw[0][0]}, result: {err_dom_hw[0][1]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "err_for_ar = time_series_param_tune(model='ar', data=jeju_for, \n",
    "                                    n_test=n_test, param_grid=ar_param_grid)\n",
    "print(f'AR: Best paramters: {err_for_ar[0][0]}, result: {err_for_ar[0][1]}')\n",
    "\n",
    "err_for_hw = time_series_param_tune(model='hw', data=jeju_for, \n",
    "                                    n_test=n_test, param_grid=hw_param_grid)\n",
    "print(f'HW: Best paramters: {err_for_hw[0][0]}, result: {err_for_hw[0][1]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Save best parameter grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = os.path.join('..', 'result', 'model', 'model_1')\n",
    "\n",
    "m1_dom_ar_best_params = {key:val for key, val in err_dom_ar[0][0]}\n",
    "f = open(os.path.join(save_path, 'm1_dom_ar_best_params.pickle'), 'wb')\n",
    "pickle.dump(m1_dom_ar_best_params, f)\n",
    "f.close()\n",
    "\n",
    "m1_for_ar_best_params = {key:val for key, val in err_for_ar[0][0]}\n",
    "f = open(os.path.join(save_path, 'm1_for_ar_best_params.pickle'), 'wb')\n",
    "pickle.dump(m1_for_ar_best_params, f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load paramters and fit the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Load best parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = os.path.join('..', 'result', 'model', 'model_1')\n",
    "f = open(os.path.join(save_path, 'm1_dom_ar_best_params.pickle'), 'rb')\n",
    "m1_dom_ar_best_params = pickle.load(f)\n",
    "f.close()\n",
    "\n",
    "f = open(os.path.join(save_path, 'm1_dom_ar_best_params.pickle'), 'rb')\n",
    "m1_for_ar_best_params = pickle.load(f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Fit the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction periods(3 month)\n",
    "pred_steps = 88 + 31"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Domestic\n",
    "model_dom = AutoReg(endog=jeju_dom, \n",
    "                    lags=m1_dom_ar_best_params['lags'],\n",
    "                    trend=m1_dom_ar_best_params['trend'])\n",
    "model_dom_fit = model_dom.fit()\n",
    "pred_dom = model_dom_fit.predict(start=len(jeju_dom), end=len(jeju_dom) + pred_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Foreign\n",
    "model_for = AutoReg(endog=jeju_for, \n",
    "                    lags=m1_for_ar_best_params['lags'],\n",
    "                    trend=m1_for_ar_best_params['trend'])\n",
    "model_for_fit = model_for.fit()\n",
    "pred_for = model_for_fit.predict(start=len(jeju_for), end=len(jeju_for) + pred_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Predict domestic and foreign"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visit_19_12 = jeju_visit.loc[dt.datetime(2019,12,1):dt.datetime(2019,12,31)]['total']\n",
    "visit_20_01 = jeju_visit.loc[dt.datetime(2020,1,1):dt.datetime(2020,1,31)]['total']\n",
    "visit_20_02 = jeju_visit.loc[dt.datetime(2020,2,1):dt.datetime(2020,2,28)]['total']\n",
    "\n",
    "pred_tot = pred_dom + pred_for\n",
    "\n",
    "pred_20_12 = pred_tot.loc[dt.datetime(2020,12,1):dt.datetime(2020,12,31)]\n",
    "pred_21_01 = pred_tot.loc[dt.datetime(2021,1,1):dt.datetime(2021,1,31)]\n",
    "pred_21_02 = pred_tot.loc[dt.datetime(2021,2,1):dt.datetime(2021,2,28)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Get rate of change on jeju visitors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chg_rate_20_12 = round((pred_20_12.mean() / visit_19_12.mean() -1), 2)\n",
    "chg_rate_21_01 = round((pred_21_01.mean() / visit_20_01.mean() -1), 2)\n",
    "chg_rate_21_02 = round((pred_21_02.mean() / visit_20_02.mean() -1), 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Save Prediction result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dmd_pred = pd.DataFrame({'date': ['202012', '202101', '202102'],\n",
    "                         'dmd_chg': [chg_rate_20_12, chg_rate_21_01 , chg_rate_21_02]})\n",
    "dmd_pred.to_csv(os.path.join('..', 'result', 'data', 'model_1', 'dmd_pred_2012_2102.csv'), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 2: Predict Cum. Res. Counts (Training)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Make training dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_path = os.path.join('..', 'result', 'data', 'model_2')\n",
    "\n",
    "# Load Count dataset\n",
    "res_cnt_av = pd.read_csv(os.path.join(root_path, 'model_2_cnt_av.csv')) \n",
    "res_cnt_k3 = pd.read_csv(os.path.join(root_path, 'model_2_cnt_k3.csv')) \n",
    "res_cnt_vl = pd.read_csv(os.path.join(root_path, 'model_2_cnt_vl.csv')) \n",
    "\n",
    "# Load Utilization dataset\n",
    "res_util_av = pd.read_csv(os.path.join(root_path, 'model_2_util_av.csv')) \n",
    "res_util_k3 = pd.read_csv(os.path.join(root_path, 'model_2_util_k3.csv')) \n",
    "res_util_vl = pd.read_csv(os.path.join(root_path, 'model_2_util_vl.csv')) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Divide into input and output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_col = ['cum_util_time', 'cum_util_cnt', 'capa', 'cum_util_time_rate', 'cum_util_cnt_rate']\n",
    "\n",
    "# Count dataset\n",
    "# Avante\n",
    "m2_cnt_av_x = res_cnt_av.drop(columns=['cum_cnt'])\n",
    "m2_cnt_av_y = res_cnt_av['cum_cnt']\n",
    "\n",
    "# K3\n",
    "m2_cnt_k3_x = res_cnt_k3.drop(columns=['cum_cnt'])\n",
    "m2_cnt_k3_y = res_cnt_k3['cum_cnt'] \n",
    "\n",
    "# Veloster\n",
    "m2_cnt_vl_x = res_cnt_vl.drop(columns=['cum_cnt'])\n",
    "m2_cnt_vl_y = res_cnt_vl['cum_cnt']\n",
    "\n",
    "# Utilization dataset\n",
    "# Avante\n",
    "m2_util_av_x = res_util_av.drop(columns=drop_col)\n",
    "m2_util_av_y = res_util_av['cum_util_time_rate']    # cum_util_day / cum_util_time\n",
    "\n",
    "# K3\n",
    "m2_util_k3_x = res_util_k3.drop(columns=drop_col)\n",
    "m2_util_k3_y = res_util_k3['cum_util_time_rate']    # cum_util_day / cum_util_time\n",
    "\n",
    "# Veloster\n",
    "m2_util_vl_x = res_util_vl.drop(columns=drop_col)\n",
    "m2_util_vl_y = res_util_vl['cum_util_time_rate']    # cum_util_day / cum_util_time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Split train and test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random seed\n",
    "rd_st = 2020\n",
    "\n",
    "# Count dataset\n",
    "cnt_av_x_train, cnt_av_x_test, cnt_av_y_train, cnt_av_y_test = train_test_split(m2_cnt_av_x,\n",
    "                                                                                m2_cnt_av_y, \n",
    "                                                                                test_size=0.2, \n",
    "                                                                                random_state=rd_st, \n",
    "                                                                                shuffle=True)\n",
    "cnt_k3_x_train, cnt_k3_x_test, cnt_k3_y_train, cnt_k3_y_test = train_test_split(m2_cnt_k3_x,\n",
    "                                                                                m2_cnt_k3_y, \n",
    "                                                                                test_size=0.2, \n",
    "                                                                                random_state=rd_st, \n",
    "                                                                                shuffle=True)\n",
    "cnt_vl_x_train, cnt_vl_x_test, cnt_vl_y_train, cnt_vl_y_test = train_test_split(m2_cnt_vl_x,\n",
    "                                                                                m2_cnt_vl_y, \n",
    "                                                                                test_size=0.2, \n",
    "                                                                                random_state=rd_st, \n",
    "                                                                                shuffle=True)\n",
    "# Utilization dataset\n",
    "util_av_x_train, util_av_x_test, util_av_y_train, util_av_y_test = train_test_split(m2_util_av_x,\n",
    "                                                                                    m2_util_av_y, \n",
    "                                                                                    test_size=0.2, \n",
    "                                                                                    random_state=rd_st, \n",
    "                                                                                    shuffle=True)\n",
    "util_k3_x_train, util_k3_x_test, util_k3_y_train, util_k3_y_test = train_test_split(m2_util_k3_x,\n",
    "                                                                                    m2_util_k3_y, \n",
    "                                                                                    test_size=0.2, \n",
    "                                                                                    random_state=rd_st, \n",
    "                                                                                    shuffle=True)\n",
    "util_vl_x_train, util_vl_x_test, util_vl_y_train, util_vl_y_test = train_test_split(m2_util_vl_x,\n",
    "                                                                                    m2_util_vl_y, \n",
    "                                                                                    test_size=0.2, \n",
    "                                                                                    random_state=rd_st, \n",
    "                                                                                    shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pre-test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Pre-test parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparamter of Support Vector Machine \n",
    "param_svm = {\n",
    "    'kernel': 'rbf',    # 'linear', 'poly', 'rbf'\n",
    "    'tol':1e-3, \n",
    "    'max_iter': -1\n",
    "}\n",
    "\n",
    "# Hyperparamter of Extra-tree model\n",
    "param_extra = {\n",
    "    'n_estimators': 100,\n",
    "    'criterion': 'mse',\n",
    "    'max_depth': None,\n",
    "    'min_samples_split': 2,\n",
    "    'max_features': 'auto',\n",
    "    'random_state': 2020\n",
    "}\n",
    "\n",
    "# Hyperparamter of XGboost model\n",
    "xgb_param = {'max_depth': 6, 'eta': 1, 'objective': 'reg:squarederror'}\n",
    "xgb_param['nthread'] = 4\n",
    "xgb_param['eval_metric'] = 'rmse'    # 'rmse', 'auc', 'logloss', 'map'\n",
    "num_round = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Model: SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model\n",
    "regr_svm = svm.SVR(kernel=param_svm['kernel'],\n",
    "                   tol=param_svm['tol'], \n",
    "                   max_iter=param_svm['max_iter'])\n",
    "# Fit model\n",
    "regr_svm.fit(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_svm = regr_svm.predict(X_test_scaled)    # Prediction on test dataset\n",
    "rmse_svm = mean_squared_error(y_test, pred_svm)    # Calculate RMSE\n",
    "pred_svm_df = pd.DataFrame({'real': y_test['cum_cnt'].values, 'prediction': pred_svm})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Model: Extra-trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model\n",
    "regr_extr = ExtraTreesRegressor(n_estimators=param_extra['n_estimators'], \n",
    "                                criterion=param_extra['criterion'],\n",
    "                                max_depth=param_extra['max_depth'],\n",
    "                                min_samples_split=param_extra['min_samples_split'],\n",
    "                                max_features=param_extra['max_features'],\n",
    "                                random_state=param_extra['random_state'])\n",
    "# Fit model\n",
    "regr_extr.fit(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_extr = regr_extr.predict(X_test_scaled)    # Prediction on test dataset\n",
    "rmse_extr = mean_squared_error(y_test, pred_extr)    # Calculate RMSE\n",
    "pred_extr_df = pd.DataFrame({'real':  y_test['cum_cnt'].values, 'prediction': pred_extr})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Model: XGboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make dataset\n",
    "dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "dtest = xgb.DMatrix(X_test, label=y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regr_xgb = xgb.train(xgb_param, dtrain, num_round)\n",
    "regr_xgb.save_model(os.path.join(save_path, 'model', 'xgboost.model'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_xgb = regr_xgb.predict(dtest)\n",
    "rmse_xgb = mean_squared_error(y_test, pred_xgb)\n",
    "pred_xgv_df = pd.DataFrame({'real':  y_test['cum_cnt'].values, 'prediction': pred_xgb}, index=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Compare results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse_svm, rmse_extr, rmse_xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_svm_df = pred_svm_df.sort_values(by=['real'])\n",
    "pred_svm_df = pred_svm_df.reset_index(drop=True)\n",
    "\n",
    "pred_extr_df = pred_extr_df.sort_values(by=['real'])\n",
    "pred_extr_df = pred_extr_df.reset_index(drop=True)\n",
    "\n",
    "pred_xgv_df = pred_xgv_df.sort_values(by=['real'])\n",
    "pred_xgv_df = pred_xgv_df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot SVM model result\n",
    "fig, axes = plt.subplots(1, 1)\n",
    "pred_svm_df['real'].plot.line(figsize=(15,6), linewidth=1.1, \n",
    "                              color='firebrick', label='Real', ax=axes)\n",
    "pred_svm_df['prediction'].plot.line(figsize=(15,6),linewidth=0.9, alpha=0.4,\n",
    "                                     color='coral', label='Prediction', ax=axes)\n",
    "axes.legend()\n",
    "plt.title('Result: SVM')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Extra-trees model result\n",
    "fig, axes = plt.subplots(1, 1)\n",
    "pred_extr_df['real'].plot.line(figsize=(15,6), linewidth=1.1, \n",
    "                              color='firebrick',  label='Real', ax=axes)\n",
    "pred_extr_df['prediction'].plot.line(figsize=(15,6),linewidth=0.9, alpha=0.4,\n",
    "                                     color='coral', label='Prediction', ax=axes)\n",
    "axes.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot XGboost model result\n",
    "fig, axes = plt.subplots(1, 1)\n",
    "pred_xgv_df['real'].plot.line(figsize=(15,6), linewidth=1.1, \n",
    "                              color='firebrick', label='Real', ax=axes)\n",
    "pred_xgv_df['prediction'].plot.line(figsize=(15,6),linewidth=0.9, alpha=0.4,\n",
    "                                     color='coral', label='Prediction', ax=axes)\n",
    "axes.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training: Extra-trees Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Set model & paramter grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Training models\n",
    "regressors = {}\n",
    "regressors.update({\"Extra Trees Regressor\": ExtraTreesRegressor()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameter grid\n",
    "param_grids = {}\n",
    "param_grids.update({\"Extra Trees Regressor\": {\n",
    "    'n_estimators': list(np.arange(100, 500, 100)),\n",
    "    'criterion': ['mse'],\n",
    "    'min_samples_split': list(np.arange(2, 6, 1)),   # minimum number of samples required to split inner node \n",
    "    'min_samples_leaf': list(np.arange(1, 6, 1)),    # have the effect of smoothing the model\n",
    "    'max_features': ['auto']\n",
    "}})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Grid search cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_best_hyper_param(x, y, regr, scoring='neg_root_mean_squared_error'):\n",
    "    # Select regressor algorithm\n",
    "    regressor = regressors[regr]\n",
    "    \n",
    "    # Define parameter grid\n",
    "    param_grid = param_grids[regr]\n",
    "    \n",
    "    # Initialize Grid Search object\n",
    "    gscv = GridSearchCV(estimator=regressor, param_grid=param_grid,\n",
    "                        scoring=scoring, n_jobs=1, cv=5, verbose=1)\n",
    "    \n",
    "    # Fit gscv\n",
    "    print(f'Tuning {regr}')\n",
    "    gscv.fit(x, y)\n",
    "    \n",
    "    # Get best paramters and score\n",
    "    best_params = gscv.best_params_\n",
    "    best_score = gscv.best_score_\n",
    "    \n",
    "    # Update regressor paramters\n",
    "    regressor.set_params(**best_params)\n",
    "    \n",
    "    return regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuning Extra Trees Regressor\n",
      "Fitting 5 folds for each of 80 candidates, totalling 400 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 400 out of 400 | elapsed:  1.7min finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuning Extra Trees Regressor\n",
      "Fitting 5 folds for each of 80 candidates, totalling 400 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 400 out of 400 | elapsed:  1.6min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuning Extra Trees Regressor\n",
      "Fitting 5 folds for each of 80 candidates, totalling 400 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 400 out of 400 | elapsed:  1.6min finished\n"
     ]
    }
   ],
   "source": [
    "# Model for reservation dataset\n",
    "# Best paramter grid for Avante\n",
    "cnt_av_extr_best = get_best_hyper_param(x=cnt_av_x_train, y=cnt_av_y_train.to_numpy(), \n",
    "                                        regr='Extra Trees Regressor')\n",
    "\n",
    "# Best paramter grid for K3\n",
    "cnt_k3_extr_best = get_best_hyper_param(x=cnt_k3_x_train, y=cnt_k3_y_train.to_numpy(), \n",
    "                                        regr='Extra Trees Regressor')\n",
    "\n",
    "# Best paramter grid for Veloster\n",
    "cnt_vl_extr_best = get_best_hyper_param(x=cnt_vl_x_train, y=cnt_vl_y_train.to_numpy(), \n",
    "                                        regr='Extra Trees Regressor')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuning Extra Trees Regressor\n",
      "Fitting 5 folds for each of 80 candidates, totalling 400 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 400 out of 400 | elapsed:  1.7min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuning Extra Trees Regressor\n",
      "Fitting 5 folds for each of 80 candidates, totalling 400 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 400 out of 400 | elapsed:  1.7min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuning Extra Trees Regressor\n",
      "Fitting 5 folds for each of 80 candidates, totalling 400 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 400 out of 400 | elapsed:  1.7min finished\n"
     ]
    }
   ],
   "source": [
    "# Model for utilization dataset\n",
    "# Best paramter grid for Avante\n",
    "util_av_extr_best = get_best_hyper_param(x=util_av_x_train, y=util_av_y_train.to_numpy(), \n",
    "                                        regr='Extra Trees Regressor')\n",
    "\n",
    "# Best paramter grid for K3\n",
    "util_k3_extr_best = get_best_hyper_param(x=util_k3_x_train, y=util_k3_y_train.to_numpy(), \n",
    "                                        regr='Extra Trees Regressor')\n",
    "\n",
    "# Best paramter grid for Veloster\n",
    "util_vl_extr_best = get_best_hyper_param(x=util_vl_x_train, y=util_vl_y_train.to_numpy(), \n",
    "                                        regr='Extra Trees Regressor')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Save best parameter grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save best paramters of extra-trees regressor\n",
    "save_path = os.path.join('..', 'result', 'model', 'model_2')\n",
    "\n",
    "# Model for reservation dataset\n",
    "# Avante\n",
    "cnt_av_extr_best_params = cnt_av_extr_best.get_params()\n",
    "f = open(os.path.join(save_path, \"cnt_av_extr_best_params.pickle\"), \"wb\")\n",
    "pickle.dump(cnt_av_extr_best_params, f)\n",
    "f.close()\n",
    "\n",
    "# K3\n",
    "cnt_k3_extr_best_params= cnt_k3_extr_best.get_params()\n",
    "f = open(os.path.join(save_path, \"cnt_k3_extr_best_params.pickle\"), \"wb\")\n",
    "pickle.dump(cnt_k3_extr_best_params, f)\n",
    "f.close()\n",
    "\n",
    "# Veloster\n",
    "cnt_vl_extr_best_params= cnt_vl_extr_best.get_params()\n",
    "f = open(os.path.join(save_path, \"cnt_vl_extr_best_params.pickle\"), \"wb\")\n",
    "pickle.dump(cnt_vl_extr_best_params, f)\n",
    "f.close()\n",
    "\n",
    "# Model for utilization dataset\n",
    "# Avante\n",
    "util_av_extr_best_params = util_av_extr_best.get_params()\n",
    "f = open(os.path.join(save_path, \"util_av_extr_best_params.pickle\"), \"wb\")\n",
    "pickle.dump(util_av_extr_best_params, f)\n",
    "f.close()\n",
    "\n",
    "# K3\n",
    "util_k3_extr_best_params= util_k3_extr_best.get_params()\n",
    "f = open(os.path.join(save_path, \"util_k3_extr_best_params.pickle\"), \"wb\")\n",
    "pickle.dump(util_k3_extr_best_params, f)\n",
    "f.close()\n",
    "\n",
    "# Veloster\n",
    "util_vl_extr_best_params= util_vl_extr_best.get_params()\n",
    "f = open(os.path.join(save_path, \"util_vl_extr_best_params.pickle\"), \"wb\")\n",
    "pickle.dump(util_vl_extr_best_params, f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 2: Predict Cum. Res. Counts (Prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load dataset & parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_path = os.path.join('..', 'result', 'data', 'model_2')\n",
    "\n",
    "# Load Count dataset\n",
    "res_cnt_av = pd.read_csv(os.path.join(root_path, 'model_2_cnt_av.csv')) \n",
    "res_cnt_k3 = pd.read_csv(os.path.join(root_path, 'model_2_cnt_k3.csv')) \n",
    "res_cnt_vl = pd.read_csv(os.path.join(root_path, 'model_2_cnt_vl.csv')) \n",
    "\n",
    "# Load Utilization dataset\n",
    "res_util_av = pd.read_csv(os.path.join(root_path, 'model_2_util_av.csv')) \n",
    "res_util_k3 = pd.read_csv(os.path.join(root_path, 'model_2_util_k3.csv')) \n",
    "res_util_vl = pd.read_csv(os.path.join(root_path, 'model_2_util_vl.csv')) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Divide into input and output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_col = ['cum_util_time', 'cum_util_cnt', 'capa', 'cum_util_time_rate', 'cum_util_cnt_rate']\n",
    "\n",
    "# Count dataset\n",
    "# Avante\n",
    "m2_cnt_av_x = res_cnt_av.drop(columns=['cum_cnt'])\n",
    "m2_cnt_av_y = res_cnt_av['cum_cnt']\n",
    "\n",
    "# K3\n",
    "m2_cnt_k3_x = res_cnt_k3.drop(columns=['cum_cnt'])\n",
    "m2_cnt_k3_y = res_cnt_k3['cum_cnt'] \n",
    "\n",
    "# Veloster\n",
    "m2_cnt_vl_x = res_cnt_vl.drop(columns=['cum_cnt'])\n",
    "m2_cnt_vl_y = res_cnt_vl['cum_cnt']\n",
    "\n",
    "# Utilization dataset\n",
    "# Avante\n",
    "m2_util_av_x = res_util_av.drop(columns=drop_col)\n",
    "m2_util_av_y = res_util_av['cum_util_time_rate']    # cum_util_day / cum_util_time\n",
    "\n",
    "# K3\n",
    "m2_util_k3_x = res_util_k3.drop(columns=drop_col)\n",
    "m2_util_k3_y = res_util_k3['cum_util_time_rate']    # cum_util_day / cum_util_time\n",
    "\n",
    "# Veloster\n",
    "m2_util_vl_x = res_util_vl.drop(columns=drop_col)\n",
    "m2_util_vl_y = res_util_vl['cum_util_time_rate']    # cum_util_day / cum_util_time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Load best hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model for reservation dataset\n",
    "# Avante\n",
    "load_path = os.path.join('..', 'result', 'model', 'model_2')\n",
    "f = open(os.path.join(load_path, \"cnt_av_extr_best_params.pickle\"), \"rb\")\n",
    "cnt_av_extr_best_params = pickle.load(f)\n",
    "f.close()\n",
    "\n",
    "# K3\n",
    "f = open(os.path.join(load_path, \"cnt_k3_extr_best_params.pickle\"), \"rb\")\n",
    "cnt_k3_extr_best_params = pickle.load(f)\n",
    "f.close()\n",
    "\n",
    "# Veloster\n",
    "f = open(os.path.join(load_path, \"cnt_vl_extr_best_params.pickle\"), \"rb\")\n",
    "cnt_vl_extr_best_params = pickle.load(f)\n",
    "f.close()\n",
    "\n",
    "# Model for utilization dataset\n",
    "# Avante\n",
    "load_path = os.path.join('..', 'result', 'model', 'model_2')\n",
    "f = open(os.path.join(load_path, \"util_av_extr_best_params.pickle\"), \"rb\")\n",
    "util_av_extr_best_params = pickle.load(f)\n",
    "f.close()\n",
    "\n",
    "# K3\n",
    "f = open(os.path.join(load_path, \"util_k3_extr_best_params.pickle\"), \"rb\")\n",
    "util_k3_extr_best_params = pickle.load(f)\n",
    "f.close()\n",
    "\n",
    "# Veloster\n",
    "f = open(os.path.join(load_path, \"util_vl_extr_best_params.pickle\"), \"rb\")\n",
    "util_vl_extr_best_params = pickle.load(f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fit the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ExtraTreesRegressor(n_estimators=400)"
      ]
     },
     "execution_count": 275,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Model for reservation dataset\n",
    "# Avante\n",
    "cnt_av_extr = ExtraTreesRegressor(**cnt_av_extr_best_params)\n",
    "cnt_av_extr.fit(m2_cnt_av_x, m2_cnt_av_y)\n",
    "\n",
    "# K3\n",
    "cnt_k3_extr = ExtraTreesRegressor(**cnt_k3_extr_best_params)\n",
    "cnt_k3_extr.fit(m2_cnt_k3_x, m2_cnt_k3_y)\n",
    "\n",
    "# Veloster\n",
    "cnt_vl_extr = ExtraTreesRegressor(**cnt_vl_extr_best_params)\n",
    "cnt_vl_extr.fit(m2_cnt_vl_x, m2_cnt_vl_y)\n",
    "\n",
    "# Model for utilization dataset\n",
    "# Avante\n",
    "util_av_extr = ExtraTreesRegressor(**util_av_extr_best_params)\n",
    "util_av_extr.fit(m2_util_av_x, m2_util_av_y)\n",
    "\n",
    "# K3\n",
    "util_k3_extr = ExtraTreesRegressor(**util_k3_extr_best_params)\n",
    "util_k3_extr.fit(m2_util_k3_x, m2_util_k3_y)\n",
    "\n",
    "# Veloster\n",
    "util_vl_extr = ExtraTreesRegressor(**util_vl_extr_best_params)\n",
    "util_vl_extr.fit(m2_util_vl_x, m2_util_vl_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Set initial variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial Setting\n",
    "\n",
    "# load recent seasonality dataset\n",
    "load_path = os.path.join('..', 'input', 'seasonality')\n",
    "ss_curr = pd.read_csv(os.path.join(load_path, 'seasonality_curr.csv'), delimiter='\\t')\n",
    "ss_curr['date'] = pd.to_datetime(ss_curr['date'], format='%Y%m%d')\n",
    "day_to_season = {day: season for day, season in zip(ss_curr['date'], ss_curr['seasonality'])}\n",
    "\n",
    "# Initial discount rate for each season\n",
    "load_path = os.path.join('..', 'input', 'discount')\n",
    "dscnt_init = pd.read_csv(os.path.join(load_path, 'discount_init.csv'), delimiter='\\t')\n",
    "dscnt_init['date'] = pd.to_datetime(dscnt_init['date'], format='%Y%m%d')\n",
    "day_to_init_discount = {day: discount for day, discount in zip(dscnt_init['date'], \n",
    "                                                               dscnt_init['discount_init'])}\n",
    "\n",
    "# Initial capacity of each model\n",
    "load_path = os.path.join('..', 'input', 'capa')\n",
    "capa_init = pd.read_csv(os.path.join(load_path, 'capa_curr.csv'), delimiter='\\t', \n",
    "                        dtype={'date': str, 'model': str, 'capa': int})\n",
    "capa_init_dict = {(date, model): capa for date, model, capa in zip(capa_init['date'], \n",
    "                                                   capa_init['model'],\n",
    "                                                   capa_init['capa'])}\n",
    "# Lead Time Setting\n",
    "lead_time = np.arange(-83, 1, 1)\n",
    "lead_time_vec = np.arange(-36, 1, 1)    \n",
    "\n",
    "lt_to_lt_vec = {-1 * i : (((i//7) + 24) * -1 if i > 28 else i * -1) for i in range(0, 84, 1)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Set prediction day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target rent day\n",
    "predict_day = '2020-12-18'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Make dataframe of initial setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get season value and initial discount rate\n",
    "pred_month = predict_day.split('-')[0] + predict_day.split('-')[1]\n",
    "predict_datetime = dt.datetime(*list(map(int, predict_day.split('-'))))    # Convert to datetime\n",
    "season = day_to_season[predict_datetime]    # Get seasonality on the day\n",
    "init_discount = day_to_init_discount[predict_datetime]\n",
    "\n",
    "# Set initial capacity of model\n",
    "init_capa_av = capa_init_dict[(pred_month, 'AVANTE')]\n",
    "init_capa_k3 = capa_init_dict[(pred_month, 'K3')]\n",
    "init_capa_vl = capa_init_dict[(pred_month, 'VELOSTER')]\n",
    "\n",
    "# Inintial settting to dataframe\n",
    "pred_input = pd.DataFrame({'season': season, 'lead_time': lead_time_vec, 'discount': init_discount})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Predict res. count and utilization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reservation Count\n",
    "# Prediction \n",
    "cnt_av_extr_pred = cnt_av_extr.predict(pred_input)    # Avante\n",
    "cnt_k3_extr_pred = cnt_k3_extr.predict(pred_input)    # K3\n",
    "cnt_vl_extr_pred = cnt_vl_extr.predict(pred_input)    # Veloster\n",
    "\n",
    "# Correct decimal point\n",
    "cnt_av_extr_pred = np.round(cnt_av_extr_pred, 3)\n",
    "cnt_k3_extr_pred = np.round(cnt_k3_extr_pred, 3)\n",
    "cnt_vl_extr_pred = np.round(cnt_vl_extr_pred, 3)\n",
    "\n",
    "# Reservation Utilization\n",
    "# Prediction \n",
    "util_av_extr_pred = util_av_extr.predict(pred_input)    # Avante\n",
    "util_k3_extr_pred = util_k3_extr.predict(pred_input)    # K3\n",
    "util_vl_extr_pred = util_vl_extr.predict(pred_input)    # Veloster\n",
    "\n",
    "# Correct decimal point\n",
    "util_av_extr_pred = np.round(util_av_extr_pred, 3)\n",
    "util_k3_extr_pred = np.round(util_k3_extr_pred, 3)\n",
    "util_vl_extr_pred = np.round(util_vl_extr_pred, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reservation Count\n",
    "lt_vec_to_cnt_av = {lt_vec: av for lt_vec, av in zip(lead_time_vec, cnt_av_extr_pred)}\n",
    "lt_vec_to_cnt_k3 = {lt_vec: k3 for lt_vec, k3 in zip(lead_time_vec, cnt_k3_extr_pred)}\n",
    "lt_vec_to_cnt_vl = {lt_vec: vl for lt_vec, vl in zip(lead_time_vec, cnt_vl_extr_pred)}\n",
    "\n",
    "# Reservation Utilization\n",
    "lt_vec_to_util_av = {lt_vec: av for lt_vec, av in zip(lead_time_vec, util_av_extr_pred)}\n",
    "lt_vec_to_util_k3 = {lt_vec: k3 for lt_vec, k3 in zip(lead_time_vec, util_k3_extr_pred)}\n",
    "lt_vec_to_util_vl = {lt_vec: vl for lt_vec, vl in zip(lead_time_vec, util_vl_extr_pred)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make result dataframe\n",
    "exp_res = pd.DataFrame({\n",
    "        'date': [predict_datetime - dt.timedelta(days=int(i*-1)) for i in lead_time],\n",
    "        'lead_time': lead_time,\n",
    "        'curr_discount': init_discount,\n",
    "        'exp_res_cnt_av': [lt_vec_to_cnt_av[lt_to_lt_vec[i]] for i in lead_time],\n",
    "        'exp_util_time_av': [lt_vec_to_util_av[lt_to_lt_vec[i]] * init_capa_av for i in lead_time],\n",
    "        'exp_util_rate_av': [lt_vec_to_util_av[lt_to_lt_vec[i]] for i in lead_time],\n",
    "        'exp_res_cnt_k3': [lt_vec_to_cnt_k3[lt_to_lt_vec[i]] for i in lead_time],\n",
    "        'exp_util_time_k3': [lt_vec_to_util_k3[lt_to_lt_vec[i]] * init_capa_k3 for i in lead_time],\n",
    "        'exp_util_rate_k3': [lt_vec_to_util_k3[lt_to_lt_vec[i]] for i in lead_time],\n",
    "        'exp_res_cnt_vl': [lt_vec_to_cnt_vl[lt_to_lt_vec[i]] for i in lead_time],\n",
    "        'exp_util_time_vl': [lt_vec_to_util_vl[lt_to_lt_vec[i]] * init_capa_vl for i in lead_time],\n",
    "        'exp_util_rate_vl': [lt_vec_to_util_vl[lt_to_lt_vec[i]] for i in lead_time]\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_res['exp_util_rate_av'] = exp_res['exp_util_rate_av']  * 100\n",
    "exp_res['exp_util_rate_k3'] = exp_res['exp_util_rate_k3']  * 100\n",
    "exp_res['exp_util_rate_vl'] = exp_res['exp_util_rate_vl']  * 100\n",
    "\n",
    "exp_res['exp_util_time_av'] = np.round(exp_res['exp_util_time_av'].to_numpy(), 1)\n",
    "exp_res['exp_util_time_k3'] = np.round(exp_res['exp_util_time_k3'].to_numpy(), 1)\n",
    "exp_res['exp_util_time_vl'] = np.round(exp_res['exp_util_time_vl'].to_numpy(), 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Save result dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path =  os.path.join('..', 'result', 'data', 'prediction')\n",
    "exp_res.to_csv(os.path.join(save_path, 'original', 'res_pred(' + predict_day + ').csv'), index=False)\n",
    "exp_res.T.to_csv(os.path.join(save_path, 'transpose', 'res_pred_T(' + predict_day + ').csv'), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 3: Recommend Best Price"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define Discount Recommendation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [],
   "source": [
    "# theta2 : Season 및 lead time 별 할인율 조정 전략에 관한 기준 예약률 산출\n",
    "# 과거 season 및 lead time 별 예약률 트렌드 데이터를 기준으로 산출\n",
    "def calc_tehta2(curr_exp_cnt: int, exp_res_cnt: int, exp_capa: int):\n",
    "    return round((curr_exp_cnt + exp_res_cnt) / exp_capa, 2)\n",
    "\n",
    "def dscnt_rec_fun(curr_util: float, exp_util: float, d: float):\n",
    "    \"\"\"\n",
    "    Customized Exponential function\n",
    "    curr_util: Current Utilization Rate\n",
    "    d: Exp Demand Change Rate \n",
    "    ex_util: Exp. Utilization rate\n",
    "    \"\"\"\n",
    "    # Hyperparamters (need to tune)\n",
    "    theta1 = 1       # ratio of increasing / decreasing magnitude : discount\n",
    "    theta2 = 0.05    # ratio of increasing / decreasing magnitude : demand\n",
    "    phi_low = 1.7    # 1 < phi_low < phi_high < 2\n",
    "    phi_high = 1.2   # 1 < phi_low < phi_high < 2\n",
    "\n",
    "    if d > 0:\n",
    "        y = 1 - theta1 * (curr_util ** (phi_high ** (-1 * (curr_util * theta2 * d))) - exp_util)\n",
    "    else:\n",
    "        y = 1 - theta1 * (curr_util ** (phi_low ** (-1 * ((1 - curr_util) * theta2 * d))) - exp_util)\n",
    "        \n",
    "    return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [],
   "source": [
    "# recent resrvation update day\n",
    "update_day = '201113'\n",
    "\n",
    "# Define prediction day\n",
    "predict_day = '2020-12-16'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 463,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_month = list(predict_day.split('-'))[0] + list(predict_day.split('-'))[1]\n",
    "\n",
    "# Load Exp. Cum. reservation counts\n",
    "load_path =  os.path.join('..', 'result', 'data', 'prediction', 'original')\n",
    "exp_res = pd.read_csv(os.path.join(load_path, 'res_pred(' + predict_day + ').csv'))\n",
    "\n",
    "# Load current reservation counts\n",
    "load_path = os.path.join('..', 'result', 'data', 'reservation')\n",
    "curr_res_cnt = pd.read_csv(os.path.join(load_path, 'res_curr_cnt(' + update_day + ').csv'))\n",
    "curr_res_cnt = curr_res_cnt[curr_res_cnt['rent_day'] == predict_day]\n",
    "\n",
    "# Load recent reservation util rate\n",
    "load_path = os.path.join('..', 'result', 'data', 'reservation')\n",
    "curr_res_util = pd.read_csv(os.path.join(load_path, 'res_curr_util(' + update_day + ').csv'))\n",
    "curr_res_util = curr_res_util[curr_res_util['rent_day'] == predict_day]\n",
    "\n",
    "# Load history seasonality  \n",
    "load_path = os.path.join('..', 'input', 'seasonality')\n",
    "seasonality_hx = pd.read_csv(os.path.join(load_path, 'seasonality_hx.csv'))\n",
    "\n",
    "# Load current seasonality \n",
    "load_path = os.path.join('..', 'input', 'seasonality')\n",
    "seasonality_curr = pd.read_csv(os.path.join(load_path, 'seasonality_curr.csv'), delimiter='\\t')\n",
    "\n",
    "# Capacity history of car models\n",
    "capa_curr_path = os.path.join('..', 'input', 'capa', 'capa_curr.csv')\n",
    "capa_curr = pd.read_csv(capa_curr_path, delimiter='\\t', dtype={'date':str, 'model':str, 'capa':int})\n",
    "capa_curr_dict = {(month, model): capa for month, model, capa in zip(capa_curr['date'],\n",
    "                                                                     capa_curr['model'], \n",
    "                                                                     capa_curr['capa'])}\n",
    "\n",
    "# Load capacity of jeju 1.6 grade cars \n",
    "load_path = os.path.join('..', 'input')\n",
    "jeju_capa = pd.read_csv(os.path.join(load_path, 'jeju_1.6_capa.csv'), delimiter='\\t')\n",
    "\n",
    "# Load demand change prediction of jeju visitors\n",
    "load_path = os.path.join('..', 'result', 'data', 'model_1')\n",
    "dmd_pred = pd.read_csv(os.path.join(load_path, 'dmd_pred_2012_2102.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Rescaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 464,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_res['exp_res_cnt_av'] = np.round(exp_res['exp_res_cnt_av'].to_numpy(), 1)\n",
    "exp_res['exp_res_cnt_k3'] = np.round(exp_res['exp_res_cnt_k3'].to_numpy(), 1)\n",
    "exp_res['exp_res_cnt_vl'] = np.round(exp_res['exp_res_cnt_vl'].to_numpy(), 1)\n",
    "\n",
    "curr_res_util['cum_util_time_rate'] = curr_res_util['cum_util_time_rate'] * 100\n",
    "curr_res_util['cum_util_cnt_rate'] = curr_res_util['cum_util_cnt_rate'] * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split by each model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 465,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reservation dataset\n",
    "curr_res_cnt_av = curr_res_cnt[curr_res_cnt['model'] == 'AVANTE']\n",
    "curr_res_cnt_k3 = curr_res_cnt[curr_res_cnt['model'] == 'K3']\n",
    "curr_res_cnt_vl = curr_res_cnt[curr_res_cnt['model'] == 'VELOSTER']\n",
    "\n",
    "# Utilization dataset\n",
    "curr_res_util_av = curr_res_util[curr_res_util['model'] == 'AVANTE']\n",
    "curr_res_util_k3 = curr_res_util[curr_res_util['model'] == 'K3']\n",
    "curr_res_util_vl = curr_res_util[curr_res_util['model'] == 'VELOSTER']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Drop columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 466,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reservation dataset\n",
    "curr_res_cnt_av = curr_res_cnt_av.drop(columns=['rent_day' ,'model'], errors='ignore')\n",
    "curr_res_cnt_k3 = curr_res_cnt_k3.drop(columns=['rent_day' ,'model'], errors='ignore')\n",
    "curr_res_cnt_vl = curr_res_cnt_vl.drop(columns=['rent_day' ,'model'], errors='ignore')\n",
    "\n",
    "# Utilization dataset\n",
    "drop_cols = ['rent_day', 'model', 'cum_util_cnt_rate']\n",
    "curr_res_util_av = curr_res_util_av.drop(columns=drop_cols, errors='ignore')\n",
    "curr_res_util_k3 = curr_res_util_k3.drop(columns=drop_cols, errors='ignore')\n",
    "curr_res_util_vl = curr_res_util_vl.drop(columns=drop_cols, errors='ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Rename columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 467,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reservation dataset\n",
    "curr_res_cnt_av = curr_res_cnt_av.rename(columns={'res_cum_cnt': 'curr_cnt_av'})\n",
    "curr_res_cnt_k3 = curr_res_cnt_k3.rename(columns={'res_cum_cnt': 'curr_cnt_k3'})\n",
    "curr_res_cnt_vl = curr_res_cnt_vl.rename(columns={'res_cum_cnt': 'curr_cnt_vl'})\n",
    "\n",
    "# Utilization dataset\n",
    "rename_cols = {'cum_util_time': 'curr_util_time_av', 'cum_util_cnt': 'curr_util_cnt_av',\n",
    "              'cum_util_time_rate': 'curr_util_rate_av', 'avail_capa': 'avail_capa_av'}\n",
    "curr_res_util_av = curr_res_util_av.rename(columns=rename_cols)\n",
    "\n",
    "rename_cols = {'cum_util_time': 'curr_util_time_k3', 'cum_util_cnt': 'curr_util_cnt_k3',\n",
    "              'cum_util_time_rate': 'curr_util_rate_k3', 'avail_capa': 'avail_capa_k3'}\n",
    "curr_res_util_k3 = curr_res_util_k3.rename(columns=rename_cols)\n",
    "\n",
    "rename_cols = {'cum_util_time': 'curr_util_time_vl', 'cum_util_cnt': 'curr_util_cnt_vl',\n",
    "              'cum_util_time_rate': 'curr_util_rate_vl', 'avail_capa': 'avail_capa_vl'}\n",
    "curr_res_util_vl = curr_res_util_vl.rename(columns=rename_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Merge dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 468,
   "metadata": {},
   "outputs": [],
   "source": [
    "seasonality = pd.concat([seasonality_hx, seasonality_curr])\n",
    "seasonality_dict = {day: season for day, season in zip()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 469,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reservation dataset\n",
    "exp_res = pd.merge(exp_res, curr_res_cnt_av, how='left', on='lead_time', \n",
    "                   left_index=True, right_index=False)\n",
    "exp_res = pd.merge(exp_res, curr_res_cnt_k3, how='left', on='lead_time', \n",
    "                   left_index=True, right_index=False)\n",
    "exp_res = pd.merge(exp_res, curr_res_cnt_vl, how='left', on='lead_time', \n",
    "                   left_index=True, right_index=False)\n",
    "\n",
    "# Utilization dataset\n",
    "exp_res = pd.merge(exp_res, curr_res_util_av, how='left', on='lead_time', \n",
    "                   left_index=True, right_index=False)\n",
    "exp_res = pd.merge(exp_res, curr_res_util_k3, how='left', on='lead_time', \n",
    "                   left_index=True, right_index=False)\n",
    "exp_res = pd.merge(exp_res, curr_res_util_vl, how='left', on='lead_time', \n",
    "                   left_index=True, right_index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fill NA values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 470,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_res = exp_res.fillna(method='ffill', axis=0)\n",
    "exp_res['avail_capa_av'] = exp_res['avail_capa_av'].fillna(capa_curr_dict[(pred_month, 'AVANTE')])\n",
    "exp_res['avail_capa_k3'] = exp_res['avail_capa_k3'].fillna(capa_curr_dict[(pred_month, 'K3')])\n",
    "exp_res['avail_capa_vl'] = exp_res['avail_capa_vl'].fillna(capa_curr_dict[(pred_month, 'VELOSTER')])\n",
    "exp_res = exp_res.fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get pred. demand change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 471,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Exp. Cum. reservation counts\n",
    "exp_res['date'] = pd.to_datetime(exp_res['date'], format='%Y-%m-%d')\n",
    "\n",
    "# Demand change prediction\n",
    "dmd_pred['date'] = pd.to_datetime(dmd_pred['date'], format='%Y%m')\n",
    "\n",
    "# set predicted demand change\n",
    "predict_datetime = dt.datetime(*list(map(int, predict_day.split('-'))))\n",
    "exp_dmd_chg = 0\n",
    "for date, dmd_chg in zip(dmd_pred['date'], dmd_pred['dmd_chg']):\n",
    "    if (predict_datetime >= date) and (predict_datetime < date + relativedelta(months=1)):\n",
    "        exp_dmd_chg = dmd_chg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filter datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 472,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_res = exp_res[exp_res['date'] >= dt.datetime(2020,11,16)]\n",
    "exp_res['date'] = exp_res['date'].apply(lambda x: dt.datetime.strftime(x, '%Y-%m-%d'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Recommend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 473,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rec(x):\n",
    "    return dscnt_rec_fun(curr_util=x[0], exp_util=x[1], d=exp_dmd_chg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 474,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_res['rec_av'] = exp_res[['curr_util_rate_av', 'exp_util_rate_av']].apply(get_rec, axis=1)\n",
    "exp_res['rec_k3'] = exp_res[['curr_util_rate_k3', 'exp_util_rate_k3']].apply(get_rec, axis=1)               \n",
    "exp_res['rec_vl'] = exp_res[['curr_util_rate_vl', 'exp_util_rate_vl']].apply(get_rec, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Apply recommend change of discount rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 475,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_res['rec_dscnt_av'] = exp_res['curr_discount'] * (1 + exp_res['rec_av'] / 100)\n",
    "exp_res['rec_dscnt_k3'] = exp_res['curr_discount'] * (1 + exp_res['rec_k3'] / 100)\n",
    "exp_res['rec_dscnt_vl'] = exp_res['curr_discount'] * (1 +exp_res['rec_vl'] / 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 476,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_res = exp_res.drop(columns=['rec_av', 'rec_k3', 'rec_vl'], errors='ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Apply discount Policy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 477,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Limit discount\n",
    "exp_res['rec_dscnt_av'] = np.where(exp_res['rec_dscnt_av'] > 80, 80, exp_res['rec_dscnt_av'])\n",
    "exp_res['rec_dscnt_k3'] = np.where(exp_res['rec_dscnt_k3'] > 80, 80, exp_res['rec_dscnt_k3'])\n",
    "exp_res['rec_dscnt_vl'] = np.where(exp_res['rec_dscnt_vl'] > 80, 80, exp_res['rec_dscnt_vl'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 478,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_to_five_times(discount):\n",
    "    if discount % 5 >= 2.5:\n",
    "        return (discount//5 + 1) * 5\n",
    "    else:\n",
    "        return (discount//5) * 5\n",
    "vfunc = np.vectorize(conv_to_five_times)  \n",
    "\n",
    "exp_res['rec_dscnt_av'] = vfunc(exp_res['rec_dscnt_av'].to_numpy())\n",
    "exp_res['rec_dscnt_k3'] = vfunc(exp_res['rec_dscnt_k3'].to_numpy())\n",
    "exp_res['rec_dscnt_vl'] = vfunc(exp_res['rec_dscnt_vl'].to_numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 479,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_res_av = exp_res[['date', 'lead_time', 'curr_discount', \n",
    "                      'curr_cnt_av', 'exp_res_cnt_av', \n",
    "                      'curr_util_time_av', 'curr_util_rate_av', 'avail_capa_av',\n",
    "                      'exp_util_time_av', 'exp_util_rate_av', 'rec_dscnt_av']]\n",
    "\n",
    "exp_res_k3 = exp_res[['date', 'lead_time', 'curr_discount',\n",
    "                      'curr_cnt_k3', 'exp_res_cnt_k3', \n",
    "                      'curr_util_time_k3', 'curr_util_rate_k3', 'avail_capa_k3',\n",
    "                      'exp_util_time_k3', 'exp_util_rate_k3', 'rec_dscnt_k3']]\n",
    "\n",
    "exp_res_vl = exp_res[['date', 'lead_time', 'curr_discount', \n",
    "                      'curr_cnt_vl', 'exp_res_cnt_vl', \n",
    "                      'curr_util_time_vl', 'curr_util_rate_vl', 'avail_capa_vl',\n",
    "                      'exp_util_time_vl', 'exp_util_rate_vl', 'rec_dscnt_vl']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Rename Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 480,
   "metadata": {},
   "outputs": [],
   "source": [
    "rename_col = {'date': '날짜', 'lead_time': '리드타임', 'curr_discount': '현재 할인율',\n",
    "              'curr_cnt_av': '현재 예약건수', 'exp_res_cnt_av': '기대 예약건수', \n",
    "              'curr_util_time_av': '현재 가동대수',\n",
    "              'exp_util_time_av': '기대 가동대수', 'curr_util_rate_av': '현재 가동률',\n",
    "              'avail_capa_av': '예약가능대수', 'exp_util_cnt_av': '기대 가동대수',\n",
    "              'exp_util_rate_av': '기대 가동률', 'rec_dscnt_av': '추천할인율',\n",
    "              'curr_cnt_k3': '현재 예약건수', 'exp_res_cnt_k3': '기대 예약건수',\n",
    "              'curr_util_time_k3': '현재 가동대수',\n",
    "              'exp_util_time_k3': '기대 가동대수', 'curr_util_rate_k3': '현재 가동률',\n",
    "              'avail_capa_k3': '예약가능대수', 'exp_util_cnt_k3': '기대 가동대수',\n",
    "              'exp_util_rate_k3': '기대 가동률', 'rec_dscnt_k3': '추천할인율',\n",
    "              'curr_cnt_vl': '현재 예약건수', 'exp_res_cnt_vl': '기대 예약건수',\n",
    "              'curr_util_time_vl': '현재 가동대수',\n",
    "              'exp_util_time_vl': '기대 가동대수', 'curr_util_rate_vl': '현재 가동률',\n",
    "              'avail_capa_vl': '예약가능대수', 'exp_util_cnt_vl': '기대 가동대수',\n",
    "              'exp_util_rate_vl': '기대 가동률', 'rec_dscnt_vl': '추천할인율'}\n",
    "exp_res_av = exp_res_av.rename(columns=rename_col)\n",
    "exp_res_k3 = exp_res_k3.rename(columns=rename_col)\n",
    "exp_res_vl = exp_res_vl.rename(columns=rename_col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save Recommendation Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 481,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path =  os.path.join('..', 'result', 'data', 'recommend')\n",
    "\n",
    "# AVANTE\n",
    "exp_res_av.to_csv(os.path.join(save_path, 'original', 'dscnt_rec_av(' + predict_day + ').csv'),\n",
    "                 index=False, encoding='euc-kr')\n",
    "exp_res_av.T.to_csv(os.path.join(save_path, 'transpose', 'dscnt_rec_av_T(' + predict_day + ').csv'), \n",
    "                 header=False, encoding='euc-kr')\n",
    "# K3\n",
    "exp_res_k3.to_csv(os.path.join(save_path, 'original', 'dscnt_rec_k3(' + predict_day + ').csv'),\n",
    "                 index=False, encoding='euc-kr')\n",
    "exp_res_k3.T.to_csv(os.path.join(save_path, 'transpose', 'dscnt_rec_k3_T(' + predict_day + ').csv'), \n",
    "                 header=False, encoding='euc-kr')\n",
    "# VELOSTER\n",
    "exp_res_vl.to_csv(os.path.join(save_path, 'original', 'dscnt_rec_vl(' + predict_day + ').csv'),\n",
    "                 index=False, encoding='euc-kr')\n",
    "exp_res_vl.T.to_csv(os.path.join(save_path, 'transpose', 'dscnt_rec_vl_T(' + predict_day + ').csv'), \n",
    "                 header=False, encoding='euc-kr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 2 : Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [],
   "source": [
    "def m2_pred(pred_day: str):\n",
    "    predict_day = pred_day\n",
    "    # Get season value and initial discount rate\n",
    "    pred_month = predict_day.split('-')[0] + predict_day.split('-')[1]\n",
    "    predict_datetime = dt.datetime(*list(map(int, predict_day.split('-'))))    # Convert to datetime\n",
    "    season = day_to_season[predict_datetime]    # Get seasonality on the day\n",
    "    init_discount = day_to_init_discount[predict_datetime]\n",
    "\n",
    "    # Set initial capacity of model\n",
    "    init_capa_av = capa_init_dict[(pred_month, 'AVANTE')]\n",
    "    init_capa_k3 = capa_init_dict[(pred_month, 'K3')]\n",
    "    init_capa_vl = capa_init_dict[(pred_month, 'VELOSTER')]\n",
    "\n",
    "    # Inintial settting to dataframe\n",
    "    pred_input = pd.DataFrame({'season': season, 'lead_time': lead_time_vec, 'discount': init_discount})\n",
    "\n",
    "    # Reservation Count\n",
    "    # Prediction \n",
    "    cnt_av_extr_pred = cnt_av_extr.predict(pred_input)    # Avante\n",
    "    cnt_k3_extr_pred = cnt_k3_extr.predict(pred_input)    # K3\n",
    "    cnt_vl_extr_pred = cnt_vl_extr.predict(pred_input)    # Veloster\n",
    "\n",
    "    # Correct decimal point\n",
    "    cnt_av_extr_pred = np.round(cnt_av_extr_pred, 3)\n",
    "    cnt_k3_extr_pred = np.round(cnt_k3_extr_pred, 3)\n",
    "    cnt_vl_extr_pred = np.round(cnt_vl_extr_pred, 3)\n",
    "\n",
    "    # Reservation Utilization\n",
    "    # Prediction \n",
    "    util_av_extr_pred = util_av_extr.predict(pred_input)    # Avante\n",
    "    util_k3_extr_pred = util_k3_extr.predict(pred_input)    # K3\n",
    "    util_vl_extr_pred = util_vl_extr.predict(pred_input)    # Veloster\n",
    "\n",
    "    # Correct decimal point\n",
    "    util_av_extr_pred = np.round(util_av_extr_pred, 3)\n",
    "    util_k3_extr_pred = np.round(util_k3_extr_pred, 3)\n",
    "    util_vl_extr_pred = np.round(util_vl_extr_pred, 3)\n",
    "\n",
    "\n",
    "    # Reservation Count\n",
    "    lt_vec_to_cnt_av = {lt_vec: av for lt_vec, av in zip(lead_time_vec, cnt_av_extr_pred)}\n",
    "    lt_vec_to_cnt_k3 = {lt_vec: k3 for lt_vec, k3 in zip(lead_time_vec, cnt_k3_extr_pred)}\n",
    "    lt_vec_to_cnt_vl = {lt_vec: vl for lt_vec, vl in zip(lead_time_vec, cnt_vl_extr_pred)}\n",
    "\n",
    "    # Reservation Utilization\n",
    "    lt_vec_to_util_av = {lt_vec: av for lt_vec, av in zip(lead_time_vec, util_av_extr_pred)}\n",
    "    lt_vec_to_util_k3 = {lt_vec: k3 for lt_vec, k3 in zip(lead_time_vec, util_k3_extr_pred)}\n",
    "    lt_vec_to_util_vl = {lt_vec: vl for lt_vec, vl in zip(lead_time_vec, util_vl_extr_pred)}\n",
    "\n",
    "\n",
    "    # Make result dataframe\n",
    "    exp_res = pd.DataFrame({\n",
    "            'date': [predict_datetime - dt.timedelta(days=int(i*-1)) for i in lead_time],\n",
    "            'lead_time': lead_time,\n",
    "            'curr_discount': init_discount,\n",
    "            'exp_res_cnt_av': [lt_vec_to_cnt_av[lt_to_lt_vec[i]] for i in lead_time],\n",
    "            'exp_util_time_av': [lt_vec_to_util_av[lt_to_lt_vec[i]] * init_capa_av for i in lead_time],\n",
    "            'exp_util_rate_av': [lt_vec_to_util_av[lt_to_lt_vec[i]] for i in lead_time],\n",
    "            'exp_res_cnt_k3': [lt_vec_to_cnt_k3[lt_to_lt_vec[i]] for i in lead_time],\n",
    "            'exp_util_time_k3': [lt_vec_to_util_k3[lt_to_lt_vec[i]] * init_capa_k3 for i in lead_time],\n",
    "            'exp_util_rate_k3': [lt_vec_to_util_k3[lt_to_lt_vec[i]] for i in lead_time],\n",
    "            'exp_res_cnt_vl': [lt_vec_to_cnt_vl[lt_to_lt_vec[i]] for i in lead_time],\n",
    "            'exp_util_time_vl': [lt_vec_to_util_vl[lt_to_lt_vec[i]] * init_capa_vl for i in lead_time],\n",
    "            'exp_util_rate_vl': [lt_vec_to_util_vl[lt_to_lt_vec[i]] for i in lead_time]\n",
    "    })\n",
    "\n",
    "    exp_res['exp_util_rate_av'] = exp_res['exp_util_rate_av']  * 100\n",
    "    exp_res['exp_util_rate_k3'] = exp_res['exp_util_rate_k3']  * 100\n",
    "    exp_res['exp_util_rate_vl'] = exp_res['exp_util_rate_vl']  * 100\n",
    "\n",
    "    exp_res['exp_util_time_av'] = np.round(exp_res['exp_util_time_av'].to_numpy(), 1)\n",
    "    exp_res['exp_util_time_k3'] = np.round(exp_res['exp_util_time_k3'].to_numpy(), 1)\n",
    "    exp_res['exp_util_time_vl'] = np.round(exp_res['exp_util_time_vl'].to_numpy(), 1)\n",
    "\n",
    "\n",
    "    save_path =  os.path.join('..', 'result', 'data', 'prediction')\n",
    "    exp_res.to_csv(os.path.join(save_path, 'original', 'res_pred(' + predict_day + ').csv'), index=False)\n",
    "    exp_res.T.to_csv(os.path.join(save_path, 'transpose', 'res_pred_T(' + predict_day + ').csv'), index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [],
   "source": [
    "jan = pd.date_range(start='2021/01/01', end='2021/01/31', freq='D')\n",
    "jan = pd.Series(jan).dt.strftime('%Y-%m-%d')\n",
    "\n",
    "feb = pd.date_range(start='2021/02/01', end='2021/02/10', freq='D')\n",
    "feb = pd.Series(feb).dt.strftime('%Y-%m-%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [],
   "source": [
    "for day in jan:\n",
    "    m2_pred(pred_day=day)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [],
   "source": [
    "for day in feb:\n",
    "    m2_pred(pred_day=day)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 3 Recommendation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 483,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rec(x):\n",
    "    return dscnt_rec_fun(curr_util=x[0], exp_util=x[1], d=exp_dmd_chg)\n",
    "\n",
    "def conv_to_five_times(discount):\n",
    "    if discount % 5 >= 2.5:\n",
    "        return (discount//5 + 1) * 5\n",
    "    else:\n",
    "        return (discount//5) * 5\n",
    "    \n",
    "vfunc = np.vectorize(conv_to_five_times)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Default setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 484,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path =  os.path.join('..', 'result', 'data', 'recommend')\n",
    "\n",
    "# Load history seasonality  \n",
    "load_path = os.path.join('..', 'input', 'seasonality')\n",
    "seasonality_hx = pd.read_csv(os.path.join(load_path, 'seasonality_hx.csv'))\n",
    "\n",
    "# Load current seasonality \n",
    "load_path = os.path.join('..', 'input', 'seasonality')\n",
    "seasonality_curr = pd.read_csv(os.path.join(load_path, 'seasonality_curr.csv'), delimiter='\\t')\n",
    "\n",
    "# Capacity history of car models\n",
    "capa_curr_path = os.path.join('..', 'input', 'capa', 'capa_curr.csv')\n",
    "capa_curr = pd.read_csv(capa_curr_path, delimiter='\\t', dtype={'date':str, 'model':str, 'capa':int})\n",
    "capa_curr_dict = {(month, model): capa for month, model, capa in zip(capa_curr['date'],\n",
    "                                                                     capa_curr['model'], \n",
    "                                                                     capa_curr['capa'])}\n",
    "\n",
    "# Load capacity of jeju 1.6 grade cars \n",
    "load_path = os.path.join('..', 'input')\n",
    "jeju_capa = pd.read_csv(os.path.join(load_path, 'jeju_1.6_capa.csv'), delimiter='\\t')\n",
    "\n",
    "# Load demand change prediction of jeju visitors\n",
    "load_path = os.path.join('..', 'result', 'data', 'model_1')\n",
    "dmd_pred = pd.read_csv(os.path.join(load_path, 'dmd_pred_2012_2102.csv'))\n",
    "\n",
    "seasonality = pd.concat([seasonality_hx, seasonality_curr])\n",
    "seasonality_dict = {day: season for day, season in zip()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 485,
   "metadata": {},
   "outputs": [],
   "source": [
    "def m3_rec(predict_day: str):\n",
    "    \n",
    "    pred_month = list(predict_day.split('-'))[0] + list(predict_day.split('-'))[1]\n",
    "\n",
    "    # Load Exp. Cum. reservation counts\n",
    "    load_path =  os.path.join('..', 'result', 'data', 'prediction', 'original')\n",
    "    exp_res = pd.read_csv(os.path.join(load_path, 'res_pred(' + predict_day + ').csv'))\n",
    "\n",
    "    # Load current reservation counts\n",
    "    load_path = os.path.join('..', 'result', 'data', 'reservation')\n",
    "    curr_res_cnt = pd.read_csv(os.path.join(load_path, 'res_curr_cnt(' + update_day + ').csv'))\n",
    "\n",
    "    # Load recent reservation util rate\n",
    "    load_path = os.path.join('..', 'result', 'data', 'reservation')\n",
    "    curr_res_util = pd.read_csv(os.path.join(load_path, 'res_curr_util(' + update_day + ').csv'))\n",
    "\n",
    "\n",
    "    curr_res_cnt = curr_res_cnt[curr_res_cnt['rent_day'] == predict_day]\n",
    "    curr_res_util = curr_res_util[curr_res_util['rent_day'] == predict_day]\n",
    "\n",
    "    exp_res['exp_res_cnt_av'] = np.round(exp_res['exp_res_cnt_av'].to_numpy(), 1)\n",
    "    exp_res['exp_res_cnt_k3'] = np.round(exp_res['exp_res_cnt_k3'].to_numpy(), 1)\n",
    "    exp_res['exp_res_cnt_vl'] = np.round(exp_res['exp_res_cnt_vl'].to_numpy(), 1)\n",
    "\n",
    "    curr_res_util['cum_util_time_rate'] = curr_res_util['cum_util_time_rate'] * 100\n",
    "    curr_res_util['cum_util_cnt_rate'] = curr_res_util['cum_util_cnt_rate'] * 100\n",
    "\n",
    "\n",
    "    # Reservation dataset\n",
    "    curr_res_cnt_av = curr_res_cnt[curr_res_cnt['model'] == 'AVANTE']\n",
    "    curr_res_cnt_k3 = curr_res_cnt[curr_res_cnt['model'] == 'K3']\n",
    "    curr_res_cnt_vl = curr_res_cnt[curr_res_cnt['model'] == 'VELOSTER']\n",
    "\n",
    "    # Utilization dataset\n",
    "    curr_res_util_av = curr_res_util[curr_res_util['model'] == 'AVANTE']\n",
    "    curr_res_util_k3 = curr_res_util[curr_res_util['model'] == 'K3']\n",
    "    curr_res_util_vl = curr_res_util[curr_res_util['model'] == 'VELOSTER']\n",
    "\n",
    "\n",
    "    # Reservation dataset\n",
    "    curr_res_cnt_av = curr_res_cnt_av.drop(columns=['rent_day' ,'model'], errors='ignore')\n",
    "    curr_res_cnt_k3 = curr_res_cnt_k3.drop(columns=['rent_day' ,'model'], errors='ignore')\n",
    "    curr_res_cnt_vl = curr_res_cnt_vl.drop(columns=['rent_day' ,'model'], errors='ignore')\n",
    "\n",
    "    # Utilization dataset\n",
    "    drop_cols = ['rent_day', 'model', 'cum_util_cnt_rate']\n",
    "    curr_res_util_av = curr_res_util_av.drop(columns=drop_cols, errors='ignore')\n",
    "    curr_res_util_k3 = curr_res_util_k3.drop(columns=drop_cols, errors='ignore')\n",
    "    curr_res_util_vl = curr_res_util_vl.drop(columns=drop_cols, errors='ignore')\n",
    "\n",
    "\n",
    "    # Reservation dataset\n",
    "    curr_res_cnt_av = curr_res_cnt_av.rename(columns={'res_cum_cnt': 'curr_cnt_av'})\n",
    "    curr_res_cnt_k3 = curr_res_cnt_k3.rename(columns={'res_cum_cnt': 'curr_cnt_k3'})\n",
    "    curr_res_cnt_vl = curr_res_cnt_vl.rename(columns={'res_cum_cnt': 'curr_cnt_vl'})\n",
    "\n",
    "    # Utilization dataset\n",
    "    rename_cols = {'cum_util_time': 'curr_util_time_av', 'cum_util_cnt': 'curr_util_cnt_av',\n",
    "                  'cum_util_time_rate': 'curr_util_rate_av', 'avail_capa': 'avail_capa_av'}\n",
    "    curr_res_util_av = curr_res_util_av.rename(columns=rename_cols)\n",
    "\n",
    "    rename_cols = {'cum_util_time': 'curr_util_time_k3', 'cum_util_cnt': 'curr_util_cnt_k3',\n",
    "                  'cum_util_time_rate': 'curr_util_rate_k3', 'avail_capa': 'avail_capa_k3'}\n",
    "    curr_res_util_k3 = curr_res_util_k3.rename(columns=rename_cols)\n",
    "\n",
    "    rename_cols = {'cum_util_time': 'curr_util_time_vl', 'cum_util_cnt': 'curr_util_cnt_vl',\n",
    "                  'cum_util_time_rate': 'curr_util_rate_vl', 'avail_capa': 'avail_capa_vl'}\n",
    "    curr_res_util_vl = curr_res_util_vl.rename(columns=rename_cols)\n",
    "\n",
    "\n",
    "    # Reservation dataset\n",
    "    exp_res = pd.merge(exp_res, curr_res_cnt_av, how='left', on='lead_time', \n",
    "                       left_index=True, right_index=False)\n",
    "    exp_res = pd.merge(exp_res, curr_res_cnt_k3, how='left', on='lead_time', \n",
    "                       left_index=True, right_index=False)\n",
    "    exp_res = pd.merge(exp_res, curr_res_cnt_vl, how='left', on='lead_time', \n",
    "                       left_index=True, right_index=False)\n",
    "\n",
    "    # Utilization dataset\n",
    "    exp_res = pd.merge(exp_res, curr_res_util_av, how='left', on='lead_time', \n",
    "                       left_index=True, right_index=False)\n",
    "    exp_res = pd.merge(exp_res, curr_res_util_k3, how='left', on='lead_time', \n",
    "                       left_index=True, right_index=False)\n",
    "    exp_res = pd.merge(exp_res, curr_res_util_vl, how='left', on='lead_time', \n",
    "                       left_index=True, right_index=False)\n",
    "\n",
    "\n",
    "    exp_res = exp_res.fillna(method='ffill', axis=0)\n",
    "    exp_res['avail_capa_av'] = exp_res['avail_capa_av'].fillna(capa_curr_dict[(pred_month, 'AVANTE')])\n",
    "    exp_res['avail_capa_k3'] = exp_res['avail_capa_k3'].fillna(capa_curr_dict[(pred_month, 'K3')])\n",
    "    exp_res['avail_capa_vl'] = exp_res['avail_capa_vl'].fillna(capa_curr_dict[(pred_month, 'VELOSTER')])\n",
    "    exp_res = exp_res.fillna(0)\n",
    "\n",
    "    #Exp. Cum. reservation counts\n",
    "    exp_res['date'] = pd.to_datetime(exp_res['date'], format='%Y-%m-%d')\n",
    "\n",
    "    # Demand change prediction\n",
    "    dmd_pred['date'] = pd.to_datetime(dmd_pred['date'], format='%Y%m')\n",
    "\n",
    "    # set predicted demand change\n",
    "    predict_datetime = dt.datetime(*list(map(int, predict_day.split('-'))))\n",
    "    exp_dmd_chg = 0\n",
    "    for date, dmd_chg in zip(dmd_pred['date'], dmd_pred['dmd_chg']):\n",
    "        if (predict_datetime >= date) and (predict_datetime < date + relativedelta(months=1)):\n",
    "            exp_dmd_chg = dmd_chg\n",
    "\n",
    "    exp_res = exp_res[exp_res['date'] >= dt.datetime(2020,11,16)]\n",
    "    exp_res['date'] = exp_res['date'].apply(lambda x: dt.datetime.strftime(x, '%Y-%m-%d'))\n",
    "\n",
    "    exp_res['rec_av'] = exp_res[['curr_util_rate_av', 'exp_util_rate_av']].apply(get_rec, axis=1)\n",
    "    exp_res['rec_k3'] = exp_res[['curr_util_rate_k3', 'exp_util_rate_k3']].apply(get_rec, axis=1)               \n",
    "    exp_res['rec_vl'] = exp_res[['curr_util_rate_vl', 'exp_util_rate_vl']].apply(get_rec, axis=1)\n",
    "\n",
    "    exp_res['rec_dscnt_av'] = exp_res['curr_discount'] * (1 + exp_res['rec_av'] / 100)\n",
    "    exp_res['rec_dscnt_k3'] = exp_res['curr_discount'] * (1 + exp_res['rec_k3'] / 100)\n",
    "    exp_res['rec_dscnt_vl'] = exp_res['curr_discount'] * (1 +exp_res['rec_vl'] / 100)\n",
    "\n",
    "    exp_res = exp_res.drop(columns=['rec_av', 'rec_k3', 'rec_vl'], errors='ignore')\n",
    "\n",
    "    # Limit discount\n",
    "    exp_res['rec_dscnt_av'] = np.where(exp_res['rec_dscnt_av'] > 80, 80, exp_res['rec_dscnt_av'])\n",
    "    exp_res['rec_dscnt_k3'] = np.where(exp_res['rec_dscnt_k3'] > 80, 80, exp_res['rec_dscnt_k3'])\n",
    "    exp_res['rec_dscnt_vl'] = np.where(exp_res['rec_dscnt_vl'] > 80, 80, exp_res['rec_dscnt_vl'])\n",
    "\n",
    "    exp_res['rec_dscnt_av'] = vfunc(exp_res['rec_dscnt_av'].to_numpy())\n",
    "    exp_res['rec_dscnt_k3'] = vfunc(exp_res['rec_dscnt_k3'].to_numpy())\n",
    "    exp_res['rec_dscnt_vl'] = vfunc(exp_res['rec_dscnt_vl'].to_numpy())\n",
    "\n",
    "    exp_res_av = exp_res[['date', 'lead_time', 'curr_discount', \n",
    "                          'curr_cnt_av', 'exp_res_cnt_av', \n",
    "                          'curr_util_time_av', 'curr_util_rate_av', 'avail_capa_av',\n",
    "                          'exp_util_time_av', 'exp_util_rate_av', 'rec_dscnt_av']]\n",
    "\n",
    "    exp_res_k3 = exp_res[['date', 'lead_time', 'curr_discount',\n",
    "                          'curr_cnt_k3', 'exp_res_cnt_k3', \n",
    "                          'curr_util_time_k3', 'curr_util_rate_k3', 'avail_capa_k3',\n",
    "                          'exp_util_time_k3', 'exp_util_rate_k3', 'rec_dscnt_k3']]\n",
    "\n",
    "    exp_res_vl = exp_res[['date', 'lead_time', 'curr_discount', \n",
    "                          'curr_cnt_vl', 'exp_res_cnt_vl', \n",
    "                          'curr_util_time_vl', 'curr_util_rate_vl', 'avail_capa_vl',\n",
    "                          'exp_util_time_vl', 'exp_util_rate_vl', 'rec_dscnt_vl']]\n",
    "\n",
    "    rename_col = {'date': '날짜', 'lead_time': '리드타임', 'curr_discount': '현재 할인율',\n",
    "                  'curr_cnt_av': '현재 예약건수', 'exp_res_cnt_av': '기대 예약건수', \n",
    "                  'curr_util_time_av': '현재 가동대수',\n",
    "                  'exp_util_time_av': '기대 가동대수', 'curr_util_rate_av': '현재 가동률',\n",
    "                  'avail_capa_av': '예약가능대수', 'exp_util_cnt_av': '기대 가동대수',\n",
    "                  'exp_util_rate_av': '기대 가동률', 'rec_dscnt_av': '추천할인율',\n",
    "                  'curr_cnt_k3': '현재 예약건수', 'exp_res_cnt_k3': '기대 예약건수',\n",
    "                  'curr_util_time_k3': '현재 가동대수',\n",
    "                  'exp_util_time_k3': '기대 가동대수', 'curr_util_rate_k3': '현재 가동률',\n",
    "                  'avail_capa_k3': '예약가능대수', 'exp_util_cnt_k3': '기대 가동대수',\n",
    "                  'exp_util_rate_k3': '기대 가동률', 'rec_dscnt_k3': '추천할인율',\n",
    "                  'curr_cnt_vl': '현재 예약건수', 'exp_res_cnt_vl': '기대 예약건수',\n",
    "                  'curr_util_time_vl': '현재 가동대수',\n",
    "                  'exp_util_time_vl': '기대 가동대수', 'curr_util_rate_vl': '현재 가동률',\n",
    "                  'avail_capa_vl': '예약가능대수', 'exp_util_cnt_vl': '기대 가동대수',\n",
    "                  'exp_util_rate_vl': '기대 가동률', 'rec_dscnt_vl': '추천할인율'}\n",
    "    exp_res_av = exp_res_av.rename(columns=rename_col)\n",
    "    exp_res_k3 = exp_res_k3.rename(columns=rename_col)\n",
    "    exp_res_vl = exp_res_vl.rename(columns=rename_col)\n",
    "\n",
    "\n",
    "\n",
    "    # AVANTE\n",
    "    exp_res_av.to_csv(os.path.join(save_path, 'original', 'dscnt_rec_av(' + predict_day + ').csv'),\n",
    "                     index=False, encoding='euc-kr')\n",
    "    exp_res_av.T.to_csv(os.path.join(save_path, 'transpose', 'dscnt_rec_av_T(' + predict_day + ').csv'), \n",
    "                     header=False, encoding='euc-kr')\n",
    "    # K3\n",
    "    exp_res_k3.to_csv(os.path.join(save_path, 'original', 'dscnt_rec_k3(' + predict_day + ').csv'),\n",
    "                     index=False, encoding='euc-kr')\n",
    "    exp_res_k3.T.to_csv(os.path.join(save_path, 'transpose', 'dscnt_rec_k3_T(' + predict_day + ').csv'), \n",
    "                     header=False, encoding='euc-kr')\n",
    "    # VELOSTER\n",
    "    exp_res_vl.to_csv(os.path.join(save_path, 'original', 'dscnt_rec_vl(' + predict_day + ').csv'),\n",
    "                     index=False, encoding='euc-kr')\n",
    "    exp_res_vl.T.to_csv(os.path.join(save_path, 'transpose', 'dscnt_rec_vl_T(' + predict_day + ').csv'), \n",
    "                     header=False, encoding='euc-kr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 488,
   "metadata": {},
   "outputs": [],
   "source": [
    "# recent resrvation update day\n",
    "update_day = '201113'\n",
    "\n",
    "dec = pd.date_range(start='2020/12/17', end='2020/12/31', freq='D')\n",
    "dec = pd.Series(dec).dt.strftime('%Y-%m-%d')\n",
    "\n",
    "jan = pd.date_range(start='2021/01/01', end='2021/01/31', freq='D')\n",
    "jan = pd.Series(jan).dt.strftime('%Y-%m-%d')\n",
    "\n",
    "feb = pd.date_range(start='2021/02/01', end='2021/02/10', freq='D')\n",
    "feb = pd.Series(feb).dt.strftime('%Y-%m-%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 489,
   "metadata": {},
   "outputs": [],
   "source": [
    "for day in dec:\n",
    "    m3_rec(day)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 490,
   "metadata": {},
   "outputs": [],
   "source": [
    "for day in jan:\n",
    "    m3_rec(day)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 491,
   "metadata": {},
   "outputs": [],
   "source": [
    "for day in feb:\n",
    "    m3_rec(day)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "349.091px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
