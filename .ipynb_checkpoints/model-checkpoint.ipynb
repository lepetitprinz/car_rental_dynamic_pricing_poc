{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Import-Dataset\" data-toc-modified-id=\"Import-Dataset-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Import Dataset</a></span><ul class=\"toc-item\"><li><span><a href=\"#Define-File-Path\" data-toc-modified-id=\"Define-File-Path-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Define File Path</a></span></li><li><span><a href=\"#Load-File\" data-toc-modified-id=\"Load-File-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>Load File</a></span></li><li><span><a href=\"#API-Data\" data-toc-modified-id=\"API-Data-1.3\"><span class=\"toc-item-num\">1.3&nbsp;&nbsp;</span>API Data</a></span></li></ul></li><li><span><a href=\"#Data-Preprocessing\" data-toc-modified-id=\"Data-Preprocessing-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Data Preprocessing</a></span><ul class=\"toc-item\"><li><span><a href=\"#Merge-Dataset\" data-toc-modified-id=\"Merge-Dataset-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Merge Dataset</a></span></li><li><span><a href=\"#Remap-Columns\" data-toc-modified-id=\"Remap-Columns-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>Remap Columns</a></span></li><li><span><a href=\"#Check-NA-Values\" data-toc-modified-id=\"Check-NA-Values-2.3\"><span class=\"toc-item-num\">2.3&nbsp;&nbsp;</span>Check NA Values</a></span></li><li><span><a href=\"#Fill-Na-values\" data-toc-modified-id=\"Fill-Na-values-2.4\"><span class=\"toc-item-num\">2.4&nbsp;&nbsp;</span>Fill Na values</a></span></li><li><span><a href=\"#Change-Data-Types\" data-toc-modified-id=\"Change-Data-Types-2.5\"><span class=\"toc-item-num\">2.5&nbsp;&nbsp;</span>Change Data Types</a></span></li><li><span><a href=\"#Filter-Dataset\" data-toc-modified-id=\"Filter-Dataset-2.6\"><span class=\"toc-item-num\">2.6&nbsp;&nbsp;</span>Filter Dataset</a></span></li><li><span><a href=\"#Data-Grouping\" data-toc-modified-id=\"Data-Grouping-2.7\"><span class=\"toc-item-num\">2.7&nbsp;&nbsp;</span>Data Grouping</a></span></li><li><span><a href=\"#Drop-Columns\" data-toc-modified-id=\"Drop-Columns-2.8\"><span class=\"toc-item-num\">2.8&nbsp;&nbsp;</span>Drop Columns</a></span></li><li><span><a href=\"#Ordering\" data-toc-modified-id=\"Ordering-2.9\"><span class=\"toc-item-num\">2.9&nbsp;&nbsp;</span>Ordering</a></span></li></ul></li><li><span><a href=\"#Data-Preprocessing-for-each-dataset\" data-toc-modified-id=\"Data-Preprocessing-for-each-dataset-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Data Preprocessing for each dataset</a></span><ul class=\"toc-item\"><li><span><a href=\"#Reservation-Dataset\" data-toc-modified-id=\"Reservation-Dataset-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>Reservation Dataset</a></span></li><li><span><a href=\"#Jeju-Visit-Dataset\" data-toc-modified-id=\"Jeju-Visit-Dataset-3.2\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>Jeju Visit Dataset</a></span></li><li><span><a href=\"#Discount-Dataset\" data-toc-modified-id=\"Discount-Dataset-3.3\"><span class=\"toc-item-num\">3.3&nbsp;&nbsp;</span>Discount Dataset</a></span></li><li><span><a href=\"#Merge-Dataset\" data-toc-modified-id=\"Merge-Dataset-3.4\"><span class=\"toc-item-num\">3.4&nbsp;&nbsp;</span>Merge Dataset</a></span></li><li><span><a href=\"#Add-seasonality-column\" data-toc-modified-id=\"Add-seasonality-column-3.5\"><span class=\"toc-item-num\">3.5&nbsp;&nbsp;</span>Add seasonality column</a></span></li><li><span><a href=\"#Add-Lead-Time-Interval\" data-toc-modified-id=\"Add-Lead-Time-Interval-3.6\"><span class=\"toc-item-num\">3.6&nbsp;&nbsp;</span>Add Lead Time Interval</a></span></li><li><span><a href=\"#Convert-Rent-Periods-to-hours\" data-toc-modified-id=\"Convert-Rent-Periods-to-hours-3.7\"><span class=\"toc-item-num\">3.7&nbsp;&nbsp;</span>Convert Rent Periods to hours</a></span></li><li><span><a href=\"#Update-Discount-Rate\" data-toc-modified-id=\"Update-Discount-Rate-3.8\"><span class=\"toc-item-num\">3.8&nbsp;&nbsp;</span>Update Discount Rate</a></span></li><li><span><a href=\"#Calculate-reservation-fee\" data-toc-modified-id=\"Calculate-reservation-fee-3.9\"><span class=\"toc-item-num\">3.9&nbsp;&nbsp;</span>Calculate reservation fee</a></span></li><li><span><a href=\"#Remove-Exception\" data-toc-modified-id=\"Remove-Exception-3.10\"><span class=\"toc-item-num\">3.10&nbsp;&nbsp;</span>Remove Exception</a></span></li><li><span><a href=\"#Save-preprocessed-dataset\" data-toc-modified-id=\"Save-preprocessed-dataset-3.11\"><span class=\"toc-item-num\">3.11&nbsp;&nbsp;</span>Save preprocessed dataset</a></span></li></ul></li><li><span><a href=\"#Model-Input\" data-toc-modified-id=\"Model-Input-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Model Input</a></span><ul class=\"toc-item\"><li><span><a href=\"#Model-2:-Cum.-Reservation-Counts\" data-toc-modified-id=\"Model-2:-Cum.-Reservation-Counts-4.1\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;</span>Model 2: Cum. Reservation Counts</a></span><ul class=\"toc-item\"><li><span><a href=\"#Slicing\" data-toc-modified-id=\"Slicing-4.1.1\"><span class=\"toc-item-num\">4.1.1&nbsp;&nbsp;</span>Slicing</a></span></li><li><span><a href=\"#Grouping\" data-toc-modified-id=\"Grouping-4.1.2\"><span class=\"toc-item-num\">4.1.2&nbsp;&nbsp;</span>Grouping</a></span></li><li><span><a href=\"#Merging\" data-toc-modified-id=\"Merging-4.1.3\"><span class=\"toc-item-num\">4.1.3&nbsp;&nbsp;</span>Merging</a></span></li><li><span><a href=\"#Re-group\" data-toc-modified-id=\"Re-group-4.1.4\"><span class=\"toc-item-num\">4.1.4&nbsp;&nbsp;</span>Re-group</a></span></li><li><span><a href=\"#Save-Dataset\" data-toc-modified-id=\"Save-Dataset-4.1.5\"><span class=\"toc-item-num\">4.1.5&nbsp;&nbsp;</span>Save Dataset</a></span></li></ul></li><li><span><a href=\"#Model-2:-Cum.-Reservation-Counts-by-model\" data-toc-modified-id=\"Model-2:-Cum.-Reservation-Counts-by-model-4.2\"><span class=\"toc-item-num\">4.2&nbsp;&nbsp;</span>Model 2: Cum. Reservation Counts by model</a></span><ul class=\"toc-item\"><li><span><a href=\"#Slicing\" data-toc-modified-id=\"Slicing-4.2.1\"><span class=\"toc-item-num\">4.2.1&nbsp;&nbsp;</span>Slicing</a></span></li><li><span><a href=\"#Grouping\" data-toc-modified-id=\"Grouping-4.2.2\"><span class=\"toc-item-num\">4.2.2&nbsp;&nbsp;</span>Grouping</a></span></li><li><span><a href=\"#Merging\" data-toc-modified-id=\"Merging-4.2.3\"><span class=\"toc-item-num\">4.2.3&nbsp;&nbsp;</span>Merging</a></span></li><li><span><a href=\"#Re-group\" data-toc-modified-id=\"Re-group-4.2.4\"><span class=\"toc-item-num\">4.2.4&nbsp;&nbsp;</span>Re-group</a></span></li><li><span><a href=\"#Divide-into-each-car-model\" data-toc-modified-id=\"Divide-into-each-car-model-4.2.5\"><span class=\"toc-item-num\">4.2.5&nbsp;&nbsp;</span>Divide into each car model</a></span></li><li><span><a href=\"#Save-dataset\" data-toc-modified-id=\"Save-dataset-4.2.6\"><span class=\"toc-item-num\">4.2.6&nbsp;&nbsp;</span>Save dataset</a></span></li></ul></li><li><span><a href=\"#Input----Reservation-Counts\" data-toc-modified-id=\"Input----Reservation-Counts-4.3\"><span class=\"toc-item-num\">4.3&nbsp;&nbsp;</span>Input  - Reservation Counts</a></span><ul class=\"toc-item\"><li><span><a href=\"#Drop-Unnecessary-columns\" data-toc-modified-id=\"Drop-Unnecessary-columns-4.3.1\"><span class=\"toc-item-num\">4.3.1&nbsp;&nbsp;</span>Drop Unnecessary columns</a></span></li><li><span><a href=\"#Data-Split\" data-toc-modified-id=\"Data-Split-4.3.2\"><span class=\"toc-item-num\">4.3.2&nbsp;&nbsp;</span>Data Split</a></span></li><li><span><a href=\"#Save--Dataset\" data-toc-modified-id=\"Save--Dataset-4.3.3\"><span class=\"toc-item-num\">4.3.3&nbsp;&nbsp;</span>Save  Dataset</a></span></li></ul></li></ul></li><li><span><a href=\"#Model-1:-Predict-Jeju-Visitors\" data-toc-modified-id=\"Model-1:-Predict-Jeju-Visitors-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Model 1: Predict Jeju Visitors</a></span><ul class=\"toc-item\"><li><span><a href=\"#Load-dataset\" data-toc-modified-id=\"Load-dataset-5.1\"><span class=\"toc-item-num\">5.1&nbsp;&nbsp;</span>Load dataset</a></span></li><li><span><a href=\"#Preprocessing\" data-toc-modified-id=\"Preprocessing-5.2\"><span class=\"toc-item-num\">5.2&nbsp;&nbsp;</span>Preprocessing</a></span><ul class=\"toc-item\"><li><span><a href=\"#Filter-the-periods\" data-toc-modified-id=\"Filter-the-periods-5.2.1\"><span class=\"toc-item-num\">5.2.1&nbsp;&nbsp;</span>Filter the periods</a></span></li><li><span><a href=\"#Split-dataset\" data-toc-modified-id=\"Split-dataset-5.2.2\"><span class=\"toc-item-num\">5.2.2&nbsp;&nbsp;</span>Split dataset</a></span></li></ul></li><li><span><a href=\"#Define-Models-&amp;-Validation-Method\" data-toc-modified-id=\"Define-Models-&amp;-Validation-Method-5.3\"><span class=\"toc-item-num\">5.3&nbsp;&nbsp;</span>Define Models &amp; Validation Method</a></span><ul class=\"toc-item\"><li><span><a href=\"#Define-Time-Series-Models\" data-toc-modified-id=\"Define-Time-Series-Models-5.3.1\"><span class=\"toc-item-num\">5.3.1&nbsp;&nbsp;</span>Define Time Series Models</a></span></li><li><span><a href=\"#Validation:-Walk-forward-validation\" data-toc-modified-id=\"Validation:-Walk-forward-validation-5.3.2\"><span class=\"toc-item-num\">5.3.2&nbsp;&nbsp;</span>Validation: Walk forward validation</a></span></li></ul></li><li><span><a href=\"#Evaluation:-Walk-Forward-Validation\" data-toc-modified-id=\"Evaluation:-Walk-Forward-Validation-5.4\"><span class=\"toc-item-num\">5.4&nbsp;&nbsp;</span>Evaluation: Walk Forward Validation</a></span><ul class=\"toc-item\"><li><span><a href=\"#Set-parameter-grid\" data-toc-modified-id=\"Set-parameter-grid-5.4.1\"><span class=\"toc-item-num\">5.4.1&nbsp;&nbsp;</span>Set parameter grid</a></span></li><li><span><a href=\"#Walk-forward-validation\" data-toc-modified-id=\"Walk-forward-validation-5.4.2\"><span class=\"toc-item-num\">5.4.2&nbsp;&nbsp;</span>Walk forward validation</a></span></li><li><span><a href=\"#Save-best-paramter-grid\" data-toc-modified-id=\"Save-best-paramter-grid-5.4.3\"><span class=\"toc-item-num\">5.4.3&nbsp;&nbsp;</span>Save best paramter grid</a></span></li></ul></li><li><span><a href=\"#Load-Parameters-and-fit-the-model\" data-toc-modified-id=\"Load-Parameters-and-fit-the-model-5.5\"><span class=\"toc-item-num\">5.5&nbsp;&nbsp;</span>Load Parameters and fit the model</a></span><ul class=\"toc-item\"><li><span><a href=\"#Load-best-parameter\" data-toc-modified-id=\"Load-best-parameter-5.5.1\"><span class=\"toc-item-num\">5.5.1&nbsp;&nbsp;</span>Load best parameter</a></span></li><li><span><a href=\"#Fit-the-model\" data-toc-modified-id=\"Fit-the-model-5.5.2\"><span class=\"toc-item-num\">5.5.2&nbsp;&nbsp;</span>Fit the model</a></span></li></ul></li><li><span><a href=\"#Prediction\" data-toc-modified-id=\"Prediction-5.6\"><span class=\"toc-item-num\">5.6&nbsp;&nbsp;</span>Prediction</a></span></li></ul></li><li><span><a href=\"#Model-2-:-Predict-Cum.-Reservation-Counts\" data-toc-modified-id=\"Model-2-:-Predict-Cum.-Reservation-Counts-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>Model 2 : Predict Cum. Reservation Counts</a></span><ul class=\"toc-item\"><li><span><a href=\"#Make-training-dataset\" data-toc-modified-id=\"Make-training-dataset-6.1\"><span class=\"toc-item-num\">6.1&nbsp;&nbsp;</span>Make training dataset</a></span><ul class=\"toc-item\"><li><span><a href=\"#Load-Dataset\" data-toc-modified-id=\"Load-Dataset-6.1.1\"><span class=\"toc-item-num\">6.1.1&nbsp;&nbsp;</span>Load Dataset</a></span></li><li><span><a href=\"#Divide-into-input-and-output-dataset\" data-toc-modified-id=\"Divide-into-input-and-output-dataset-6.1.2\"><span class=\"toc-item-num\">6.1.2&nbsp;&nbsp;</span>Divide into input and output dataset</a></span></li><li><span><a href=\"#Split-train-and-test-dataset\" data-toc-modified-id=\"Split-train-and-test-dataset-6.1.3\"><span class=\"toc-item-num\">6.1.3&nbsp;&nbsp;</span>Split train and test dataset</a></span></li></ul></li><li><span><a href=\"#Pre-test\" data-toc-modified-id=\"Pre-test-6.2\"><span class=\"toc-item-num\">6.2&nbsp;&nbsp;</span>Pre-test</a></span><ul class=\"toc-item\"><li><span><a href=\"#Pre-test-paramters\" data-toc-modified-id=\"Pre-test-paramters-6.2.1\"><span class=\"toc-item-num\">6.2.1&nbsp;&nbsp;</span>Pre-test paramters</a></span></li><li><span><a href=\"#Model:-SVM\" data-toc-modified-id=\"Model:-SVM-6.2.2\"><span class=\"toc-item-num\">6.2.2&nbsp;&nbsp;</span>Model: SVM</a></span></li><li><span><a href=\"#Model:-Extra-trees\" data-toc-modified-id=\"Model:-Extra-trees-6.2.3\"><span class=\"toc-item-num\">6.2.3&nbsp;&nbsp;</span>Model: Extra-trees</a></span></li><li><span><a href=\"#Model:-XGboost\" data-toc-modified-id=\"Model:-XGboost-6.2.4\"><span class=\"toc-item-num\">6.2.4&nbsp;&nbsp;</span>Model: XGboost</a></span></li><li><span><a href=\"#Comparing-results\" data-toc-modified-id=\"Comparing-results-6.2.5\"><span class=\"toc-item-num\">6.2.5&nbsp;&nbsp;</span>Comparing results</a></span></li></ul></li><li><span><a href=\"#Training-:-Extra-trees-Model\" data-toc-modified-id=\"Training-:-Extra-trees-Model-6.3\"><span class=\"toc-item-num\">6.3&nbsp;&nbsp;</span>Training : Extra-trees Model</a></span><ul class=\"toc-item\"><li><span><a href=\"#Set-model-and-parameter-grids\" data-toc-modified-id=\"Set-model-and-parameter-grids-6.3.1\"><span class=\"toc-item-num\">6.3.1&nbsp;&nbsp;</span>Set model and parameter grids</a></span></li><li><span><a href=\"#Grid-Search-Cross-Validation\" data-toc-modified-id=\"Grid-Search-Cross-Validation-6.3.2\"><span class=\"toc-item-num\">6.3.2&nbsp;&nbsp;</span>Grid Search Cross Validation</a></span></li><li><span><a href=\"#Save-best-parameter-grid\" data-toc-modified-id=\"Save-best-parameter-grid-6.3.3\"><span class=\"toc-item-num\">6.3.3&nbsp;&nbsp;</span>Save best parameter grid</a></span></li></ul></li><li><span><a href=\"#Load-paramters-and-fit-the-model\" data-toc-modified-id=\"Load-paramters-and-fit-the-model-6.4\"><span class=\"toc-item-num\">6.4&nbsp;&nbsp;</span>Load paramters and fit the model</a></span><ul class=\"toc-item\"><li><span><a href=\"#Load-best-hyperparameters\" data-toc-modified-id=\"Load-best-hyperparameters-6.4.1\"><span class=\"toc-item-num\">6.4.1&nbsp;&nbsp;</span>Load best hyperparameters</a></span></li><li><span><a href=\"#Fit-the-model\" data-toc-modified-id=\"Fit-the-model-6.4.2\"><span class=\"toc-item-num\">6.4.2&nbsp;&nbsp;</span>Fit the model</a></span></li></ul></li><li><span><a href=\"#Prediction\" data-toc-modified-id=\"Prediction-6.5\"><span class=\"toc-item-num\">6.5&nbsp;&nbsp;</span>Prediction</a></span><ul class=\"toc-item\"><li><span><a href=\"#Set-initial-variables\" data-toc-modified-id=\"Set-initial-variables-6.5.1\"><span class=\"toc-item-num\">6.5.1&nbsp;&nbsp;</span>Set initial variables</a></span></li><li><span><a href=\"#Make-dataframe-of-initial-setting\" data-toc-modified-id=\"Make-dataframe-of-initial-setting-6.5.2\"><span class=\"toc-item-num\">6.5.2&nbsp;&nbsp;</span>Make dataframe of initial setting</a></span></li><li><span><a href=\"#Predict-cum.-reservation-counts\" data-toc-modified-id=\"Predict-cum.-reservation-counts-6.5.3\"><span class=\"toc-item-num\">6.5.3&nbsp;&nbsp;</span>Predict cum. reservation counts</a></span></li><li><span><a href=\"#Save-result-dataset\" data-toc-modified-id=\"Save-result-dataset-6.5.4\"><span class=\"toc-item-num\">6.5.4&nbsp;&nbsp;</span>Save result dataset</a></span></li></ul></li></ul></li><li><span><a href=\"#Model-3-:-Optimize-Best-Pricing\" data-toc-modified-id=\"Model-3-:-Optimize-Best-Pricing-7\"><span class=\"toc-item-num\">7&nbsp;&nbsp;</span>Model 3 : Optimize Best Pricing</a></span><ul class=\"toc-item\"><li><span><a href=\"#Define-Discount-Recommendation-Function\" data-toc-modified-id=\"Define-Discount-Recommendation-Function-7.1\"><span class=\"toc-item-num\">7.1&nbsp;&nbsp;</span>Define Discount Recommendation Function</a></span></li><li><span><a href=\"#Save-Recommendation-Results\" data-toc-modified-id=\"Save-Recommendation-Results-7.2\"><span class=\"toc-item-num\">7.2&nbsp;&nbsp;</span>Save Recommendation Results</a></span></li></ul></li><li><span><a href=\"#After-Recommendation\" data-toc-modified-id=\"After-Recommendation-8\"><span class=\"toc-item-num\">8&nbsp;&nbsp;</span>After Recommendation</a></span></li><li><span><a href=\"#Additional-Analysis\" data-toc-modified-id=\"Additional-Analysis-9\"><span class=\"toc-item-num\">9&nbsp;&nbsp;</span>Additional Analysis</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import math\n",
    "import time\n",
    "import copy\n",
    "import datetime as dt\n",
    "import pickle\n",
    "from itertools import product\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Plotting\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.font_manager as fm\n",
    "import seaborn as sns\n",
    "font_path = r'C:\\Windows\\Fonts\\NanumBarunGothic.ttf'\n",
    "font_name = fm.FontProperties(fname=font_path).get_name()\n",
    "matplotlib.rc('font', family=font_name)\n",
    "plt.style.use('seaborn')\n",
    "\n",
    "# Data Preprocessing\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler    # Scaling\n",
    "from sklearn.model_selection import train_test_split    # Split train and test dataset\n",
    "\n",
    "# Evaluation\n",
    "from sklearn.metrics import accuracy_score, mean_squared_error\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV, KFold\n",
    "\n",
    "# Time Series Models\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from statsmodels.tsa.ar_model import AutoReg\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "from statsmodels.tsa.holtwinters import ExponentialSmoothing\n",
    "\n",
    "# Machine Learning Models\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.linear_model import LogisticRegression \n",
    "from sklearn.linear_model import SGDRegressor\n",
    "from sklearn import svm    # Support Vector Machine\n",
    "from sklearn.ensemble import ExtraTreesRegressor    # Extra-trees Regressor\n",
    "import xgboost as xgb    # XGboost\n",
    "import lightgbm as lgb   # Light gradient boosting machine\n",
    "\n",
    "# Customized Exponential Model\n",
    "from scipy.optimize import curve_fit\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define File Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Folder root path\n",
    "root_path = os.path.join('..', 'input')\n",
    "save_path = os.path.join('..', 'result')\n",
    "\n",
    "# Reservation data path (yearly dataset )\n",
    "res_18_path = os.path.join(root_path, 'reservation_18_discount.csv')\n",
    "res_19_path = os.path.join(root_path, 'reservation_19_discount.csv')\n",
    "res_20_path = os.path.join(root_path, 'reservation_20_discount.csv')\n",
    "\n",
    "# Reservation Discount\n",
    "res_discount_18_path = os.path.join(root_path, 'reservation_18.csv')\n",
    "res_discount_19_path = os.path.join(root_path, 'reservation_19.csv')\n",
    "\n",
    "# Jeju visitor data path\n",
    "jeju_visit_path = os.path.join(root_path, 'jeju_visit_daily.csv')\n",
    "\n",
    "# Discount \n",
    "discount_type_path = os.path.join(root_path, 'discount_type.csv')\n",
    "discount_season_path = os.path.join(root_path, 'discount_season.csv')\n",
    "discount_policy_path = os.path.join(root_path, 'discount_policy.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reservation dataset\n",
    "res_data_type = {'계약번호': int, '예약경로명': str, '고객구분명': str, '총 청구액(VAT포함)': int,\n",
    "                 '예약모델명': str, '차급': str, '대여일': str, '대여시간': str, '반납일': str,\n",
    "                 '반납시간': str, '차량대여요금(VAT포함)': int, 'CDW요금구분명': str, 'CDW요금': int, \n",
    "                 '총대여료(VAT포함)': int, '적용할인율(%) ': float, '예약일자': str}\n",
    "\n",
    "res_18 = pd.read_csv(res_18_path, delimiter='\\t', dtype=res_data_type)\n",
    "res_19 = pd.read_csv(res_19_path, delimiter='\\t', dtype=res_data_type)\n",
    "res_20 = pd.read_csv(res_20_path, delimiter='\\t', dtype=res_data_type)\n",
    "# res_20 = pd.read_csv(res_20_path, delimiter='\\t', dtype=res_data_type)\n",
    "\n",
    "# Reservation Discount dataset\n",
    "res_discount_data_type = {'예약경로': int,'예약경로명': str, '계약번호': int,  '고객': int, '고객구분': float, \n",
    "                          '고객구분명': str, '총 청구액(VAT포함)': int, '총 수납금액(VAT포함)': int,\n",
    "                          '총 잔액(VAT포함)': int, '예약모델': str, '예약모델명': str, '차급': str, \n",
    "                          '대여일': str, '대여시간': str, '반납일': str,'반납시간': str, '대여기간(일)': int,\n",
    "                          '대여기간(시간)': int, '실반납일시': int, '실대여기간(일)': int, '실대여기간(시간)': int,\n",
    "                          '차량대여요금(VAT포함)': int, 'CDW가입여부': str, 'CDW요금구분': float, \n",
    "                          'CDW요금구분명': str, 'CDW요금': str, '회원등급': str, '차종': str, '구매목적': str, \n",
    "                          '내부매출액': int, '수납': str, '예약일자': str, '할인유형': str, '할인유형명': str, \n",
    "                          '적용할인명': str}\n",
    "\n",
    "res_18_discount = pd.read_csv(res_discount_18_path, delimiter='\\t', dtype=res_discount_data_type)\n",
    "res_19_discount = pd.read_csv(res_discount_19_path, delimiter='\\t', dtype=res_discount_data_type)\n",
    "\n",
    "# Jeju dataset\n",
    "jeju_visit = pd.read_csv(jeju_visit_path, delimiter='\\t', \n",
    "                         dtype={'date': int, 'domestic': int, 'foreign': int, 'total': int})\n",
    "\n",
    "# Discount dataset\n",
    "discount_type = pd.read_csv(discount_type_path, delimiter='\\t', dtype={'kind': str, 'discount': int})\n",
    "discount_season = pd.read_csv(discount_season_path, delimiter='\\t',\n",
    "                                 dtype={'YYYYMMDD': int, 'M3': float, 'M2': float, 'M1': float, 'W4': float,\n",
    "                                        'W3': float, 'W2': float, 'W1': float, 'weekend': float, 'season': float, \n",
    "                                        'event': float})\n",
    "discount_policy = pd.read_csv(discount_policy_path, delimiter='\\t', \n",
    "                              dtype={'date': int, '20160701': float, '20160901': float, '20170101': float,\n",
    "                                    '20180101': float, '20180209': float, '20180312': float, '20180314': float,\n",
    "                                    '20180327': float, '20180417': float, '20180508': float, '20180628': float,\n",
    "                                    '20180720': float, '20180726': float, '20180808': float, '20180817': float,\n",
    "                                    '20180903': float, '20180910': float, '20180914': float, '20181010': float,\n",
    "                                    '20181029': float, '20181129': float, '20181218': float, '20190111': float,\n",
    "                                    '20190122': float, '20190225': float, '20190313': float, '20190322': float,\n",
    "                                    '20190405': float, '20190409': float, '20190419': float, '20190510': float,\n",
    "                                    '20190521': float, '20190528': float, '20190612': float, '20190701': float,\n",
    "                                    '20190718': float, '20190723': float, '20190816': float, '20190823': float,\n",
    "                                    '20190909': float, '20191029': float})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### API Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Jeju Airport Arrival Passenger Dataset\n",
    "\n",
    "# API URL & Key\n",
    "url = 'http://openapi.airport.co.kr/service/rest/totalAirportStatsService/getAirportStats'\n",
    "service_key = 'UYRNns1wVRWz8MIyaMqUcL%2BHhIsbY0xjNyzRyvBNZRwh9zefraNj4lh9eBLgOw%2B2c8lBV%2Fh1SbzyNV96aO3DUw%3D%3D'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# API \n",
    "def get_api_data(date_from_to: list, ulr: str, service_key: str):\n",
    "    for date in date_from_to:\n",
    "        url_opt = f'?startDePd={date[0]}&endDePd={date[0]}&routeBe=1&pasngrCargoBe=0&serviceKey={service_key}'\n",
    "        url_fin = ulr + url_opt\n",
    "        response = urlopen(url_fin).read()\n",
    "        xtree = ET.fromstring(response)\n",
    "        rows = get_xml_data(xtree=xtree)\n",
    "\n",
    "    return rows\n",
    "\n",
    "def get_xml_data(xtree):\n",
    "    rows = []\n",
    "    for node in xtree[1][0]:\n",
    "        airport = node.find(\"airport\").text\n",
    "        arrflgt = node.find(\"arrflgt\").text\n",
    "        arrpassenger = node.find(\"arrpassenger\").text\n",
    "        depflgt = node.find(\"depflgt\").text\n",
    "        deppassenger = node.find(\"deppassenger\").text\n",
    "        subflgt = node.find(\"subflgt\").text\n",
    "        subpassenger = node.find(\"subpassenger\").text\n",
    "\n",
    "        rows.append({\"airport\": airport, \"arrflgt\": arrflgt, \"arrpassenger\": arrpassenger, \n",
    "                     \"arrpassenger\": arrpassenger, \"depflgt\": depflgt, \"deppassenger\": deppassenger,\n",
    "                     \"subflgt\": subflgt, \"subpassenger\": subpassenger})\n",
    "\n",
    "    return rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Merge Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Reservation \n",
    "res = pd.concat([res_18, res_19], axis=0, ignore_index=True)\n",
    "\n",
    "# Reservation discount\n",
    "res_discount = pd.concat([res_18_discount, res_19_discount], axis=0, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remap Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Reservation\n",
    "res_remap_cols = {'계약번호': 'res_num', '예약경로명': 'res_route_nm', '고객구분명': 'cust_kind_nm',\n",
    "                 '총 청구액(VAT포함)': 'tot_fee', '예약모델명': 'res_model_nm', '차급': 'car_grd', \n",
    "                 '대여일': 'rent_day', '대여시간': 'rent_time', '반납일': 'return_day', '반납시간': 'return_time', \n",
    "                 '차량대여요금(VAT포함)': 'car_rent_fee', 'CDW요금구분명': 'cdw_fee_kind_nm', 'CDW요금': 'cdw_fee',\n",
    "                 '회원등급': 'member_grd','차종': 'car_kind', '예약일자': 'res_day', '할인유형': 'discount_type', \n",
    "                 '총대여료(VAT포함)': 'fin_fee', '적용할인율(%)': 'fin_discount'\n",
    "            }\n",
    "res = res.rename(columns=res_remap_cols)\n",
    "res_20 = res_20.rename(columns=res_remap_cols)\n",
    "\n",
    "# Reservation Discount\n",
    "res_discount_remap_cols = {'계약번호': 'res_num', '할인유형': 'discount_type', \n",
    "                           '할인유형명': 'discount_type_nm', '적용할인명': 'applyed_discount'}\n",
    "\n",
    "res_discount = res_discount.rename(columns=res_discount_remap_cols)\n",
    "res_20 = res_20.rename(columns=res_discount_remap_cols)\n",
    "\n",
    "# Jeju visit\n",
    "jeju_remap_cols = {'domestic': 'visit_dom', 'foreign': 'visit_for', 'total': 'visit_tot'}\n",
    "jeju_visit = jeju_visit.rename(columns=jeju_remap_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 특이사항 처리\n",
    "res_20 = res_20[pd.to_datetime(res_20['res_day'], format='%Y-%m-%d') >= dt.datetime(2020,1,1)]\n",
    "\n",
    "res_20_discount = res_20[['res_num', 'discount_type_nm', 'applyed_discount']]\n",
    "res_20 = res_20.drop(columns=['member_grd', 'discount_type_nm', 'applyed_discount'], axis=1)\n",
    "res = res.drop(columns=['cust_kind_nm'], axis=1, errors='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = pd.concat([res, res_20], axis=0, ignore_index=True)\n",
    "res_discount = pd.concat([res_discount, res_20_discount], axis=0, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check NA Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39949 870 0 0 3774 62155\n"
     ]
    }
   ],
   "source": [
    "# Reservation\n",
    "na_cnt_res = res.isna().sum().sum()\n",
    "na_cnt_res_20 = res_20.isna().sum().sum()\n",
    "\n",
    "# Jeju\n",
    "na_cnt_jeju = jeju_visit.isna().sum().sum()\n",
    "\n",
    "# discount type\n",
    "na_cnt_discount = discount_type.isna().sum().sum()\n",
    "\n",
    "# Discount Lead Time\n",
    "na_cnt_discount_season = discount_season.isna().sum().sum()\n",
    "\n",
    "# Discount Policy\n",
    "na_cnt_discount_policy = discount_policy.isna().sum().sum()\n",
    "\n",
    "print(na_cnt_res, na_cnt_res_20, na_cnt_jeju, na_cnt_discount, na_cnt_discount_season, na_cnt_discount_policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fill Na values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Jeju visit\n",
    "jeju_visit = jeju_visit.fillna(0)\n",
    "\n",
    "# Discount Poilicy\n",
    "discount_policy['date'] = pd.to_datetime(discount_policy['date'], format='%Y%m%d')\n",
    "discount_policy = discount_policy.set_index('date')\n",
    "discount_policy = discount_policy.fillna(method='ffill', axis=1)\n",
    "discount_policy = discount_policy.fillna(method='bfill', axis=1)\n",
    "discount_policy = discount_policy.reset_index()\n",
    "\n",
    "# Discount Season\n",
    "discount_season = discount_season.fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Change Data Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Reservation dataset\n",
    "res['res_day'] = pd.to_datetime(res['res_day'], format='%Y-%m-%d')\n",
    "res['rent_day'] = pd.to_datetime(res['rent_day'], format='%Y-%m-%d')\n",
    "res['return_day'] = pd.to_datetime(res['return_day'], format='%Y-%m-%d')\n",
    "\n",
    "# Jeju Visit dataset\n",
    "jeju_visit['visit_dom'] = jeju_visit['visit_dom'].astype(int)\n",
    "jeju_visit['visit_for'] = jeju_visit['visit_for'].astype(int)\n",
    "jeju_visit['visit_tot'] = jeju_visit['visit_tot'].astype(int)\n",
    "\n",
    "jeju_visit['rent_day'] = pd.to_datetime(jeju_visit['date'], format='%Y%m%d')\n",
    "# jeju_visit = jeju_visit.set_index('rent_day', drop=False)\n",
    "\n",
    "# Discount Season dataset\n",
    "discount_season['rent_day'] = pd.to_datetime(discount_season['YYYYMMDD'], format='%Y%m%d')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Filter Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reservation dataset\n",
    "# Filter 1.6 Grade cars\n",
    "res = res[res['res_model_nm'].isin(['K3', 'THE NEW K3 (G)', 'ALL NEW K3 (G)',\n",
    "                                    '아반떼 AD (G) F/L', '올 뉴 아반떼 (G)', '아반떼 AD (G)',\n",
    "                                    '쏘울 (G)', '더 올 뉴 벨로스터 (G)'])]\n",
    "\n",
    "# Discount dataset\n",
    "discount_season = discount_season[discount_season['YYYYMMDD'] >= 20180101]    # Except 2017 year"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Grouping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Car Model group\n",
    "# SOUL 모델은 VELOSTER 모델에 포함해서 분석 (실적 데이터가 적음)\n",
    "\n",
    "conditions = [\n",
    "    res['res_model_nm'].isin(['K3', 'THE NEW K3 (G)', 'ALL NEW K3 (G)']),\n",
    "    res['res_model_nm'].isin(['아반떼 AD (G)', '아반떼 AD (G) F/L', '올 뉴 아반떼 (G)']),\n",
    "    res['res_model_nm'].isin(['더 올 뉴 벨로스터 (G)', '쏘울 (G)'])    \n",
    "]\n",
    "\n",
    "values = ['K3', 'AVANTE', 'VELOSTER']\n",
    "\n",
    "res['res_model_grp'] = np.select(conditions, values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Drop Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reservation dataset\n",
    "res_drop_cols = ['res_route_nm', 'car_grd', 'res_model_nm', 'cdw_fee_kind_nm', 'tot_fee']\n",
    "res = res.drop(columns=res_drop_cols, axis=1, errors='ignore')\n",
    "\n",
    "# Reservation Discount dataset\n",
    "res_discount = res_discount[['res_num', 'discount_type_nm', 'applyed_discount']]\n",
    "\n",
    "# jeju visit\n",
    "jeju_visit = jeju_visit.drop(columns=['yyyymmdd'], axis=1, errors='ignore')\n",
    "\n",
    "# Discount\n",
    "discount_season = discount_season[['rent_day', 'weekend', 'season', 'event']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ordering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = res.sort_values(by=['rent_day', 'res_day'])\n",
    "jeju_visit = jeju_visit.sort_values(by=['rent_day'])\n",
    "discount_season = discount_season.sort_values(by=['rent_day'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing for each dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reservation Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Caculate lead time of reservation \n",
    "res['lead_time'] = res['rent_day'] - res['res_day']\n",
    "res['lead_time'] = res['lead_time'].apply(lambda x: x.days)\n",
    "\n",
    "# Conver rent/return day and time to datetime (YYYYMMDD HHMISS)\n",
    "rent_datetime = res['rent_day'].apply(lambda x: dt.datetime.strftime(x, '%Y-%m-%d')) + ' ' + res['rent_time']\n",
    "return_datetime = res['return_day'].apply(lambda x: dt.datetime.strftime(x, '%Y-%m-%d')) + ' ' + res['return_time']\n",
    "res['rent_datetime'] = pd.to_datetime(rent_datetime, format='%Y-%m-%d %H:%M:%S')\n",
    "res['return_datetime'] = pd.to_datetime(return_datetime, format='%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "res['rent_period'] = res['return_datetime'] - res['rent_datetime']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make reservation counts on each day\n",
    "res_cnt_on_rent_day = res[['rent_day', 'res_num']].groupby(by=['rent_day']).count()\n",
    "res_cnt_on_rent_day = res_cnt_on_rent_day.rename(columns={'res_num': 'res_cnt'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Jeju Visit Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "jeju_visit['visit_tot'] = jeju_visit['visit_dom'] + jeju_visit['visit_for']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Discount Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rechange data type of reservation\n",
    "res['res_day'] = pd.to_datetime(res['res_day'], format='%Y-%m-%d')\n",
    "res['rent_day'] = pd.to_datetime(res['rent_day'], format='%Y-%m-%d')\n",
    "res['return_day'] = pd.to_datetime(res['return_day'], format='%Y-%m-%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Discount Policy\n",
    "discount_idx_to_lead_time = {idx: day for idx, day in enumerate(discount_policy.columns)}\n",
    "discount_lead_time_to_idx = {day: idx for idx, day in enumerate(discount_policy.columns)}\n",
    "\n",
    "res_day_to_idx = {}\n",
    "res_col = discount_policy.columns[1:]\n",
    "for i in range(len(res_col)-1):\n",
    "    date_range_temp = pd.date_range(start=res_col[i], end=res_col[i+1])\n",
    "    date_range_temp = date_range_temp[:-1]\n",
    "    for date in date_range_temp:\n",
    "        res_day_to_idx[date] = i\n",
    "        \n",
    "date_range_last = pd.date_range(start=res_col[-1], end=discount_policy['date'].iloc[-1].strftime('%Y%m%d'))\n",
    "for date in date_range_last:\n",
    "    res_day_to_idx[date] = len(res_col) - 1\n",
    "    \n",
    "rent_day_to_idx = {date: idx for idx, date in enumerate(discount_policy['date'])}\n",
    "discount_idx_col = [(rent_day_to_idx[idx], res_day_to_idx[col]) for idx, col in zip(res['rent_day'], res['res_day'])]\n",
    "res['res_discount'] = [discount_policy.iloc[idx, col] for idx, col in discount_idx_col]\n",
    "res['diff_discount'] = res['fin_discount'] - res['res_discount']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Discount type\n",
    "discount_type['discount'] = discount_type['discount'] - 60    # 비회원 기준 할인율\n",
    "discount_type_dict = {kind: discount for kind, discount in zip(discount_type['kind'], discount_type['discount'])}\n",
    "\n",
    "# Correct naming on 2018/2019 years \n",
    "discount_type_dict[\"인터넷일반회원\"] = discount_type_dict[\"일반회원\"]\n",
    "discount_type_dict[\"인터넷골드회원\"] = discount_type_dict[\"골드회원\"]\n",
    "discount_type_dict[\"인터넷더블골드회원\"] = discount_type_dict[\"더블골드회원\"]\n",
    "discount_type_dict[\"예약어플(골드회원)\"] = discount_type_dict[\"골드회원\"]\n",
    "discount_type_dict[\"예약어플(더블골드회원)\"] = discount_type_dict[\"더블골드회원\"]\n",
    "discount_type_dict['비씨카드(주)'] = discount_type_dict['BC카드']\n",
    "discount_type_dict[\"신한Top's club\"] = discount_type_dict[\"신한 Top's club\"]\n",
    "discount_type_dict[\"현대카드오토케어\"] = discount_type_dict[\"현대카드 오토케어\"]\n",
    "discount_type_dict[\"롯데그룹 임직원\"] = discount_type_dict[\"롯데그룹임직원\"]\n",
    "discount_type_dict[\"금호그룹 임직원\"] = discount_type_dict[\"금호그룹임직원\"]\n",
    "discount_type_dict[\"KT그룹 임직원\"] = discount_type_dict[\"KT그룹임직원\"]\n",
    "discount_type_dict[\"VIP거래처(제주)\"] = discount_type_dict[\"VIP거래처\"]\n",
    "discount_type_dict[\"에어부산(FLY.FUN)\"] = discount_type_dict[\"에어부산(FLY & FUN)\"]\n",
    "discount_type_dict[\"삼성전자 서비스\"] = discount_type_dict[\"삼성전자서비스\"]\n",
    "discount_type_dict[\"하나카드 VIP 컨시어지\"] = discount_type_dict[\"하나카드VIP컨시어지\"]\n",
    "discount_type_dict[\"BC-kt 제휴카드\"] = discount_type_dict[\"BC-kt제휴카드\"]\n",
    "discount_type_dict[\"인천공항홈페이지\"] = discount_type_dict[\"인천공항 홈페이지\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Merge Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reservation + Reservation Counts on each day\n",
    "res = pd.merge(res, res_cnt_on_rent_day, how='left', on='rent_day', left_index=True, right_index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reservation + Jeju visit on rental day\n",
    "res = pd.merge(res, jeju_visit, how='left', on='rent_day', left_index=True, right_index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reservaion + Discount season\n",
    "res = pd.merge(res, discount_season, how='left', on='rent_day', left_index=True, right_index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reservation + Reservation discount\n",
    "res = pd.merge(res, res_discount, how='left', on='res_num', left_index=True, right_index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop unnecessary Columns\n",
    "res = res.drop(columns=['rent_time', 'return_time', 'rent_datetime', 'return_datetime'],\n",
    "               axis=1, errors='ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add seasonality column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "conditions = [\n",
    "    (res['season'] != 0),\n",
    "    (res['event'] == 1),\n",
    "    ((res['weekend'] == 1) & (res['weekend'] == 1) & (res['weekend'] == 1)),\n",
    "    (res['season'] + res['event'] + res['weekend'] == 0)]\n",
    "\n",
    "values =[4, 3, 2, 1]\n",
    "\n",
    "res['seasonality'] = np.select(conditions, values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add Lead Time Interval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "conditions = [\n",
    "    ((res['lead_time'] >= 7 * 4) & (res['lead_time'] < 7 * 5)),\n",
    "    ((res['lead_time'] >= 7 * 5) & (res['lead_time'] < 7 * 6)),\n",
    "    ((res['lead_time'] >= 7 * 6) & (res['lead_time'] < 7 * 7)),\n",
    "    ((res['lead_time'] >= 7 * 7) & (res['lead_time'] < 7 * 8)),\n",
    "    ((res['lead_time'] >= 7 * 8) & (res['lead_time'] < 7 * 9)),\n",
    "    ((res['lead_time'] >= 7 * 9) & (res['lead_time'] < 7 * 10)),\n",
    "    ((res['lead_time'] >= 7 * 10) & (res['lead_time'] < 7 * 11)),\n",
    "    ((res['lead_time'] >= 7 * 11) & (res['lead_time'] < 7 * 12)),\n",
    "    (res['lead_time'] >= 7 * 12)]\n",
    "\n",
    "values = np.arange(28, 28 + 9, 1)\n",
    "\n",
    "res['lead_time_vector'] = np.select(conditions, values)\n",
    "res['lead_time_vector'] = np.where(res['lead_time_vector'] <= 28, \n",
    "                                   res['lead_time'].values,\n",
    "                                   res['lead_time_vector'].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert Rent Periods to hours "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "res['rent_period'] = res['rent_period'].apply(lambda x: x.days* 24 + x.seconds//3600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Update Discount Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "res['discount_add'] = res['applyed_discount'].apply(lambda x: discount_type_dict.get(x, 0))\n",
    "res['res_discount'] += res['discount_add']\n",
    "res = res.drop(columns=['discount_add'], errors='ignore')\n",
    "res['res_discount'] = np.where(res['res_discount'] > 90, 90, res['res_discount'])    # 할인율 상한선: 90%\n",
    "# res['diff_discount'] = res['fin_discount'] - res['rev_discount']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate reservation fee"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "res['res_fee'] = res['fin_fee'] - res['cdw_fee']\n",
    "res['res_fee'] = res['res_fee'] / (1 - res['fin_discount']/100)\n",
    "res['res_fee'] = res['res_fee'] * (1 - res['res_discount']/100) + res['cdw_fee']\n",
    "res['res_fee'] = res['res_fee'].apply(lambda x: round(x, 0))\n",
    "res = res.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "res.loc[res['res_fee'] == np.inf, 'res_fee'] = ((res['fin_fee'] - res['cdw_fee']) * res['res_discount']) + res['cdw_fee']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove Exception"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 예약 가격이 없는 경우 제외\n",
    "res = res[res['res_fee'].notnull()]\n",
    "\n",
    "# 쿠폰할인 제외\n",
    "res = res[res['discount_type_nm'] != '쿠폰할인']\n",
    "\n",
    "# 임의할인: 할인율 100% 제외\n",
    "res = res[res['fin_fee'] != 0] \n",
    "\n",
    "# 업체할인: 비씨카드 이외 할인 제외\n",
    "res = res[(res['discount_type_nm'] != '업체할인') | (res['applyed_discount'] == '비씨카드(주)')] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save preprocessed dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "seasonality = res[['rent_day', 'seasonality']].drop_duplicates().reset_index(drop=True)\n",
    "seasonality.to_csv(os.path.join(save_path, 'data', 'seasonality.csv'), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model 2: Cum. Reservation Counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Slicing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_2 = res.loc[:, ['res_day', 'rent_day', 'lead_time', 'lead_time_vector', \n",
    "                    'seasonality', 'res_model_grp', 'res_discount']]\n",
    "\n",
    "# Convert lead time to negative values\n",
    "res_2['lead_time'] = res_2['lead_time'] * -1\n",
    "res_2['lead_time_vector'] = res_2['lead_time_vector'] * -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Grouping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Counts on group 'rent day', 'reservation day'\n",
    "res_cnt_by_rent_day = res_2.groupby(by=['rent_day', 'res_day']).count()['lead_time']\n",
    "res_cum_cnt_by_rent_day = res_cnt_by_rent_day.groupby(by=['rent_day']).cumsum()\n",
    "\n",
    "# Summation on group 'rent day', 'reservation day'\n",
    "res_dscnt_by_rent_day = res_2.groupby(by=['rent_day', 'res_day']).sum()['res_discount']\n",
    "res_cum_dscnt_by_rent_day = res_dscnt_by_rent_day.groupby(by=['rent_day']).cumsum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_cum = pd.DataFrame({'cum_dscnt': res_cum_dscnt_by_rent_day,\n",
    "                        'cum_res_cnt': res_cum_cnt_by_rent_day}, \n",
    "                        index=res_cum_dscnt_by_rent_day.index)\n",
    "res_cum['cum_dscnt_mean'] = res_cum['cum_dscnt'] / res_cum['cum_res_cnt']\n",
    "res_cum['cum_dscnt_mean'] = res_cum['cum_dscnt_mean'].apply(lambda x: round(x, 2))\n",
    "res_cum = res_cum.reset_index(level=(0, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Merging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_2 = pd.merge(res_2, res_cum, how='left', on=['rent_day', 'res_day'], \n",
    "                   left_index=True, right_index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_2 = model_2.drop(columns=['res_day', 'rent_day', 'lead_time', 'res_model_grp', \n",
    "                                'res_discount', 'cum_dscnt'], errors='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_2 = model_2.sort_values(by=['seasonality', 'lead_time_vector']).drop_duplicates()\n",
    "model_2 = model_2[['seasonality', 'lead_time_vector', 'cum_dscnt_mean', 'cum_res_cnt']]\n",
    "model_2 = model_2.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Re-group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_2_grp = model_2.groupby(by=['seasonality', 'lead_time_vector']).mean()\n",
    "model_2_grp = model_2_grp.reset_index(level=(0, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Save Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_2.to_csv(os.path.join(save_path, 'data', 'model_2.csv'), index=False)\n",
    "model_2_grp.to_csv(os.path.join(save_path, 'data', 'model_2_grp.csv'), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Model 2: Cum. Reservation Counts by model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Slicing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_2 = res.loc[:, ['res_day', 'rent_day', 'lead_time', 'lead_time_vector', \n",
    "                    'seasonality', 'res_model_grp', 'res_discount']]\n",
    "res_2['lead_time'] = res_2['lead_time'] * -1\n",
    "res_2['lead_time_vector'] = res_2['lead_time_vector'] * -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Grouping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_cnt_by_model = res_2.groupby(by=['rent_day', 'res_model_grp', 'res_day']).count()['lead_time_vector']\n",
    "res_cum_cnt_by_model = res_cnt_by_model.groupby(by=['rent_day', 'res_model_grp']).cumsum()\n",
    "\n",
    "res_dscnt_by_model = res_2.groupby(by=['rent_day', 'res_model_grp', 'res_day']).sum()['res_discount']\n",
    "res_cum_dscnt_by_model = res_dscnt_by_model.groupby(by=['rent_day', 'res_model_grp']).cumsum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_cum_model = pd.DataFrame({'cum_dscnt': res_cum_dscnt_by_model,\n",
    "                              'cum_res_cnt': res_cum_cnt_by_model}, \n",
    "                              index=res_cum_dscnt_by_model.index)\n",
    "res_cum_model['cum_dscnt_mean'] = res_cum_model['cum_dscnt'] / res_cum_model['cum_res_cnt']\n",
    "res_cum_model['cum_dscnt_mean'] = res_cum_model['cum_dscnt_mean'].apply(lambda x: round(x, 2))\n",
    "res_cum_model = res_cum_model.reset_index(level=(0, 1, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Merging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_2_car = pd.merge(res_2, res_cum_model, how='left', on=['rent_day', 'res_model_grp', 'res_day'], \n",
    "                       left_index=True, right_index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort dataframe\n",
    "model_2_car = model_2_car.sort_values(by=['rent_day', 'res_model_grp', 'res_day'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop unnecessary columns\n",
    "model_2_car = model_2_car.drop(columns=['res_day', 'rent_day', 'lead_time', \n",
    "                                'res_discount', 'cum_dscnt'], errors='ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Re-group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_2_car_grp = model_2_car.groupby(by=['res_model_grp', 'seasonality', 'lead_time_vector']).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Divide into each car model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_2_av = model_2_car_grp.loc['AVANTE', :]\n",
    "model_2_k3 = model_2_car_grp.loc['K3', :]\n",
    "model_2_vl = model_2_car_grp.loc['VELOSTER', :]\n",
    "\n",
    "model_2_av = model_2_av.reset_index(level=(0, 1))\n",
    "model_2_k3 = model_2_k3.reset_index(level=(0, 1))\n",
    "model_2_vl = model_2_vl.reset_index(level=(0, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Save dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_2_av.to_csv(os.path.join(save_path, 'data', 'model_2_av.csv'), index=False)\n",
    "model_2_k3.to_csv(os.path.join(save_path, 'data', 'model_2_k3.csv'), index=False)\n",
    "model_2_vl.to_csv(os.path.join(save_path, 'data', 'model_2_vl.csv'), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Input  - Reservation Counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "code_folding": []
   },
   "source": [
    "##### Drop Unnecessary columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_cols_1 = ['res_num', 'rent_day', 'return_day', 'res_day', 'diff_discount']\n",
    "drop_cols_2 = ['visit_dom', 'visit_for']    # Jeju visitor (Using total number of visitors)\n",
    "drop_cols_3 = ['weekend', 'season', 'event']    # use customized seasonal factor\n",
    "drop_cols_4 = ['discount_type_nm', 'applyed_discount']    # only use discount rate\n",
    "drop_cols_5 = ['car_rent_fee', 'cdw_fee', 'fin_fee', 'res_fee']    # not use fee\n",
    "drop_cols_6 = ['fin_discount']    # use reservation discount rate ('res_discount')\n",
    "drop_cols_7 = ['lead_time']    # use customized lead time vector ('lead_time_vector')  \n",
    "drop_cols_8 = ['rent_period']  \n",
    "res_1 = res.drop(columns=drop_cols_1 + drop_cols_2 + drop_cols_3 + drop_cols_4 + \n",
    "                       drop_cols_5 + drop_cols_6 + drop_cols_7 + drop_cols_8, axis=1, errors='ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Data Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = res_1.drop(columns=['res_cnt'], axis=1)\n",
    "y = res_1['res_cnt']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Save  Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.to_csv(os.path.join(save_path, 'data', 'res_input_1.csv'), index=False)\n",
    "y.to_csv(os.path.join(save_path, 'data', 'res_target_1.csv'), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 1: Predict Jeju Visitors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "jeju_visit = pd.read_csv(os.path.join('..', 'input', 'jeju_visit_daily.csv'), delimiter='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Filter the periods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interval: 1 year\n",
    "start_date = 20191101\n",
    "end_date = 20201031\n",
    "\n",
    "jeju_visit = jeju_visit[(jeju_visit['date'] >= start_date) & (jeju_visit['date'] <= end_date)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Split dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "# String date convert to datetime\n",
    "jeju_visit['date'] = pd.to_datetime(jeju_visit['date'], format='%Y%m%d')\n",
    "jeju_visit = jeju_visit.set_index('date')\n",
    "\n",
    "# Split dataset to domestic and foreign\n",
    "jeju_dom = jeju_visit[['domestic']]\n",
    "jeju_for = jeju_visit[['foreign']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define Models & Validation Method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Define Time Series Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ar_model(data, config: dict, pred_steps=0):\n",
    "    \"\"\"\n",
    "    :param data: times series data\n",
    "    :param lags: the number of lags\n",
    "    :param trend: the trend to include in the model\n",
    "            n: No Trend\n",
    "            c: Constant Only\n",
    "            t: Time Trend Only\n",
    "            ct: Constant and time trend\n",
    "    :param seasonal: Flag indicating whether to include seasonal dummies in the model    \n",
    "    :param pred_step: prediction steps\n",
    "    \"\"\"\n",
    "    model = AutoReg(endog=data, lags=config['lags'], trend=config['trend'])\n",
    "    model_fit = model.fit()\n",
    "    yhat = model_fit.predict(start=len(data), end=len(data) + pred_steps-1)\n",
    "    \n",
    "    return yhat\n",
    "    \n",
    "def arima_model(data, config: dict, pred_steps=0):\n",
    "    \"\"\"\n",
    "    :param data: time series data\n",
    "    :param order: (p, d, q)\n",
    "            p: Trend autoregression order\n",
    "            d: Trend difference order\n",
    "            q: Trend moving average order\n",
    "    :param trend: the trend to include in the model\n",
    "            n: No Trend\n",
    "            c: Constant\n",
    "            t: Trend\n",
    "            ct: Constant and Trend\n",
    "    :param freq: frequency of the time series (‘B’, ‘D’, ‘W’, ‘M’, ‘A’, ‘Q)\n",
    "    \"\"\"\n",
    "    model = ARIMA(data, order=config['order'], trend=config['trend'])\n",
    "    model_fit = model.fit()\n",
    "    yhat = model_fit.predict(start=len(data), end=len(data) + pred_steps-1)\n",
    "    \n",
    "    return yhat\n",
    "    \n",
    "def holt_winters_model(data, config: dict, pred_steps=0):\n",
    "    \"\"\"\n",
    "    :param data: time series data\n",
    "    :param trend - type of trend component\n",
    "        ('add', 'mul', 'additive', 'multiplicative')\n",
    "    :param damped_trend - should the trend component be damped\n",
    "    :param seasonal - Type of seasonal component\n",
    "            ('add', 'mul', 'additive', 'multiplicative', None)\n",
    "    :param seasonal_periods - The number of periods in a complete seasonal cycle\n",
    "    :param pred_step:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    model = ExponentialSmoothing(data, trend=config['trend'], damped_trend=config['damped_trend'])\n",
    "    model_fit = model.fit()\n",
    "    yhat = model_fit.predict(start=len(data), end=len(data) + pred_steps-1)\n",
    "    \n",
    "    return yhat\n",
    "\n",
    "time_series_model = {'ar': ar_model,\n",
    "                     'arima': arima_model,\n",
    "                     'hw': holt_winters_model}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Validation: Walk forward validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "def walk_forward_validation(model: str, data, n_test, config):\n",
    "    # split dataset\n",
    "    train, test = data[:-n_test], data[-n_test:]\n",
    "    \n",
    "    # calculate \n",
    "    yhat = time_series_model[model](data=train, config=config, pred_steps=n_test)\n",
    "    \n",
    "    # add actual observation to history for the next loop\n",
    "    error = math.sqrt(mean_squared_error(test.values, yhat))\n",
    "    \n",
    "    return error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_series_param_tune(model: str, data, n_test, param_grid: dict):\n",
    "    err_list = list()\n",
    "    for params in list(product(*list(param_grid.values()))):\n",
    "        config = dict()\n",
    "        config = {key: val for key, val in zip(list(param_grid.keys()), params)}\n",
    "        err = walk_forward_validation(model, data, n_test=n_test, config=config)\n",
    "        err_list.append((list(config.items()), round(err, 2)))\n",
    "        \n",
    "    err_list_sorted = sorted(err_list, key=lambda err: err[1])    \n",
    "        \n",
    "    return err_list_sorted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation: Walk Forward Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Set parameter grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paramter Grid\n",
    "ar_param_grid = {\n",
    "    'lags': list(np.arange(1,15,1)),\n",
    "    'trend': ['c', 't', 'ct'] \n",
    "}\n",
    "\n",
    "arima_param_grid = {}\n",
    "\n",
    "hw_param_grid = {\n",
    "    'trend': ['add', 'additive'],\n",
    "    'damped_trend': [True, False] \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Range\n",
    "n_test = 28"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Walk forward validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AR: Best paramters: [('lags', 7), ('trend', 't')], result: 6692.77\n",
      "HW: Best paramters: [('trend', 'add'), ('damped_trend', True)], result: 14461.68\n"
     ]
    }
   ],
   "source": [
    "err_dom_ar = time_series_param_tune(model='ar', data=jeju_dom, \n",
    "                                    n_test=n_test, param_grid=ar_param_grid)\n",
    "print(f'AR: Best paramters: {err_dom_ar[0][0]}, result: {err_dom_ar[0][1]}')\n",
    "\n",
    "err_dom_hw = time_series_param_tune(model='hw', data=jeju_dom, \n",
    "                                    n_test=n_test, param_grid=hw_param_grid)\n",
    "print(f'HW: Best paramters: {err_dom_hw[0][0]}, result: {err_dom_hw[0][1]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AR: Best paramters: [('lags', 1), ('trend', 't')], result: 27.83\n",
      "HW: est paramters: [('trend', 'add'), ('damped_trend', True)], result: 148.46\n"
     ]
    }
   ],
   "source": [
    "err_for_ar = time_series_param_tune(model='ar', data=jeju_for, \n",
    "                                    n_test=n_test, param_grid=ar_param_grid)\n",
    "print(f'AR: Best paramters: {err_for_ar[0][0]}, result: {err_for_ar[0][1]}')\n",
    "\n",
    "err_for_hw = time_series_param_tune(model='hw', data=jeju_for, \n",
    "                                    n_test=n_test, param_grid=hw_param_grid)\n",
    "print(f'HW: Best paramters: {err_for_hw[0][0]}, result: {err_for_hw[0][1]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Save best paramter grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "m1_dom_ar_best_params = {key:val for key, val in err_dom_ar[0][0]}\n",
    "f = open(os.path.join(save_path, 'model', 'model_1', 'm1_dom_ar_best_params.pickle'), 'wb')\n",
    "pickle.dump(m1_dom_ar_best_params, f)\n",
    "f.close()\n",
    "\n",
    "m1_for_ar_best_params = {key:val for key, val in err_for_ar[0][0]}\n",
    "f = open(os.path.join(save_path, 'model', 'model_1', 'm1_for_ar_best_params.pickle'), 'wb')\n",
    "pickle.dump(m1_for_ar_best_params, f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Parameters and fit the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Load best parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(os.path.join(save_path, 'model', 'model_1', 'm1_dom_ar_best_params.pickle'), 'rb')\n",
    "m1_dom_ar_best_params = pickle.load(f)\n",
    "f.close()\n",
    "\n",
    "f = open(os.path.join(save_path, 'model', 'model_1', 'm1_dom_ar_best_params.pickle'), 'rb')\n",
    "m1_for_ar_best_params = pickle.load(f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Fit the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_steps = 88\n",
    "model_dom = AutoReg(endog=jeju_dom, \n",
    "                    lags=m1_dom_ar_best_params['lags'],\n",
    "                    trend=m1_dom_ar_best_params['trend'])\n",
    "model_dom_fit = model_dom.fit()\n",
    "pred_dom = model_dom_fit.predict(start=len(jeju_dom), end=len(jeju_dom) + pred_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_for = AutoReg(endog=jeju_for, \n",
    "                    lags=m1_for_ar_best_params['lags'],\n",
    "                    trend=m1_for_ar_best_params['trend'])\n",
    "model_for_fit = model_for.fit()\n",
    "pred_for = model_for_fit.predict(start=len(jeju_for), end=len(jeju_for) + pred_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_tot = pred_dom + pred_for\n",
    "\n",
    "pred_20_12 = pred_tot.loc[dt.datetime(2020,12,1):dt.datetime(2020,12,31)]\n",
    "pred_21_01 = pred_tot.loc[dt.datetime(2021,1,1):dt.datetime(2021,1,31)]\n",
    "pred_21_02 = pred_tot.loc[dt.datetime(2021,2,1):dt.datetime(2021,1,28)]\n",
    "\n",
    "visit_19_12 = jeju_visit.loc[dt.datetime(2019,12,1):dt.datetime(2019,12,31)]['total']\n",
    "visit_20_01 = jeju_visit.loc[dt.datetime(2020,1,1):dt.datetime(2020,1,31)]['total']\n",
    "visit_20_02 = jeju_visit.loc[dt.datetime(2020,2,1):dt.datetime(2020,2,28)]['total']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 2 : Predict Cum. Reservation Counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Make training dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_path = os.path.join('..', 'result')\n",
    "random_state = 2020\n",
    "\n",
    "# Load each model\n",
    "model_2_av = pd.read_csv(os.path.join(root_path, 'data', 'model_2_av.csv')) \n",
    "model_2_k3 = pd.read_csv(os.path.join(root_path, 'data', 'model_2_k3.csv')) \n",
    "model_2_vl = pd.read_csv(os.path.join(root_path, 'data', 'model_2_vl.csv')) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Divide into input and output dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Avante\n",
    "m2_av_x = model_2_av.drop(columns=['cum_res_cnt'])\n",
    "m2_av_y = model_2_av['cum_res_cnt']\n",
    "\n",
    "# K3\n",
    "m2_k3_x = model_2_k3.drop(columns=['cum_res_cnt'])\n",
    "m2_k3_y = model_2_k3['cum_res_cnt']\n",
    "\n",
    "# Veloster\n",
    "m2_vl_x = model_2_k3.drop(columns=['cum_res_cnt'])\n",
    "m2_vl_y = model_2_k3['cum_res_cnt']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Split train and test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "m2_av_x_train, m2_av_x_test, m2_av_y_train, m2_av_y_test = train_test_split(m2_av_x, m2_av_y, \n",
    "                                                                            test_size=0.2, \n",
    "                                                                            random_state=random_state, \n",
    "                                                                            shuffle=True)\n",
    "m2_k3_x_train, m2_k3_x_test, m2_k3_y_train, m2_k3_y_test = train_test_split(m2_k3_x, m2_k3_y, \n",
    "                                                                            test_size=0.2, \n",
    "                                                                            random_state=random_state, \n",
    "                                                                            shuffle=True)\n",
    "m2_vl_x_train, m2_vl_x_test, m2_vl_y_train, m2_vl_y_test = train_test_split(m2_vl_x, m2_vl_y, \n",
    "                                                                            test_size=0.2, \n",
    "                                                                            random_state=random_state, \n",
    "                                                                            shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pre-test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Pre-test paramters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparamter of Support Vector Machine \n",
    "param_svm = {\n",
    "    'kernel': 'rbf',    # 'linear', 'poly', 'rbf'\n",
    "    'tol':1e-3, \n",
    "    'max_iter': -1\n",
    "}\n",
    "\n",
    "# Hyperparamter of Extra-tree model\n",
    "param_extra = {\n",
    "    'n_estimators': 100,\n",
    "    'criterion': 'mse',\n",
    "    'max_depth': None,\n",
    "    'min_samples_split': 2,\n",
    "    'max_features': 'auto',\n",
    "    'random_state': 2020\n",
    "}\n",
    "\n",
    "# Hyperparamter of XGboost model\n",
    "xgb_param = {'max_depth': 6, 'eta': 1, 'objective': 'reg:squarederror'}\n",
    "xgb_param['nthread'] = 4\n",
    "xgb_param['eval_metric'] = 'rmse'    # 'rmse', 'auc', 'logloss', 'map'\n",
    "num_round = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Model: SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model\n",
    "regr_svm = svm.SVR(kernel=param_svm['kernel'],\n",
    "                   tol=param_svm['tol'], \n",
    "                   max_iter=param_svm['max_iter'])\n",
    "# Fit model\n",
    "regr_svm.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Save model\n",
    "pickle.dump(regr_svm, open(os.path.join(root_path, 'model', 'svm.model'), 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_svm = regr_svm.predict(X_test_scaled)    # Prediction on test dataset\n",
    "rmse_svm = mean_squared_error(y_test, pred_svm)    # Calculate RMSE\n",
    "pred_svm_df = pd.DataFrame({'real': y_test['cum_res_cnt'].values, 'prediction': pred_svm})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Model: Extra-trees "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model\n",
    "regr_extr = ExtraTreesRegressor(n_estimators=param_extra['n_estimators'], \n",
    "                                criterion=param_extra['criterion'],\n",
    "                                max_depth=param_extra['max_depth'],\n",
    "                                min_samples_split=param_extra['min_samples_split'],\n",
    "                                max_features=param_extra['max_features'],\n",
    "                                random_state=param_extra['random_state'])\n",
    "# Fit model\n",
    "regr_extr.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Save model\n",
    "pickle.dump(regr_extr, open(os.path.join(save_path, 'model', 'extra_tree.model'), 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_extr = regr_extr.predict(X_test_scaled)    # Prediction on test dataset\n",
    "rmse_extr = mean_squared_error(y_test, pred_extr)    # Calculate RMSE\n",
    "pred_extr_df = pd.DataFrame({'real':  y_test['cum_res_cnt'].values, 'prediction': pred_extr})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Model: XGboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make dataset\n",
    "dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "dtest = xgb.DMatrix(X_test, label=y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regr_xgb = xgb.train(xgb_param, dtrain, num_round)\n",
    "regr_xgb.save_model(os.path.join(save_path, 'model', 'xgboost.model'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_xgb = regr_xgb.predict(dtest)\n",
    "rmse_xgb = mean_squared_error(y_test, pred_xgb)\n",
    "pred_xgv_df = pd.DataFrame({'real':  y_test['cum_res_cnt'].values, 'prediction': pred_xgb}, index=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Comparing results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse_svm, rmse_extr, rmse_xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_svm_df = pred_svm_df.sort_values(by=['real'])\n",
    "pred_svm_df = pred_svm_df.reset_index(drop=True)\n",
    "\n",
    "pred_extr_df = pred_extr_df.sort_values(by=['real'])\n",
    "pred_extr_df = pred_extr_df.reset_index(drop=True)\n",
    "\n",
    "pred_xgv_df = pred_xgv_df.sort_values(by=['real'])\n",
    "pred_xgv_df = pred_xgv_df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot SVM model result\n",
    "fig, axes = plt.subplots(1, 1)\n",
    "pred_svm_df['real'].plot.line(figsize=(15,6), linewidth=1.1, \n",
    "                              color='firebrick', label='Real', ax=axes)\n",
    "pred_svm_df['prediction'].plot.line(figsize=(15,6),linewidth=0.9, alpha=0.4,\n",
    "                                     color='coral', label='Prediction', ax=axes)\n",
    "axes.legend()\n",
    "plt.title('Result: SVM')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Extra-trees model result\n",
    "fig, axes = plt.subplots(1, 1)\n",
    "pred_extr_df['real'].plot.line(figsize=(15,6), linewidth=1.1, \n",
    "                              color='firebrick',  label='Real', ax=axes)\n",
    "pred_extr_df['prediction'].plot.line(figsize=(15,6),linewidth=0.9, alpha=0.4,\n",
    "                                     color='coral', label='Prediction', ax=axes)\n",
    "axes.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Plot XGboost model result\n",
    "fig, axes = plt.subplots(1, 1)\n",
    "pred_xgv_df['real'].plot.line(figsize=(15,6), linewidth=1.1, \n",
    "                              color='firebrick', label='Real', ax=axes)\n",
    "pred_xgv_df['prediction'].plot.line(figsize=(15,6),linewidth=0.9, alpha=0.4,\n",
    "                                     color='coral', label='Prediction', ax=axes)\n",
    "axes.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training : Extra-trees Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Set model and parameter grids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Training models\n",
    "regressors = {}\n",
    "regressors.update({\"Extra Trees Regressor\": ExtraTreesRegressor()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameter grid\n",
    "param_grids = {}\n",
    "param_grids.update({\"Extra Trees Regressor\": {\n",
    "    'n_estimators': list(np.arange(100, 500, 100)),\n",
    "    'criterion': ['mse'],\n",
    "    'min_samples_split': list(np.arange(2, 6, 1)),   # minimum number of samples required to split inner node \n",
    "    'min_samples_leaf': list(np.arange(1, 6, 1)),    # have the effect of smoothing the model\n",
    "    'max_features': ['auto']\n",
    "}})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Grid Search Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_best_hyper_param(x, y, regr, scoring='neg_root_mean_squared_error'):\n",
    "    # Select regressor algorithm\n",
    "    regressor = regressors[regr]\n",
    "    \n",
    "    # Define parameter grid\n",
    "    param_grid = param_grids[regr]\n",
    "    \n",
    "    # Initialize Grid Search object\n",
    "    gscv = GridSearchCV(estimator=regressor, param_grid=param_grid,\n",
    "                        scoring=scoring, n_jobs=1, cv=5, verbose=1)\n",
    "    \n",
    "    # Fit gscv\n",
    "    print(f'Tuning {regr}')\n",
    "    gscv.fit(x, y)\n",
    "    \n",
    "    # Get best paramters and score\n",
    "    best_params = gscv.best_params_\n",
    "    best_score = gscv.best_score_\n",
    "    \n",
    "    # Update regressor paramters\n",
    "    regressor.set_params(**best_params)\n",
    "    \n",
    "    return regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Best paramter grid for Avante\n",
    "m2_av_extr_best = get_best_hyper_param(x=m2_av_x_train, y=m2_av_y_train.to_numpy(), \n",
    "                                       regr='Extra Trees Regressor')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best paramter grid for K3\n",
    "m2_k3_extr_best = get_best_hyper_param(x=m2_k3_x_train, y=m2_k3_y_train.to_numpy(), \n",
    "                                       regr='Extra Trees Regressor')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best paramter grid for Veloster\n",
    "m2_vl_extr_best = get_best_hyper_param(x=m2_vl_x_train, y=m2_vl_y_train.to_numpy(), \n",
    "                                       regr='Extra Trees Regressor')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Save best parameter grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save best paramters of extra-trees regressor\n",
    "\n",
    "# Avante\n",
    "m2_av_extr_best_params = m2_av_extr_best.get_params()\n",
    "f = open(os.path.join(save_path, 'model', 'model_2', \"m2_av_extr_best_params.pickle\"), \"wb\")\n",
    "pickle.dump(m2_av_extr_best_params, f)\n",
    "f.close()\n",
    "\n",
    "# K3\n",
    "m2_k3_extr_best_params= m2_k3_extr_best.get_params()\n",
    "f = open(os.path.join(save_path, 'model', 'model_2', \"m2_k3_extr_best_params.pickle\"), \"wb\")\n",
    "pickle.dump(m2_k3_extr_best_params, f)\n",
    "f.close()\n",
    "\n",
    "# Veloster\n",
    "m2_vl_extr_best_params= m2_vl_extr_best.get_params()\n",
    "f = open(os.path.join(save_path, 'model', 'model_2', \"m2_vl_extr_best_params.pickle\"), \"wb\")\n",
    "pickle.dump(m2_vl_extr_best_params, f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load paramters and fit the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Load best hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Avante\n",
    "f = open(os.path.join(save_path, 'model', \"m2_av_extr_best_params.pickle\"), \"rb\")\n",
    "m2_av_extr_best_params = pickle.load(f)\n",
    "f.close()\n",
    "\n",
    "# K3\n",
    "f = open(os.path.join(save_path, 'model', \"m2_k3_extr_best_params.pickle\"), \"rb\")\n",
    "m2_k3_extr_best_params = pickle.load(f)\n",
    "f.close()\n",
    "\n",
    "# Veloster\n",
    "f = open(os.path.join(save_path, 'model', \"m2_vl_extr_best_params.pickle\"), \"rb\")\n",
    "m2_vl_extr_best_params = pickle.load(f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Fit the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ExtraTreesRegressor(n_estimators=300)"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Avante\n",
    "extr_av = ExtraTreesRegressor(**m2_av_extr_best_params)\n",
    "extr_av.fit(m2_av_x, m2_av_y)\n",
    "\n",
    "# K3\n",
    "extr_k3 = ExtraTreesRegressor(**m2_k3_extr_best_params)\n",
    "extr_k3.fit(m2_k3_x, m2_k3_y)\n",
    "\n",
    "# Veloster\n",
    "extr_vl = ExtraTreesRegressor(**m2_vl_extr_best_params)\n",
    "extr_vl.fit(m2_vl_x, m2_vl_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Set initial variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting\n",
    "seasonality = pd.read_csv(os.path.join(save_path, 'data', 'seasonality.csv'))\n",
    "seasonality['rent_day'] = pd.to_datetime(seasonality['rent_day'], format='%Y-%m-%d')\n",
    "day_to_season = {day: season for day, season in zip(seasonality['rent_day'], \n",
    "                                                    seasonality['seasonality'])}\n",
    "# Initial discount rate for each season\n",
    "season_init_discount = {1: 70,   # Weekdays\n",
    "                        2: 60,   # Weekends\n",
    "                        3: 45,   # Holidays\n",
    "                        4: 10}    # Peak Season\n",
    "\n",
    "# Target rent day\n",
    "predict_day = '2020.9.27'\n",
    "\n",
    "# Lead Time Setting\n",
    "lead_time = np.arange(-83, 1, 1)\n",
    "lead_time_vec = np.arange(-36, 1, 1)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Make dataframe of initial setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get season value and initial discount rate\n",
    "predict_datetime = dt.datetime(*list(map(int, predict_day.split('.'))))    # Convert to datetime\n",
    "season = day_to_season[predict_datetime]    # Get seasonality on the day\n",
    "init_discount = season_init_discount[season]\n",
    "\n",
    "# Initial capacity of each car model\n",
    "init_capa_av = 40\n",
    "init_capa_k3 = 35\n",
    "init_capa_vl = 35\n",
    "\n",
    "# Inintial settting to dataframe\n",
    "pred_input = pd.DataFrame({'season': season, 'lead_time': lead_time_vec, 'discount': init_discount})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Predict cum. reservation counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction\n",
    "extr_pred_av = extr_av.predict(pred_input)    # Avante\n",
    "extr_pred_k3 = extr_k3.predict(pred_input)    # K3\n",
    "extr_pred_vl = extr_vl.predict(pred_input)    # Veloster\n",
    "\n",
    "# Correct decimal point\n",
    "vfun = np.vectorize(lambda x: round(x, 2))\n",
    "extr_pred_av = vfun(extr_pred_av)\n",
    "extr_pred_k3 = vfun(extr_pred_k3)\n",
    "extr_pred_vl = vfun(extr_pred_vl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "lt_to_lt_vec = {-1 * i : (((i//7) + 24) * -1 if i > 28 else i * -1) for i in range(0, 84, 1)}\n",
    "\n",
    "lt_vec_to_av = {lt_vec: av for lt_vec, av in zip(lead_time_vec, extr_pred_av)}\n",
    "lt_vec_to_k3 = {lt_vec: k3 for lt_vec, k3 in zip(lead_time_vec, extr_pred_k3)}\n",
    "lt_vec_to_vl = {lt_vec: vl for lt_vec, vl in zip(lead_time_vec, extr_pred_vl)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Make result dataframe\n",
    "exp_res = pd.DataFrame({\n",
    "        'date': [predict_datetime - dt.timedelta(days=int(i*-1)) for i in lead_time],\n",
    "        'lead_time': lead_time,\n",
    "        'curr_discount': init_discount,\n",
    "        'capa_av': init_capa_av,\n",
    "        'exp_cum_av': [lt_vec_to_av[lt_to_lt_vec[i]] for i in lead_time],\n",
    "        'exp_res_rate_av': [lt_vec_to_av[lt_to_lt_vec[i]]/init_capa_av for i in lead_time],\n",
    "        'capa_k3': init_capa_k3,\n",
    "        'exp_cum_k3': [lt_vec_to_k3[lt_to_lt_vec[i]] for i in lead_time],\n",
    "        'exp_res_rate_k3': [lt_vec_to_k3[lt_to_lt_vec[i]]/init_capa_k3 for i in lead_time],\n",
    "        'capa_vl': init_capa_vl,\n",
    "        'exp_cum_vl': [lt_vec_to_vl[lt_to_lt_vec[i]] for i in lead_time],\n",
    "        'exp_res_rate_vl': [lt_vec_to_vl[lt_to_lt_vec[i]]/init_capa_vl for i in lead_time]\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Save result dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_res['date'] = exp_res['date'].apply(lambda x: x.strftime('%Y.%m.%d'))\n",
    "exp_res.T.to_csv(os.path.join(save_path, 'data', 'exp_cum_res_cnt(' + predict_day + ').csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 3 : Optimize Best Pricing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define Discount Recommendation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "# theta2 : Season 및 lead time 별 할인율 조정 전략에 관한 기준 예약률 산출\n",
    "# 과거 season 및 lead time 별 예약률 트렌드 데이터를 기준으로 산출\n",
    "def calc_tehta2(curr_exp_cnt: int, exp_res_cnt: int, exp_capa: int):\n",
    "    return round((curr_exp_cnt + exp_res_cnt) / exp_capa, 2)\n",
    "\n",
    "def dscnt_rec_fun(r: float, exp_res_rate: float, d: float):\n",
    "    \"\"\"\n",
    "    Customized Exponential function\n",
    "    r: Current Reservation Rate\n",
    "    d: Exp Demand Change Rate \n",
    "    ex_res_rate: Exp. Reservation Rate\n",
    "    \"\"\"\n",
    "    # Hyperparamters (need to tune)\n",
    "    theta1 = 1.2       # ratio of increasing / decreasing magnitude : discount\n",
    "    theta2 = 1         # ratio of increasing / decreasing magnitude : demand\n",
    "    phi_low = 1.7    # 1 < phi_low < phi_high < 2\n",
    "    phi_high = 1.4   # 1 < phi_low < phi_high < 2\n",
    "\n",
    "    if d > 0:\n",
    "        y = 1 - theta1 * (r ** (phi_high ** (-1 * (r * theta2 * d))) - exp_res_rate)\n",
    "    else:\n",
    "        y = 1 - theta1 * (r ** (phi_low ** (-1 * ((1 - r) * theta2 * d))) - exp_res_rate)\n",
    "        \n",
    "    return y\n",
    "\n",
    "def calc_sug_disc_rate():\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate each "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_dmd_change = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_res['rec_rate_av'] = exp_res['exp_res_rate_av'].apply(lambda x: dscnt_rec_fun(r=x, d=0, exp_res_rate=x))\n",
    "exp_res['rec_rate_k3'] = exp_res['exp_res_rate_k3'].apply(lambda x: dscnt_rec_fun(r=x, d=0, exp_res_rate=x))\n",
    "exp_res['rec_rate_vl'] = exp_res['exp_res_rate_vl'].apply(lambda x: dscnt_rec_fun(r=x, d=0, exp_res_rate=x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_res['rec_dscnt_av'] = exp_res['curr_discount'] * exp_res['rec_rate_av']\n",
    "exp_res['rec_dscnt_k3'] = exp_res['curr_discount'] * exp_res['rec_rate_k3']\n",
    "exp_res['rec_dscnt_vl'] = exp_res['curr_discount'] * exp_res['rec_rate_vl']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>lead_time</th>\n",
       "      <th>curr_discount</th>\n",
       "      <th>capa_av</th>\n",
       "      <th>exp_cum_av</th>\n",
       "      <th>exp_res_rate_av</th>\n",
       "      <th>capa_k3</th>\n",
       "      <th>exp_cum_k3</th>\n",
       "      <th>exp_res_rate_k3</th>\n",
       "      <th>capa_vl</th>\n",
       "      <th>exp_cum_vl</th>\n",
       "      <th>exp_res_rate_vl</th>\n",
       "      <th>rec_dscnt_av</th>\n",
       "      <th>rec_dscnt_k3</th>\n",
       "      <th>rec_dscnt_vl</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2020.07.06</td>\n",
       "      <td>-83</td>\n",
       "      <td>10</td>\n",
       "      <td>40</td>\n",
       "      <td>2.43</td>\n",
       "      <td>0.06075</td>\n",
       "      <td>35</td>\n",
       "      <td>3.73</td>\n",
       "      <td>0.106571</td>\n",
       "      <td>35</td>\n",
       "      <td>3.73</td>\n",
       "      <td>0.106571</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2020.07.07</td>\n",
       "      <td>-82</td>\n",
       "      <td>10</td>\n",
       "      <td>40</td>\n",
       "      <td>2.43</td>\n",
       "      <td>0.06075</td>\n",
       "      <td>35</td>\n",
       "      <td>3.73</td>\n",
       "      <td>0.106571</td>\n",
       "      <td>35</td>\n",
       "      <td>3.73</td>\n",
       "      <td>0.106571</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2020.07.08</td>\n",
       "      <td>-81</td>\n",
       "      <td>10</td>\n",
       "      <td>40</td>\n",
       "      <td>2.43</td>\n",
       "      <td>0.06075</td>\n",
       "      <td>35</td>\n",
       "      <td>3.73</td>\n",
       "      <td>0.106571</td>\n",
       "      <td>35</td>\n",
       "      <td>3.73</td>\n",
       "      <td>0.106571</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2020.07.09</td>\n",
       "      <td>-80</td>\n",
       "      <td>10</td>\n",
       "      <td>40</td>\n",
       "      <td>2.43</td>\n",
       "      <td>0.06075</td>\n",
       "      <td>35</td>\n",
       "      <td>3.73</td>\n",
       "      <td>0.106571</td>\n",
       "      <td>35</td>\n",
       "      <td>3.73</td>\n",
       "      <td>0.106571</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2020.07.10</td>\n",
       "      <td>-79</td>\n",
       "      <td>10</td>\n",
       "      <td>40</td>\n",
       "      <td>2.43</td>\n",
       "      <td>0.06075</td>\n",
       "      <td>35</td>\n",
       "      <td>3.73</td>\n",
       "      <td>0.106571</td>\n",
       "      <td>35</td>\n",
       "      <td>3.73</td>\n",
       "      <td>0.106571</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>2020.09.23</td>\n",
       "      <td>-4</td>\n",
       "      <td>10</td>\n",
       "      <td>40</td>\n",
       "      <td>31.13</td>\n",
       "      <td>0.77825</td>\n",
       "      <td>35</td>\n",
       "      <td>24.58</td>\n",
       "      <td>0.702286</td>\n",
       "      <td>35</td>\n",
       "      <td>24.53</td>\n",
       "      <td>0.700857</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>2020.09.24</td>\n",
       "      <td>-3</td>\n",
       "      <td>10</td>\n",
       "      <td>40</td>\n",
       "      <td>33.49</td>\n",
       "      <td>0.83725</td>\n",
       "      <td>35</td>\n",
       "      <td>26.56</td>\n",
       "      <td>0.758857</td>\n",
       "      <td>35</td>\n",
       "      <td>26.23</td>\n",
       "      <td>0.749429</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>2020.09.25</td>\n",
       "      <td>-2</td>\n",
       "      <td>10</td>\n",
       "      <td>40</td>\n",
       "      <td>35.16</td>\n",
       "      <td>0.87900</td>\n",
       "      <td>35</td>\n",
       "      <td>29.33</td>\n",
       "      <td>0.838000</td>\n",
       "      <td>35</td>\n",
       "      <td>29.40</td>\n",
       "      <td>0.840000</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>2020.09.26</td>\n",
       "      <td>-1</td>\n",
       "      <td>10</td>\n",
       "      <td>40</td>\n",
       "      <td>38.14</td>\n",
       "      <td>0.95350</td>\n",
       "      <td>35</td>\n",
       "      <td>33.88</td>\n",
       "      <td>0.968000</td>\n",
       "      <td>35</td>\n",
       "      <td>33.84</td>\n",
       "      <td>0.966857</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>2020.09.27</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>40</td>\n",
       "      <td>37.03</td>\n",
       "      <td>0.92575</td>\n",
       "      <td>35</td>\n",
       "      <td>32.29</td>\n",
       "      <td>0.922571</td>\n",
       "      <td>35</td>\n",
       "      <td>32.26</td>\n",
       "      <td>0.921714</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>84 rows × 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          date  lead_time  curr_discount  capa_av  exp_cum_av  \\\n",
       "0   2020.07.06        -83             10       40        2.43   \n",
       "1   2020.07.07        -82             10       40        2.43   \n",
       "2   2020.07.08        -81             10       40        2.43   \n",
       "3   2020.07.09        -80             10       40        2.43   \n",
       "4   2020.07.10        -79             10       40        2.43   \n",
       "..         ...        ...            ...      ...         ...   \n",
       "79  2020.09.23         -4             10       40       31.13   \n",
       "80  2020.09.24         -3             10       40       33.49   \n",
       "81  2020.09.25         -2             10       40       35.16   \n",
       "82  2020.09.26         -1             10       40       38.14   \n",
       "83  2020.09.27          0             10       40       37.03   \n",
       "\n",
       "    exp_res_rate_av  capa_k3  exp_cum_k3  exp_res_rate_k3  capa_vl  \\\n",
       "0           0.06075       35        3.73         0.106571       35   \n",
       "1           0.06075       35        3.73         0.106571       35   \n",
       "2           0.06075       35        3.73         0.106571       35   \n",
       "3           0.06075       35        3.73         0.106571       35   \n",
       "4           0.06075       35        3.73         0.106571       35   \n",
       "..              ...      ...         ...              ...      ...   \n",
       "79          0.77825       35       24.58         0.702286       35   \n",
       "80          0.83725       35       26.56         0.758857       35   \n",
       "81          0.87900       35       29.33         0.838000       35   \n",
       "82          0.95350       35       33.88         0.968000       35   \n",
       "83          0.92575       35       32.29         0.922571       35   \n",
       "\n",
       "    exp_cum_vl  exp_res_rate_vl  rec_dscnt_av  rec_dscnt_k3  rec_dscnt_vl  \n",
       "0         3.73         0.106571          10.0          10.0          10.0  \n",
       "1         3.73         0.106571          10.0          10.0          10.0  \n",
       "2         3.73         0.106571          10.0          10.0          10.0  \n",
       "3         3.73         0.106571          10.0          10.0          10.0  \n",
       "4         3.73         0.106571          10.0          10.0          10.0  \n",
       "..         ...              ...           ...           ...           ...  \n",
       "79       24.53         0.700857          10.0          10.0          10.0  \n",
       "80       26.23         0.749429          10.0          10.0          10.0  \n",
       "81       29.40         0.840000          10.0          10.0          10.0  \n",
       "82       33.84         0.966857          10.0          10.0          10.0  \n",
       "83       32.26         0.921714          10.0          10.0          10.0  \n",
       "\n",
       "[84 rows x 15 columns]"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exp_res = exp_res.drop(columns=['rec_rate_av', 'rec_rate_k3', 'rec_rate_vl'], errors='ignore')\n",
    "exp_res = exp_res[['date', 'lead_time', 'curr_discount', \n",
    "                   'capa_av', 'exp_cum_av', 'exp_res_rate_av', 'rec_dscnt_av', \n",
    "                   'capa_k3', 'exp_cum_k3', 'exp_res_rate_k3', 'rec_dscnt_k3', \n",
    "                   'capa_vl', 'exp_cum_vl', 'exp_res_rate_vl', 'rec_dscnt_vl', \n",
    "                  ]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save Recommendation Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_res.T.to_csv(os.path.join(save_path, 'data', 'dscnt_rec(' + predict_day + ').csv'), header=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### After Recommendation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "season_1 = res[res['seasonality'] == 1].rent_period/24\n",
    "season_2 = res[res['seasonality'] == 2].rent_period/24\n",
    "season_3 = res[res['seasonality'] == 3].rent_period/24\n",
    "season_4 = res[res['seasonality'] == 4].rent_period/24\n",
    "\n",
    "# Remove Outlier of Rent days(above 7days)\n",
    "season_1 = season_1[season_1 < 8]\n",
    "season_2 = season_2[season_2 < 8]\n",
    "season_3 = season_3[season_3 < 8]\n",
    "season_4 = season_4[season_4 < 8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_rent_period_to_day(season):\n",
    "    conditions = [\n",
    "        (season <= 1),\n",
    "        ((season > 1) & (season <= 2)),\n",
    "        ((season > 2) & (season <= 3)),\n",
    "        ((season > 3) & (season <= 4)),\n",
    "        ((season > 4) & (season <= 5)),\n",
    "        ((season > 5) & (season <= 6)),\n",
    "        ((season > 6) & (season <= 7))\n",
    "    ]\n",
    "    \n",
    "    values = [1, 2, 3, 4, 5, 6, 7]\n",
    "    season_df = pd.DataFrame({'rent_period': season.values})\n",
    "    season_df['rent_period_hours'] = season_df['rent_period'] * 24\n",
    "    season_df['rent_days'] = season_df['rent_period_hours'] // 24\n",
    "    season_df['rent_hours'] = season_df['rent_period_hours'] % 4\n",
    "    season_df['cdw_days'] = np.select(conditions, values)\n",
    "    \n",
    "    return season_df    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rental Fee\n",
    "fee = {\n",
    "    'group_1': {\n",
    "        1: 18000, 2: 36000, 3: 54000, 4: 73000, 5: 73000, 6: 73000, \n",
    "        7: 91000, 8: 104000, 9: 104000, 10: 104000, 11: 104000, 12: 104000,\n",
    "        13: 122000, 14: 130000, 15: 130000, 16: 130000, 17: 130000, 18: 130000,\n",
    "        19: 130000, 20: 130000, 21: 130000, 22: 130000, 23: 130000, 24: 130000    \n",
    "    },\n",
    "    'group_2': {\n",
    "        1: 17000, 2: 34000, 3: 51000, 4: 67000, 5: 67000, 6: 67000, \n",
    "        7: 84000, 8: 96400, 9: 96400, 10: 96400, 11: 96400, 12: 96400,\n",
    "        13: 113000, 14: 120000, 15: 120000, 16: 120000, 17: 120000, 18: 120000,\n",
    "        19: 120000, 20: 120000, 21: 120000, 22: 120000, 23: 120000, 24: 120000    \n",
    "    }\n",
    "}\n",
    "\n",
    "cdw = {\n",
    "    'group_1': {\n",
    "        1: 24000, 2: 48000, 3: 66900, 4: 89200, 5: 111500, 6: 133800, 7: 119700\n",
    "    },\n",
    "    'group_2': {\n",
    "        1: 20000, 2: 34000, 3: 51000, 4: 67000, 5: 67000, 6: 67000, 7: 84000 \n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "season_1_df = convert_rent_period_to_day(season_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "season_1_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "res_20['res_model_nm'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additional Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "season_1 = res[res['seasonality'] == 1].rent_period/24\n",
    "season_2 = res[res['seasonality'] == 2].rent_period/24\n",
    "season_3 = res[res['seasonality'] == 3].rent_period/24\n",
    "season_4 = res[res['seasonality'] == 4].rent_period/24\n",
    "\n",
    "season_1 = season_1[season_1 < 8]\n",
    "season_2 = season_2[season_2 < 8]\n",
    "season_3 = season_3[season_3 < 8]\n",
    "season_4 = season_4[season_4 < 8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(rc={'figure.figsize':(8, 6)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(season_1, hist=True, kde=True, bins=50, color = 'darkblue',\n",
    "             hist_kws={'edgecolor':'black'}, kde_kws={'linewidth': 1.2}).set_title(\"Weekdays\")\n",
    "plt.savefig(os.path.join(save_path, 'img', 'hist', 'Rent Periods (Weekdays).png'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(season_2, hist=True, kde=True, bins=50, color = 'darkblue',\n",
    "             hist_kws={'edgecolor':'black'}, kde_kws={'linewidth': 1.2}).set_title(\"Weekends\")\n",
    "plt.savefig(os.path.join(save_path, 'img', 'hist', 'Rent Periods (Weekends).png'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(season_3, hist=True, kde=True, bins=50, color = 'darkblue',\n",
    "             hist_kws={'edgecolor':'black'}, kde_kws={'linewidth': 1.2}).set_title(\"Holidays\")\n",
    "plt.savefig(os.path.join(save_path, 'img', 'hist', 'Rent Periods (Holidays).png'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sns.distplot(season_4, hist=True, kde=True, bins=50, color = 'darkblue',\n",
    "             hist_kws={'edgecolor':'black'}, kde_kws={'linewidth': 1.2}).set_title(\"Peak Season\")\n",
    "plt.savefig(os.path.join(save_path, 'img', 'hist', 'Rent Periods (Peak Season).png'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def convert_rent_period_to_day(season):\n",
    "    conditions = [\n",
    "        (season <= 1),\n",
    "        ((season > 1) & (season <= 2)),\n",
    "        ((season > 2) & (season <= 3)),\n",
    "        ((season > 3) & (season <= 4)),\n",
    "        ((season > 4) & (season <= 5)),\n",
    "        ((season > 5) & (season <= 6)),\n",
    "        ((season > 6) & (season <= 7))\n",
    "    ]\n",
    "    \n",
    "    values = [1, 2, 3, 4, 5, 6, 7]\n",
    "    season_df = pd.DataFrame({'rent_period': season.values})\n",
    "    season_df['rent_period_hours'] = season_df['rent_period'] * 24\n",
    "    season_df['rent_days'] = season_df['rent_period_hours'] // 24\n",
    "    season_df['rent_hours'] = season_df['rent_period_hours'] % 24\n",
    "    season_df['cdw_days'] = np.select(conditions, values)\n",
    "    \n",
    "    return season_df    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "season_1_df = convert_rent_period_to_day(season_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "349.091px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
