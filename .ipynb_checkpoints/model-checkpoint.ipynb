{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Import-Dataset\" data-toc-modified-id=\"Import-Dataset-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Import Dataset</a></span><ul class=\"toc-item\"><li><span><a href=\"#Define-File-Path\" data-toc-modified-id=\"Define-File-Path-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Define File Path</a></span></li><li><span><a href=\"#Load-File\" data-toc-modified-id=\"Load-File-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>Load File</a></span></li><li><span><a href=\"#API-Data\" data-toc-modified-id=\"API-Data-1.3\"><span class=\"toc-item-num\">1.3&nbsp;&nbsp;</span>API Data</a></span></li></ul></li><li><span><a href=\"#Data-Preprocessing\" data-toc-modified-id=\"Data-Preprocessing-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Data Preprocessing</a></span><ul class=\"toc-item\"><li><span><a href=\"#Merge-Dataset\" data-toc-modified-id=\"Merge-Dataset-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Merge Dataset</a></span></li><li><span><a href=\"#Remap-Columns\" data-toc-modified-id=\"Remap-Columns-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>Remap Columns</a></span></li><li><span><a href=\"#Check-NA-Values\" data-toc-modified-id=\"Check-NA-Values-2.3\"><span class=\"toc-item-num\">2.3&nbsp;&nbsp;</span>Check NA Values</a></span></li><li><span><a href=\"#Fill-Na-values\" data-toc-modified-id=\"Fill-Na-values-2.4\"><span class=\"toc-item-num\">2.4&nbsp;&nbsp;</span>Fill Na values</a></span></li><li><span><a href=\"#Change-Data-Types\" data-toc-modified-id=\"Change-Data-Types-2.5\"><span class=\"toc-item-num\">2.5&nbsp;&nbsp;</span>Change Data Types</a></span></li><li><span><a href=\"#Filter-Dataset\" data-toc-modified-id=\"Filter-Dataset-2.6\"><span class=\"toc-item-num\">2.6&nbsp;&nbsp;</span>Filter Dataset</a></span></li><li><span><a href=\"#Data-Grouping\" data-toc-modified-id=\"Data-Grouping-2.7\"><span class=\"toc-item-num\">2.7&nbsp;&nbsp;</span>Data Grouping</a></span></li><li><span><a href=\"#Drop-Columns\" data-toc-modified-id=\"Drop-Columns-2.8\"><span class=\"toc-item-num\">2.8&nbsp;&nbsp;</span>Drop Columns</a></span></li><li><span><a href=\"#Ordering\" data-toc-modified-id=\"Ordering-2.9\"><span class=\"toc-item-num\">2.9&nbsp;&nbsp;</span>Ordering</a></span></li></ul></li><li><span><a href=\"#Data-Preprocessing-for-each-dataset\" data-toc-modified-id=\"Data-Preprocessing-for-each-dataset-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Data Preprocessing for each dataset</a></span><ul class=\"toc-item\"><li><span><a href=\"#Reservation-Dataset\" data-toc-modified-id=\"Reservation-Dataset-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>Reservation Dataset</a></span></li><li><span><a href=\"#Jeju-Visit-Dataset\" data-toc-modified-id=\"Jeju-Visit-Dataset-3.2\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>Jeju Visit Dataset</a></span></li><li><span><a href=\"#Discount-Dataset\" data-toc-modified-id=\"Discount-Dataset-3.3\"><span class=\"toc-item-num\">3.3&nbsp;&nbsp;</span>Discount Dataset</a></span></li><li><span><a href=\"#Merge-Dataset\" data-toc-modified-id=\"Merge-Dataset-3.4\"><span class=\"toc-item-num\">3.4&nbsp;&nbsp;</span>Merge Dataset</a></span></li><li><span><a href=\"#Add-seasonality-column\" data-toc-modified-id=\"Add-seasonality-column-3.5\"><span class=\"toc-item-num\">3.5&nbsp;&nbsp;</span>Add seasonality column</a></span></li><li><span><a href=\"#Add-Lead-Time-Interval\" data-toc-modified-id=\"Add-Lead-Time-Interval-3.6\"><span class=\"toc-item-num\">3.6&nbsp;&nbsp;</span>Add Lead Time Interval</a></span></li><li><span><a href=\"#Convert-Rent-Periods-to-hours\" data-toc-modified-id=\"Convert-Rent-Periods-to-hours-3.7\"><span class=\"toc-item-num\">3.7&nbsp;&nbsp;</span>Convert Rent Periods to hours</a></span></li><li><span><a href=\"#Update-Discount-Rate\" data-toc-modified-id=\"Update-Discount-Rate-3.8\"><span class=\"toc-item-num\">3.8&nbsp;&nbsp;</span>Update Discount Rate</a></span></li><li><span><a href=\"#Calculate-reservation-fee\" data-toc-modified-id=\"Calculate-reservation-fee-3.9\"><span class=\"toc-item-num\">3.9&nbsp;&nbsp;</span>Calculate reservation fee</a></span></li><li><span><a href=\"#Remove-Exception\" data-toc-modified-id=\"Remove-Exception-3.10\"><span class=\"toc-item-num\">3.10&nbsp;&nbsp;</span>Remove Exception</a></span></li></ul></li><li><span><a href=\"#Model-Input\" data-toc-modified-id=\"Model-Input-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Model Input</a></span><ul class=\"toc-item\"><li><span><a href=\"#Input----Reservation-Counts\" data-toc-modified-id=\"Input----Reservation-Counts-4.1\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;</span>Input  - Reservation Counts</a></span><ul class=\"toc-item\"><li><span><a href=\"#Drop-Unnecessary-columns\" data-toc-modified-id=\"Drop-Unnecessary-columns-4.1.1\"><span class=\"toc-item-num\">4.1.1&nbsp;&nbsp;</span>Drop Unnecessary columns</a></span></li><li><span><a href=\"#Data-Split\" data-toc-modified-id=\"Data-Split-4.1.2\"><span class=\"toc-item-num\">4.1.2&nbsp;&nbsp;</span>Data Split</a></span></li><li><span><a href=\"#Save--Dataset\" data-toc-modified-id=\"Save--Dataset-4.1.3\"><span class=\"toc-item-num\">4.1.3&nbsp;&nbsp;</span>Save  Dataset</a></span></li></ul></li><li><span><a href=\"#Model-2-Input---Cum.-Reservation-Counts\" data-toc-modified-id=\"Model-2-Input---Cum.-Reservation-Counts-4.2\"><span class=\"toc-item-num\">4.2&nbsp;&nbsp;</span>Model 2 Input - Cum. Reservation Counts</a></span><ul class=\"toc-item\"><li><span><a href=\"#Slicing\" data-toc-modified-id=\"Slicing-4.2.1\"><span class=\"toc-item-num\">4.2.1&nbsp;&nbsp;</span>Slicing</a></span></li><li><span><a href=\"#Grouping\" data-toc-modified-id=\"Grouping-4.2.2\"><span class=\"toc-item-num\">4.2.2&nbsp;&nbsp;</span>Grouping</a></span></li><li><span><a href=\"#Merge\" data-toc-modified-id=\"Merge-4.2.3\"><span class=\"toc-item-num\">4.2.3&nbsp;&nbsp;</span>Merge</a></span></li><li><span><a href=\"#Re-Group-on-'seasonality',-'lead-time'\" data-toc-modified-id=\"Re-Group-on-'seasonality',-'lead-time'-4.2.4\"><span class=\"toc-item-num\">4.2.4&nbsp;&nbsp;</span>Re-Group on 'seasonality', 'lead time'</a></span></li><li><span><a href=\"#Save-Dataset\" data-toc-modified-id=\"Save-Dataset-4.2.5\"><span class=\"toc-item-num\">4.2.5&nbsp;&nbsp;</span>Save Dataset</a></span></li></ul></li></ul></li><li><span><a href=\"#Model-1:-Predict-Jeju-Visitors\" data-toc-modified-id=\"Model-1:-Predict-Jeju-Visitors-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Model 1: Predict Jeju Visitors</a></span><ul class=\"toc-item\"><li><span><a href=\"#Preprocessing\" data-toc-modified-id=\"Preprocessing-5.1\"><span class=\"toc-item-num\">5.1&nbsp;&nbsp;</span>Preprocessing</a></span></li><li><span><a href=\"#Define-Models-&amp;-Validation-Method\" data-toc-modified-id=\"Define-Models-&amp;-Validation-Method-5.2\"><span class=\"toc-item-num\">5.2&nbsp;&nbsp;</span>Define Models &amp; Validation Method</a></span></li><li><span><a href=\"#Evaluation-Using-Walk-Forward-Validation\" data-toc-modified-id=\"Evaluation-Using-Walk-Forward-Validation-5.3\"><span class=\"toc-item-num\">5.3&nbsp;&nbsp;</span>Evaluation Using Walk Forward Validation</a></span></li></ul></li><li><span><a href=\"#Model-2-:-Predict-Cum.-Reservation-Counts\" data-toc-modified-id=\"Model-2-:-Predict-Cum.-Reservation-Counts-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>Model 2 : Predict Cum. Reservation Counts</a></span><ul class=\"toc-item\"><li><span><a href=\"#Load-Dataset\" data-toc-modified-id=\"Load-Dataset-6.1\"><span class=\"toc-item-num\">6.1&nbsp;&nbsp;</span>Load Dataset</a></span></li><li><span><a href=\"#Split-train-and-test-dataset\" data-toc-modified-id=\"Split-train-and-test-dataset-6.2\"><span class=\"toc-item-num\">6.2&nbsp;&nbsp;</span>Split train and test dataset</a></span></li><li><span><a href=\"#Scaling\" data-toc-modified-id=\"Scaling-6.3\"><span class=\"toc-item-num\">6.3&nbsp;&nbsp;</span>Scaling</a></span></li><li><span><a href=\"#Pre-training\" data-toc-modified-id=\"Pre-training-6.4\"><span class=\"toc-item-num\">6.4&nbsp;&nbsp;</span>Pre-training</a></span><ul class=\"toc-item\"><li><span><a href=\"#SVM-Model\" data-toc-modified-id=\"SVM-Model-6.4.1\"><span class=\"toc-item-num\">6.4.1&nbsp;&nbsp;</span>SVM Model</a></span></li><li><span><a href=\"#Extra-trees-Model\" data-toc-modified-id=\"Extra-trees-Model-6.4.2\"><span class=\"toc-item-num\">6.4.2&nbsp;&nbsp;</span>Extra-trees Model</a></span></li><li><span><a href=\"#XGboost-Model\" data-toc-modified-id=\"XGboost-Model-6.4.3\"><span class=\"toc-item-num\">6.4.3&nbsp;&nbsp;</span>XGboost Model</a></span></li><li><span><a href=\"#Comparing-results\" data-toc-modified-id=\"Comparing-results-6.4.4\"><span class=\"toc-item-num\">6.4.4&nbsp;&nbsp;</span>Comparing results</a></span></li></ul></li><li><span><a href=\"#Training-:-Extra-trees-Model\" data-toc-modified-id=\"Training-:-Extra-trees-Model-6.5\"><span class=\"toc-item-num\">6.5&nbsp;&nbsp;</span>Training : Extra-trees Model</a></span><ul class=\"toc-item\"><li><span><a href=\"#Set-model-and-parameter-grids\" data-toc-modified-id=\"Set-model-and-parameter-grids-6.5.1\"><span class=\"toc-item-num\">6.5.1&nbsp;&nbsp;</span>Set model and parameter grids</a></span></li><li><span><a href=\"#Grid-Search-Cross-Validation\" data-toc-modified-id=\"Grid-Search-Cross-Validation-6.5.2\"><span class=\"toc-item-num\">6.5.2&nbsp;&nbsp;</span>Grid Search Cross Validation</a></span></li><li><span><a href=\"#Test-sample-data\" data-toc-modified-id=\"Test-sample-data-6.5.3\"><span class=\"toc-item-num\">6.5.3&nbsp;&nbsp;</span>Test sample data</a></span></li></ul></li><li><span><a href=\"#Predict-Cum.-Reservation-Count\" data-toc-modified-id=\"Predict-Cum.-Reservation-Count-6.6\"><span class=\"toc-item-num\">6.6&nbsp;&nbsp;</span>Predict Cum. Reservation Count</a></span></li></ul></li><li><span><a href=\"#Model-3-:-Optimize-Best-Pricing\" data-toc-modified-id=\"Model-3-:-Optimize-Best-Pricing-7\"><span class=\"toc-item-num\">7&nbsp;&nbsp;</span>Model 3 : Optimize Best Pricing</a></span></li><li><span><a href=\"#After-Recommendation\" data-toc-modified-id=\"After-Recommendation-8\"><span class=\"toc-item-num\">8&nbsp;&nbsp;</span>After Recommendation</a></span></li><li><span><a href=\"#Additional-Analysis\" data-toc-modified-id=\"Additional-Analysis-9\"><span class=\"toc-item-num\">9&nbsp;&nbsp;</span>Additional Analysis</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import time\n",
    "import copy\n",
    "import datetime as dt\n",
    "import pickle\n",
    "from itertools import product\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Plotting\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.font_manager as fm\n",
    "import seaborn as sns\n",
    "font_path = r'C:\\Windows\\Fonts\\NanumBarunGothic.ttf'\n",
    "font_name = fm.FontProperties(fname=font_path).get_name()\n",
    "matplotlib.rc('font', family=font_name)\n",
    "plt.style.use('seaborn')\n",
    "\n",
    "# Data Preprocessing\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler    # Scaling\n",
    "from sklearn.model_selection import train_test_split    # Split train and test dataset\n",
    "\n",
    "# Evaluation\n",
    "from sklearn.metrics import accuracy_score, mean_squared_error\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV, KFold\n",
    "\n",
    "# Time Series Models\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from statsmodels.tsa.ar_model import AutoReg\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "from statsmodels.tsa.holtwinters import ExponentialSmoothing\n",
    "\n",
    "# Machine Learning Models\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.linear_model import LogisticRegression \n",
    "from sklearn.linear_model import SGDRegressor\n",
    "from sklearn import svm    # Support Vector Machine\n",
    "from sklearn.ensemble import ExtraTreesRegressor    # Extra-trees Regressor\n",
    "import xgboost as xgb    # XGboost\n",
    "import lightgbm as lgb   # Light gradient boosting machine\n",
    "\n",
    "# Customized Exponential Model\n",
    "from scipy.optimize import curve_fit\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define File Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Folder root path\n",
    "root_path = os.path.join('..', 'input')\n",
    "save_path = os.path.join('..', 'result')\n",
    "\n",
    "# Reservation data path (yearly dataset )\n",
    "res_18_path = os.path.join(root_path, 'reservation_18_discount.csv')\n",
    "res_19_path = os.path.join(root_path, 'reservation_19_discount.csv')\n",
    "res_20_path = os.path.join(root_path, 'reservation_20_discount.csv')\n",
    "\n",
    "# Reservation Discount\n",
    "res_discount_18_path = os.path.join(root_path, 'reservation_18.csv')\n",
    "res_discount_19_path = os.path.join(root_path, 'reservation_19.csv')\n",
    "\n",
    "# Jeju visitor data path\n",
    "jeju_visit_path = os.path.join(root_path, 'jeju_visit_daily.csv')\n",
    "\n",
    "# Discount \n",
    "discount_type_path = os.path.join(root_path, 'discount_type.csv')\n",
    "discount_season_path = os.path.join(root_path, 'discount_season.csv')\n",
    "discount_policy_path = os.path.join(root_path, 'discount_policy.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reservation dataset\n",
    "res_data_type = {'계약번호': int, '예약경로명': str, '고객구분명': str, '총 청구액(VAT포함)': int,\n",
    "                 '예약모델명': str, '차급': str, '대여일': str, '대여시간': str, '반납일': str,\n",
    "                 '반납시간': str, '차량대여요금(VAT포함)': int, 'CDW요금구분명': str, 'CDW요금': int, \n",
    "                 '총대여료(VAT포함)': int, '적용할인율(%) ': float, '예약일자': str}\n",
    "\n",
    "res_18 = pd.read_csv(res_18_path, delimiter='\\t', dtype=res_data_type)\n",
    "res_19 = pd.read_csv(res_19_path, delimiter='\\t', dtype=res_data_type)\n",
    "res_20 = pd.read_csv(res_20_path, delimiter='\\t', dtype=res_data_type)\n",
    "# res_20 = pd.read_csv(res_20_path, delimiter='\\t', dtype=res_data_type)\n",
    "\n",
    "# Reservation Discount dataset\n",
    "res_discount_data_type = {'예약경로': int,'예약경로명': str, '계약번호': int,  '고객': int, '고객구분': float, \n",
    "                          '고객구분명': str, '총 청구액(VAT포함)': int, '총 수납금액(VAT포함)': int,\n",
    "                          '총 잔액(VAT포함)': int, '예약모델': str, '예약모델명': str, '차급': str, \n",
    "                          '대여일': str, '대여시간': str, '반납일': str,'반납시간': str, '대여기간(일)': int,\n",
    "                          '대여기간(시간)': int, '실반납일시': int, '실대여기간(일)': int, '실대여기간(시간)': int,\n",
    "                          '차량대여요금(VAT포함)': int, 'CDW가입여부': str, 'CDW요금구분': float, \n",
    "                          'CDW요금구분명': str, 'CDW요금': str, '회원등급': str, '차종': str, '구매목적': str, \n",
    "                          '내부매출액': int, '수납': str, '예약일자': str, '할인유형': str, '할인유형명': str, \n",
    "                          '적용할인명': str}\n",
    "\n",
    "res_18_discount = pd.read_csv(res_discount_18_path, delimiter='\\t', dtype=res_discount_data_type)\n",
    "res_19_discount = pd.read_csv(res_discount_19_path, delimiter='\\t', dtype=res_discount_data_type)\n",
    "\n",
    "# Jeju dataset\n",
    "jeju_visit = pd.read_csv(jeju_visit_path, delimiter='\\t', \n",
    "                         dtype={'date': int, 'domestic': int, 'foreign': int, 'total': int})\n",
    "\n",
    "# Discount dataset\n",
    "discount_type = pd.read_csv(discount_type_path, delimiter='\\t', dtype={'kind': str, 'discount': int})\n",
    "discount_season = pd.read_csv(discount_season_path, delimiter='\\t',\n",
    "                                 dtype={'YYYYMMDD': int, 'M3': float, 'M2': float, 'M1': float, 'W4': float,\n",
    "                                        'W3': float, 'W2': float, 'W1': float, 'weekend': float, 'season': float, \n",
    "                                        'event': float})\n",
    "discount_policy = pd.read_csv(discount_policy_path, delimiter='\\t', \n",
    "                              dtype={'date': int, '20160701': float, '20160901': float, '20170101': float,\n",
    "                                    '20180101': float, '20180209': float, '20180312': float, '20180314': float,\n",
    "                                    '20180327': float, '20180417': float, '20180508': float, '20180628': float,\n",
    "                                    '20180720': float, '20180726': float, '20180808': float, '20180817': float,\n",
    "                                    '20180903': float, '20180910': float, '20180914': float, '20181010': float,\n",
    "                                    '20181029': float, '20181129': float, '20181218': float, '20190111': float,\n",
    "                                    '20190122': float, '20190225': float, '20190313': float, '20190322': float,\n",
    "                                    '20190405': float, '20190409': float, '20190419': float, '20190510': float,\n",
    "                                    '20190521': float, '20190528': float, '20190612': float, '20190701': float,\n",
    "                                    '20190718': float, '20190723': float, '20190816': float, '20190823': float,\n",
    "                                    '20190909': float, '20191029': float})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### API Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Jeju Airport Arrival Passenger Dataset\n",
    "\n",
    "# API URL & Key\n",
    "url = 'http://openapi.airport.co.kr/service/rest/totalAirportStatsService/getAirportStats'\n",
    "service_key = 'UYRNns1wVRWz8MIyaMqUcL%2BHhIsbY0xjNyzRyvBNZRwh9zefraNj4lh9eBLgOw%2B2c8lBV%2Fh1SbzyNV96aO3DUw%3D%3D'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# API \n",
    "def get_api_data(date_from_to: list, ulr: str, service_key: str):\n",
    "    for date in date_from_to:\n",
    "        url_opt = f'?startDePd={date[0]}&endDePd={date[0]}&routeBe=1&pasngrCargoBe=0&serviceKey={service_key}'\n",
    "        url_fin = ulr + url_opt\n",
    "        response = urlopen(url_fin).read()\n",
    "        xtree = ET.fromstring(response)\n",
    "        rows = get_xml_data(xtree=xtree)\n",
    "\n",
    "    return rows\n",
    "\n",
    "def get_xml_data(xtree):\n",
    "    rows = []\n",
    "    for node in xtree[1][0]:\n",
    "        airport = node.find(\"airport\").text\n",
    "        arrflgt = node.find(\"arrflgt\").text\n",
    "        arrpassenger = node.find(\"arrpassenger\").text\n",
    "        depflgt = node.find(\"depflgt\").text\n",
    "        deppassenger = node.find(\"deppassenger\").text\n",
    "        subflgt = node.find(\"subflgt\").text\n",
    "        subpassenger = node.find(\"subpassenger\").text\n",
    "\n",
    "        rows.append({\"airport\": airport, \"arrflgt\": arrflgt, \"arrpassenger\": arrpassenger, \n",
    "                     \"arrpassenger\": arrpassenger, \"depflgt\": depflgt, \"deppassenger\": deppassenger,\n",
    "                     \"subflgt\": subflgt, \"subpassenger\": subpassenger})\n",
    "\n",
    "    return rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Merge Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Reservation \n",
    "res = pd.concat([res_18, res_19], axis=0, ignore_index=True)\n",
    "\n",
    "# Reservation discount\n",
    "res_discount = pd.concat([res_18_discount, res_19_discount], axis=0, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remap Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Reservation\n",
    "res_remap_cols = {'계약번호': 'res_num', '예약경로명': 'res_route_nm', '고객구분명': 'cust_kind_nm',\n",
    "                 '총 청구액(VAT포함)': 'tot_fee', '예약모델명': 'res_model_nm', '차급': 'car_grd', \n",
    "                 '대여일': 'rent_day', '대여시간': 'rent_time', '반납일': 'return_day', '반납시간': 'return_time', \n",
    "                 '차량대여요금(VAT포함)': 'car_rent_fee', 'CDW요금구분명': 'cdw_fee_kind_nm', 'CDW요금': 'cdw_fee',\n",
    "                 '회원등급': 'member_grd','차종': 'car_kind', '예약일자': 'res_day', '할인유형': 'discount_type', \n",
    "                 '총대여료(VAT포함)': 'fin_fee', '적용할인율(%)': 'fin_discount'\n",
    "            }\n",
    "res = res.rename(columns=res_remap_cols)\n",
    "res_20 = res_20.rename(columns=res_remap_cols)\n",
    "\n",
    "# Reservation Discount\n",
    "res_discount_remap_cols = {'계약번호': 'res_num', '할인유형': 'discount_type', \n",
    "                           '할인유형명': 'discount_type_nm', '적용할인명': 'applyed_discount'}\n",
    "\n",
    "res_discount = res_discount.rename(columns=res_discount_remap_cols)\n",
    "res_20 = res_20.rename(columns=res_discount_remap_cols)\n",
    "\n",
    "# Jeju visit\n",
    "jeju_remap_cols = {'domestic': 'visit_dom', 'foreign': 'visit_for', 'total': 'visit_tot'}\n",
    "jeju_visit = jeju_visit.rename(columns=jeju_remap_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 특이사항 처리\n",
    "res_20 = res_20[pd.to_datetime(res_20['res_day'], format='%Y-%m-%d') >= dt.datetime(2020,1,1)]\n",
    "\n",
    "res_20_discount = res_20[['res_num', 'discount_type_nm', 'applyed_discount']]\n",
    "res_20 = res_20.drop(columns=['member_grd', 'discount_type_nm', 'applyed_discount'], axis=1)\n",
    "res = res.drop(columns=['cust_kind_nm'], axis=1, errors='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = pd.concat([res, res_20], axis=0, ignore_index=True)\n",
    "res_discount = pd.concat([res_discount, res_20_discount], axis=0, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check NA Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39949 870 0 0 3774 62155\n"
     ]
    }
   ],
   "source": [
    "# Reservation\n",
    "na_cnt_res = res.isna().sum().sum()\n",
    "na_cnt_res_20 = res_20.isna().sum().sum()\n",
    "\n",
    "# Jeju\n",
    "na_cnt_jeju = jeju_visit.isna().sum().sum()\n",
    "\n",
    "# discount type\n",
    "na_cnt_discount = discount_type.isna().sum().sum()\n",
    "\n",
    "# Discount Lead Time\n",
    "na_cnt_discount_season = discount_season.isna().sum().sum()\n",
    "\n",
    "# Discount Policy\n",
    "na_cnt_discount_policy = discount_policy.isna().sum().sum()\n",
    "\n",
    "print(na_cnt_res, na_cnt_res_20, na_cnt_jeju, na_cnt_discount, na_cnt_discount_season, na_cnt_discount_policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fill Na values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Jeju visit\n",
    "jeju_visit = jeju_visit.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Discount Poilicy\n",
    "discount_policy['date'] = pd.to_datetime(discount_policy['date'], format='%Y%m%d')\n",
    "discount_policy = discount_policy.set_index('date')\n",
    "discount_policy = discount_policy.fillna(method='ffill', axis=1)\n",
    "discount_policy = discount_policy.fillna(method='bfill', axis=1)\n",
    "discount_policy = discount_policy.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Discount Season\n",
    "discount_season = discount_season.fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Change Data Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Reservation dataset\n",
    "res['res_day'] = pd.to_datetime(res['res_day'], format='%Y-%m-%d')\n",
    "res['rent_day'] = pd.to_datetime(res['rent_day'], format='%Y-%m-%d')\n",
    "res['return_day'] = pd.to_datetime(res['return_day'], format='%Y-%m-%d')\n",
    "\n",
    "# Jeju Visit dataset\n",
    "jeju_visit['visit_dom'] = jeju_visit['visit_dom'].astype(int)\n",
    "jeju_visit['visit_for'] = jeju_visit['visit_for'].astype(int)\n",
    "jeju_visit['visit_tot'] = jeju_visit['visit_tot'].astype(int)\n",
    "\n",
    "jeju_visit['rent_day'] = pd.to_datetime(jeju_visit['date'], format='%Y%m%d')\n",
    "# jeju_visit = jeju_visit.set_index('rent_day', drop=False)\n",
    "\n",
    "# Discount Season dataset\n",
    "discount_season['rent_day'] = pd.to_datetime(discount_season['YYYYMMDD'], format='%Y%m%d')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Filter Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reservation dataset\n",
    "# Filter 1.6 Grade cars\n",
    "res = res[res['res_model_nm'].isin(['아반떼 AD (G)', 'K3', 'THE NEW K3 (G)', 'ALL NEW K3 (G)',\n",
    "                                    '더 올 뉴 벨로스터 (G)', '아반떼 AD (G) F/L', '올 뉴 아반떼 (G)'])]\n",
    "\n",
    "# Discount dataset\n",
    "# Except 2017 year\n",
    "discount_season = discount_season[discount_season['YYYYMMDD'] >= 20180101]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Grouping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Car Model group\n",
    "\n",
    "conditions = [\n",
    "    res['res_model_nm'].isin(['K3', 'THE NEW K3 (G)', 'ALL NEW K3 (G)']),\n",
    "    res['res_model_nm'].isin(['아반떼 AD (G)', '아반떼 AD (G) F/L', '올 뉴 아반떼 (G)']),\n",
    "    res['res_model_nm'] == '더 올 뉴 벨로스터 (G)',\n",
    "    res['res_model_nm'] == '아이오닉 (H)'\n",
    "]\n",
    "\n",
    "values = ['K3', 'AVANTE', 'VELOSTER', 'IONIC']\n",
    "\n",
    "res['res_model_grp'] = np.select(conditions, values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Drop Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reservation dataset\n",
    "res_drop_cols = ['res_route_nm', 'car_grd', 'res_model_nm', 'cdw_fee_kind_nm', 'tot_fee']\n",
    "res = res.drop(columns=res_drop_cols, axis=1, errors='ignore')\n",
    "\n",
    "# Reservation Discount dataset\n",
    "res_discount = res_discount[['res_num', 'discount_type_nm', 'applyed_discount']]\n",
    "\n",
    "# jeju visit\n",
    "jeju_visit = jeju_visit.drop(columns=['yyyymmdd'], axis=1, errors='ignore')\n",
    "\n",
    "# Discount\n",
    "discount_season = discount_season[['rent_day', 'weekend', 'season', 'event']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ordering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = res.sort_values(by=['rent_day', 'res_day'])\n",
    "jeju_visit = jeju_visit.sort_values(by=['rent_day'])\n",
    "discount_season = discount_season.sort_values(by=['rent_day'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing for each dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reservation Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Caculate lead time of reservation \n",
    "res['lead_time'] = res['rent_day'] - res['res_day']\n",
    "res['lead_time'] = res['lead_time'].apply(lambda x: x.days)\n",
    "\n",
    "# Conver rent/return day and time to datetime (YYYYMMDD HHMISS)\n",
    "rent_datetime = res['rent_day'].apply(lambda x: dt.datetime.strftime(x, '%Y-%m-%d')) + ' ' + res['rent_time']\n",
    "return_datetime = res['return_day'].apply(lambda x: dt.datetime.strftime(x, '%Y-%m-%d')) + ' ' + res['return_time']\n",
    "res['rent_datetime'] = pd.to_datetime(rent_datetime, format='%Y-%m-%d %H:%M:%S')\n",
    "res['return_datetime'] = pd.to_datetime(return_datetime, format='%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "res['rent_period'] = res['return_datetime'] - res['rent_datetime']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make reservation counts on each day\n",
    "res_cnt_on_rent_day = res[['rent_day', 'res_num']].groupby(by=['rent_day']).count()\n",
    "res_cnt_on_rent_day = res_cnt_on_rent_day.rename(columns={'res_num': 'res_cnt'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Jeju Visit Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "jeju_visit['visit_tot'] = jeju_visit['visit_dom'] + jeju_visit['visit_for']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Discount Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rechange data type of reservation\n",
    "res['res_day'] = pd.to_datetime(res['res_day'], format='%Y-%m-%d')\n",
    "res['rent_day'] = pd.to_datetime(res['rent_day'], format='%Y-%m-%d')\n",
    "res['return_day'] = pd.to_datetime(res['return_day'], format='%Y-%m-%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Discount Policy\n",
    "discount_idx_to_lead_time = {idx: day for idx, day in enumerate(discount_policy.columns)}\n",
    "discount_lead_time_to_idx = {day: idx for idx, day in enumerate(discount_policy.columns)}\n",
    "\n",
    "res_day_to_idx = {}\n",
    "res_col = discount_policy.columns[1:]\n",
    "for i in range(len(res_col)-1):\n",
    "    date_range_temp = pd.date_range(start=res_col[i], end=res_col[i+1])\n",
    "    date_range_temp = date_range_temp[:-1]\n",
    "    for date in date_range_temp:\n",
    "        res_day_to_idx[date] = i\n",
    "        \n",
    "date_range_last = pd.date_range(start=res_col[-1], end=discount_policy['date'].iloc[-1].strftime('%Y%m%d'))\n",
    "for date in date_range_last:\n",
    "    res_day_to_idx[date] = len(res_col) - 1\n",
    "    \n",
    "rent_day_to_idx = {date: idx for idx, date in enumerate(discount_policy['date'])}\n",
    "discount_idx_col = [(rent_day_to_idx[idx], res_day_to_idx[col]) for idx, col in zip(res['rent_day'], res['res_day'])]\n",
    "res['res_discount'] = [discount_policy.iloc[idx, col] for idx, col in discount_idx_col]\n",
    "res['diff_discount'] = res['fin_discount'] - res['res_discount']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Discount type\n",
    "discount_type['discount'] = discount_type['discount'] - 60    # 비회원 기준 할인율\n",
    "discount_type_dict = {kind: discount for kind, discount in zip(discount_type['kind'], discount_type['discount'])}\n",
    "\n",
    "# Correct naming on 2018/2019 years \n",
    "discount_type_dict[\"인터넷일반회원\"] = discount_type_dict[\"일반회원\"]\n",
    "discount_type_dict[\"인터넷골드회원\"] = discount_type_dict[\"골드회원\"]\n",
    "discount_type_dict[\"인터넷더블골드회원\"] = discount_type_dict[\"더블골드회원\"]\n",
    "discount_type_dict[\"예약어플(골드회원)\"] = discount_type_dict[\"골드회원\"]\n",
    "discount_type_dict[\"예약어플(더블골드회원)\"] = discount_type_dict[\"더블골드회원\"]\n",
    "discount_type_dict['비씨카드(주)'] = discount_type_dict['BC카드']\n",
    "discount_type_dict[\"신한Top's club\"] = discount_type_dict[\"신한 Top's club\"]\n",
    "discount_type_dict[\"현대카드오토케어\"] = discount_type_dict[\"현대카드 오토케어\"]\n",
    "discount_type_dict[\"롯데그룹 임직원\"] = discount_type_dict[\"롯데그룹임직원\"]\n",
    "discount_type_dict[\"금호그룹 임직원\"] = discount_type_dict[\"금호그룹임직원\"]\n",
    "discount_type_dict[\"KT그룹 임직원\"] = discount_type_dict[\"KT그룹임직원\"]\n",
    "discount_type_dict[\"VIP거래처(제주)\"] = discount_type_dict[\"VIP거래처\"]\n",
    "discount_type_dict[\"에어부산(FLY.FUN)\"] = discount_type_dict[\"에어부산(FLY & FUN)\"]\n",
    "discount_type_dict[\"삼성전자 서비스\"] = discount_type_dict[\"삼성전자서비스\"]\n",
    "discount_type_dict[\"하나카드 VIP 컨시어지\"] = discount_type_dict[\"하나카드VIP컨시어지\"]\n",
    "discount_type_dict[\"BC-kt 제휴카드\"] = discount_type_dict[\"BC-kt제휴카드\"]\n",
    "discount_type_dict[\"인천공항홈페이지\"] = discount_type_dict[\"인천공항 홈페이지\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Merge Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reservation + Reservation Counts on each day\n",
    "res = pd.merge(res, res_cnt_on_rent_day, how='left', on='rent_day', left_index=True, right_index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reservation + Jeju visit on rental day\n",
    "res = pd.merge(res, jeju_visit, how='left', on='rent_day', left_index=True, right_index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reservaion + Discount season\n",
    "res = pd.merge(res, discount_season, how='left', on='rent_day', left_index=True, right_index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reservation + Reservation discount\n",
    "res = pd.merge(res, res_discount, how='left', on='res_num', left_index=True, right_index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop unnecessary Columns\n",
    "res = res.drop(columns=['rent_time', 'return_time', 'rent_datetime', 'return_datetime'],\n",
    "               axis=1, errors='ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add seasonality column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "conditions = [\n",
    "    (res['season'] != 0),\n",
    "    (res['event'] == 1),\n",
    "    ((res['weekend'] == 1) & (res['weekend'] == 1) & (res['weekend'] == 1)),\n",
    "    (res['season'] + res['event'] + res['weekend'] == 0)]\n",
    "\n",
    "values =[4, 3, 2, 1]\n",
    "\n",
    "res['seasonality'] = np.select(conditions, values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add Lead Time Interval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "conditions = [\n",
    "    ((res['lead_time'] >= 7 * 4) & (res['lead_time'] < 7 * 5)),\n",
    "    ((res['lead_time'] >= 7 * 5) & (res['lead_time'] < 7 * 6)),\n",
    "    ((res['lead_time'] >= 7 * 6) & (res['lead_time'] < 7 * 7)),\n",
    "    ((res['lead_time'] >= 7 * 7) & (res['lead_time'] < 7 * 8)),\n",
    "    ((res['lead_time'] >= 7 * 8) & (res['lead_time'] < 7 * 9)),\n",
    "    ((res['lead_time'] >= 7 * 9) & (res['lead_time'] < 7 * 10)),\n",
    "    ((res['lead_time'] >= 7 * 10) & (res['lead_time'] < 7 * 11)),\n",
    "    ((res['lead_time'] >= 7 * 11) & (res['lead_time'] < 7 * 12)),\n",
    "    (res['lead_time'] >= 7 * 12)]\n",
    "\n",
    "values = np.arange(28, 28+9, 1)\n",
    "\n",
    "res['lead_time_vector'] = np.select(conditions, values)\n",
    "res['lead_time_vector'] = np.where(res['lead_time_vector'] <= 28, \n",
    "                                   res['lead_time'].values,\n",
    "                                   res['lead_time_vector'].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert Rent Periods to hours "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "res['rent_period'] = res['rent_period'].apply(lambda x: x.days* 24 + x.seconds//3600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Update Discount Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "res['discount_add'] = res['applyed_discount'].apply(lambda x: discount_type_dict.get(x, 0))\n",
    "res['res_discount'] += res['discount_add']\n",
    "res = res.drop(columns=['discount_add'], errors='ignore')\n",
    "res['res_discount'] = np.where(res['res_discount'] > 90, 90, res['res_discount'])    # 할인율 상한선: 90%\n",
    "# res['diff_discount'] = res['fin_discount'] - res['rev_discount']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate reservation fee"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "res['res_fee'] = res['fin_fee'] - res['cdw_fee']\n",
    "res['res_fee'] = res['res_fee'] / (1 - res['fin_discount']/100)\n",
    "res['res_fee'] = res['res_fee'] * (1 - res['res_discount']/100) + res['cdw_fee']\n",
    "res['res_fee'] = res['res_fee'].apply(lambda x: round(x, 0))\n",
    "res = res.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "res.loc[res['res_fee'] == np.inf, 'res_fee'] = ((res['fin_fee'] - res['cdw_fee']) * res['res_discount']) + res['cdw_fee']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove Exception"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 예약 가격이 없는 경우 제외\n",
    "res = res[res['res_fee'].notnull()]\n",
    "\n",
    "# 쿠폰할인 제외\n",
    "res = res[res['discount_type_nm'] != '쿠폰할인']\n",
    "\n",
    "# 임의할인: 할인율 100% 제외\n",
    "res = res[res['fin_fee'] != 0] \n",
    "\n",
    "# 업체할인: 비씨카드 이외 할인 제외\n",
    "res = res[(res['discount_type_nm'] != '업체할인') | (res['applyed_discount'] == '비씨카드(주)')] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Input  - Reservation Counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Drop Unnecessary columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_cols_1 = ['res_num', 'rent_day', 'return_day', 'res_day', 'diff_discount']\n",
    "drop_cols_2 = ['visit_dom', 'visit_for']    # Jeju visitor (Using total number of visitors)\n",
    "drop_cols_3 = ['weekend', 'season', 'event']    # use customized seasonal factor\n",
    "drop_cols_4 = ['discount_type_nm', 'applyed_discount']    # only use discount rate\n",
    "drop_cols_5 = ['car_rent_fee', 'cdw_fee', 'fin_fee', 'res_fee']    # not use fee\n",
    "drop_cols_6 = ['fin_discount']    # use reservation discount rate ('res_discount')\n",
    "drop_cols_7 = ['lead_time']    # use customized lead time vector ('lead_time_vector')  \n",
    "drop_cols_8 = ['rent_period']  \n",
    "res_1 = res.drop(columns=drop_cols_1 + drop_cols_2 + drop_cols_3 + drop_cols_4 + \n",
    "                       drop_cols_5 + drop_cols_6 + drop_cols_7 + drop_cols_8, axis=1, errors='ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Data Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = res_1.drop(columns=['res_cnt'], axis=1)\n",
    "y = res_1['res_cnt']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Save  Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.to_csv(os.path.join(save_path, 'data', 'res_input_1.csv'), index=False)\n",
    "y.to_csv(os.path.join(save_path, 'data', 'res_target_1.csv'), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model 2 Input - Cum. Reservation Counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Slicing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_2 = res.loc[:, ['res_day', 'rent_day', 'lead_time', 'lead_time_vector', \n",
    "                    'seasonality', 'res_model_grp', 'res_discount']]\n",
    "res_2['lead_time'] = res_2['lead_time'] * -1\n",
    "res_2['lead_time_vector'] = res_2['lead_time_vector'] * -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Grouping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_cnt_by_rent_day = res_2.groupby(by=['rent_day', 'res_day']).count()['lead_time']\n",
    "res_cum_cnt_by_rent_day = res_cnt_by_rent_day.groupby(by=['rent_day']).cumsum()\n",
    "res_dscnt_by_rent_day = res_2.groupby(by=['rent_day', 'res_day']).sum()['res_discount']\n",
    "res_cum_dscnt_by_rent_day = res_dscnt_by_rent_day.groupby(by=['rent_day']).cumsum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_cum = pd.DataFrame({'cum_dscnt': res_cum_dscnt_by_rent_day,\n",
    "                        'cum_res_cnt': res_cum_cnt_by_rent_day}, \n",
    "                        index=res_cum_dscnt_by_rent_day.index)\n",
    "res_cum['cum_dscnt_mean'] = res_cum['cum_dscnt'] / res_cum['cum_res_cnt']\n",
    "res_cum['cum_dscnt_mean'] = res_cum['cum_dscnt_mean'].apply(lambda x: round(x, 2))\n",
    "res_cum = res_cum.reset_index(level=(0,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_model_2 = pd.merge(res_2, res_cum, how='left', on=['rent_day', 'res_day'], left_index=True, right_index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_model_2 = res_model_2.drop(columns=['res_day', 'rent_day', 'lead_time', 'res_model_grp', \n",
    "                                        'res_discount', 'cum_dscnt'], \n",
    "                               errors='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_model_2 = res_model_2.sort_values(by=['seasonality', 'lead_time_vector']).drop_duplicates()\n",
    "res_model_2 = res_model_2[['seasonality', 'lead_time_vector', 'cum_dscnt_mean', 'cum_res_cnt']]\n",
    "res_model_2 = res_model_2.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Re-Group on 'seasonality', 'lead time'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_model_2_grp = res_model_2.groupby(by=['seasonality', 'lead_time_vector']).mean()\n",
    "res_model_2_grp = res_model_2_grp.reset_index(level=(0,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Save Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_2_input = res_model_2.drop(columns=['cum_res_cnt'])\n",
    "model_2_target = res_model_2['cum_res_cnt']\n",
    "model_2_grp_input = res_model_2_grp.drop(columns=['cum_res_cnt'])\n",
    "model_2_grp_target = res_model_2_grp['cum_res_cnt']\n",
    "\n",
    "model_2_input.to_csv(os.path.join(save_path, 'data', 'model_2_input.csv'), index=False)\n",
    "model_2_target.to_csv(os.path.join(save_path, 'data', 'model_2_target.csv'), index=False)\n",
    "model_2_grp_input.to_csv(os.path.join(save_path, 'data', 'model_2_grp_input.csv'), index=False)\n",
    "model_2_grp_target.to_csv(os.path.join(save_path, 'data', 'model_2_grp_target.csv'), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 1: Predict Jeju Visitors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'date'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32mc:\\python37\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   2894\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2895\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2896\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'date'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-47-22da111bc384>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mremap_col_jeju\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m'yyyymmdd'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'date'\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mjeju_visit\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mjeju_visit\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrename\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mremap_col_jeju\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mjeju_visit\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'date'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_datetime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjeju_visit\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'date'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'%Y%m%d'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mjeju_visit\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mjeju_visit\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'date'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python37\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   2900\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2901\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2902\u001b[1;33m             \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2903\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2904\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python37\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   2895\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2896\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2897\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2898\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2899\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mtolerance\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'date'"
     ]
    }
   ],
   "source": [
    "remap_col_jeju = {'yyyymmdd': 'date'}\n",
    "jeju_visit = jeju_visit.rename(columns=remap_col_jeju)\n",
    "jeju_visit['date'] = pd.to_datetime(jeju_visit['date'], format='%Y%m%d')\n",
    "jeju_visit = jeju_visit.set_index('date')\n",
    "\n",
    "jeju_dom = jeju_visit[['domestic']]\n",
    "jeju_for = jeju_visit[['foreign']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define Models & Validation Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ar_model(data, config: dict, pred_steps=0):\n",
    "    \"\"\"\n",
    "    :param data: times series data\n",
    "    :param lags: the number of lags\n",
    "    :param trend: the trend to include in the model\n",
    "            n: No Trend\n",
    "            c: Constant Only\n",
    "            t: Time Trend Only\n",
    "            ct: Constant and time trend\n",
    "    :param seasonal: Flag indicating whether to include seasonal dummies in the model    \n",
    "    :param pred_step: prediction steps\n",
    "    \"\"\"\n",
    "    model = AutoReg(endog=data, lags=config['lags'], trend=config['trend'])\n",
    "    model_fit = model.fit()\n",
    "    yhat = model_fit.predict(start=len(data), end=len(data) + pred_steps-1)\n",
    "    \n",
    "    return yhat\n",
    "    \n",
    "def arima_model(data, config: dict, pred_steps=0):\n",
    "    \"\"\"\n",
    "    :param data: time series data\n",
    "    :param order: (p, d, q)\n",
    "            p: Trend autoregression order\n",
    "            d: Trend difference order\n",
    "            q: Trend moving average order\n",
    "    :param trend: the trend to include in the model\n",
    "            n: No Trend\n",
    "            c: Constant\n",
    "            t: Trend\n",
    "            ct: Constant and Trend\n",
    "    :param freq: frequency of the time series (‘B’, ‘D’, ‘W’, ‘M’, ‘A’, ‘Q)\n",
    "    \"\"\"\n",
    "    model = ARIMA(data, order=config['order'], trend=config['trend'])\n",
    "    model_fit = model.fit()\n",
    "    yhat = model_fit.predict(start=len(data), end=len(data) + pred_steps-1)\n",
    "    \n",
    "    return yhat\n",
    "    \n",
    "def holt_winters_model(data, config: dict, pred_steps=0):\n",
    "    \"\"\"\n",
    "    :param data: time series data\n",
    "    :param trend - type of trend component\n",
    "        ('add', 'mul', 'additive', 'multiplicative')\n",
    "    :param damped_trend - should the trend component be damped\n",
    "    :param seasonal - Type of seasonal component\n",
    "            ('add', 'mul', 'additive', 'multiplicative', None)\n",
    "    :param seasonal_periods - The number of periods in a complete seasonal cycle\n",
    "    :param pred_step:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    model = ExponentialSmoothing(data, trend=config['trend'], damped_trend=config['damped_trend'])\n",
    "    model_fit = model.fit()\n",
    "    yhat = model_fit.predict(start=len(data), end=len(data) + pred_steps-1)\n",
    "    \n",
    "    return yhat\n",
    "\n",
    "time_series_model = {'ar': ar_model,\n",
    "                     'arima': arima_model,\n",
    "                     'hw': holt_winters_model}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def walk_forward_validation(model: str, data, n_test, config):\n",
    "    # split dataset\n",
    "    train, test = data[:-n_test], data[-n_test:]\n",
    "    \n",
    "    # calculate \n",
    "    yhat = time_series_model[model](data=train, config=config, pred_steps=n_test)\n",
    "    \n",
    "    # add actual observation to history for the next loop\n",
    "    error = math.sqrt(mean_squared_error(test.values, yhat))\n",
    "    \n",
    "    return error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_series_param_tune(model: str, data, n_test, param_grid: dict):\n",
    "    err_list = list()\n",
    "    for params in list(product(*list(param_grid.values()))):\n",
    "        config = dict()\n",
    "        config = {key: val for key, val in zip(list(param_grid.keys()), params)}\n",
    "        err = walk_forward_validation(model, data, n_test=n_test, config=config)\n",
    "        err_list.append((list(config.items()), round(err, 2)))\n",
    "        \n",
    "    err_list_sorted = sorted(err_list, key=lambda err: err[1])    \n",
    "        \n",
    "    return err_list_sorted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation Using Walk Forward Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paramter Grid\n",
    "ar_param_grid = {\n",
    "    'lags': list(np.arange(1,15,1)),\n",
    "    'trend': ['c', 't', 'ct'] \n",
    "}\n",
    "\n",
    "arima_param_grid = {}\n",
    "\n",
    "hw_param_grid = {\n",
    "    'trend': ['add', 'additive'],\n",
    "    'damped_trend': [True, False] \n",
    "}\n",
    "\n",
    "# Test Range\n",
    "n_test = 28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "err_dom_ar = time_series_param_tune(model='ar', data=jeju_dom, \n",
    "                                      n_test=n_test, param_grid=ar_param_grid)\n",
    "print(f'AR: Best paramters: {err_dom_ar[0][0]}, result: {err_dom_ar[0][1]}')\n",
    "\n",
    "err_dom_hw = time_series_param_tune(model='hw', data=jeju_dom, \n",
    "                                      n_test=n_test, param_grid=hw_param_grid)\n",
    "print(f'HW: Best paramters: {err_dom_hw[0][0]}, result: {err_dom_hw[0][1]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "err_for_ar = time_series_param_tune(model='ar', data=jeju_for, \n",
    "                                      n_test=n_test, param_grid=ar_param_grid)\n",
    "print(f'AR: Best paramters: {err_for_ar[0][0]}, result: {err_for_ar[0][1]}')\n",
    "\n",
    "err_for_hw = time_series_param_tune(model='hw', data=jeju_for, \n",
    "                                      n_test=n_test, param_grid=hw_param_grid)\n",
    "print(f'HW: est paramters: {err_for_hw[0][0]}, result: {err_for_hw[0][1]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 2 : Predict Cum. Reservation Counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_path = os.path.join('..', 'result')\n",
    "random_state = 2020\n",
    "\n",
    "X = pd.read_csv(os.path.join(root_path, 'data', 'model_2_grp_input.csv')) \n",
    "y = pd.read_csv(os.path.join(root_path, 'data', 'model_2_grp_target.csv')) \n",
    "# X = pd.read_csv(os.path.join(root_path, 'data', 'model_2_input.csv')) \n",
    "# y = pd.read_csv(os.path.join(root_path, 'data', 'model_2_target.csv')) \n",
    "# X = pd.read_csv(os.path.join(root_path, 'data', 'res_input.csv')) \n",
    "# y = pd.read_csv(os.path.join(root_path, 'data', 'res_target.csv')) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split train and test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, \n",
    "                                                    random_state=random_state, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pre-training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### SVM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define hyperparamter of Support Vector Machine \n",
    "param_svm = {\n",
    "    'kernel': 'rbf',    # 'linear', 'poly', 'rbf'\n",
    "    'tol':1e-3, \n",
    "    'max_iter': -1\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regr_svm = svm.SVR(kernel=param_svm['kernel'],\n",
    "                   tol=param_svm['tol'], \n",
    "                   max_iter=param_svm['max_iter'])\n",
    "\n",
    "regr_svm.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Save model\n",
    "pickle.dump(regr_svm, open(os.path.join(root_path, 'model', 'svm.model'), 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_svm = regr_svm.predict(X_test_scaled)\n",
    "rmse_svm = mean_squared_error(y_test, pred_svm)\n",
    "pred_svm_df = pd.DataFrame({'real': y_test['cum_res_cnt'].values, 'prediction': pred_svm})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Extra-trees Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define hyperparamter of Extra-tree model\n",
    "param_extra = {\n",
    "    'n_estimators': 100,\n",
    "    'criterion': 'mse',\n",
    "    'max_depth': None,\n",
    "    'min_samples_split': 2,\n",
    "    'max_features': 'auto',\n",
    "    'random_state': 2020\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regr_extr = ExtraTreesRegressor(n_estimators=param_extra['n_estimators'], \n",
    "                                criterion=param_extra['criterion'],\n",
    "                                max_depth=param_extra['max_depth'],\n",
    "                                min_samples_split=param_extra['min_samples_split'],\n",
    "                                max_features=param_extra['max_features'],\n",
    "                                random_state=param_extra['random_state'])\n",
    "\n",
    "regr_extr.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Save model\n",
    "pickle.dump(regr_extr, open(os.path.join(save_path, 'model', 'extra_tree.model'), 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_extr = regr_extr.predict(X_test_scaled)\n",
    "rmse_extr = mean_squared_error(y_test, pred_extr)\n",
    "pred_extr_df = pd.DataFrame({'real':  y_test['cum_res_cnt'].values, 'prediction': pred_extr})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### XGboost Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "dtest = xgb.DMatrix(X_test, label=y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param = {'max_depth': 6, 'eta': 1, 'objective': 'reg:squarederror'}\n",
    "param['nthread'] = 4\n",
    "param['eval_metric'] = 'rmse'    # 'rmse', 'auc', 'logloss', 'map'\n",
    "num_round = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regr_xgb = xgb.train(param, dtrain, num_round)\n",
    "regr_xgb.save_model(os.path.join(save_path, 'model', 'xgboost.model'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_xgb = regr_xgb.predict(dtest)\n",
    "rmse_xgb = mean_squared_error(y_test, pred_xgb)\n",
    "pred_xgv_df = pd.DataFrame({'real':  y_test['cum_res_cnt'].values, 'prediction': pred_xgb}, index=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Comparing results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse_svm, rmse_extr, rmse_xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_svm_df = pred_svm_df.sort_values(by=['real'])\n",
    "pred_svm_df = pred_svm_df.reset_index(drop=True)\n",
    "\n",
    "pred_extr_df = pred_extr_df.sort_values(by=['real'])\n",
    "pred_extr_df = pred_extr_df.reset_index(drop=True)\n",
    "\n",
    "pred_xgv_df = pred_xgv_df.sort_values(by=['real'])\n",
    "pred_xgv_df = pred_xgv_df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 1)\n",
    "pred_svm_df['real'].plot.line(figsize=(15,6), linewidth=1.1, \n",
    "                              color='firebrick', label='Real', ax=axes)\n",
    "pred_svm_df['prediction'].plot.line(figsize=(15,6),linewidth=0.9, alpha=0.4,\n",
    "                                     color='coral', label='Prediction', ax=axes)\n",
    "# axes.text(7000, 180, f'RMSE of SVM: {round(rmse_svm, 2)}', fontsize=14, color='dimgray')\n",
    "axes.legend()\n",
    "plt.title('Result: SVM')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 1)\n",
    "pred_extr_df['real'].plot.line(figsize=(15,6), linewidth=1.1, \n",
    "                              color='firebrick',  label='Real', ax=axes)\n",
    "pred_extr_df['prediction'].plot.line(figsize=(15,6),linewidth=0.9, alpha=0.4,\n",
    "                                     color='coral', label='Prediction', ax=axes)\n",
    "# axes.text(7000, 180, f'RMSE of SVM: {round(rmse_extr, 2)}', fontsize=14, color='dimgray')\n",
    "axes.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 1)\n",
    "pred_xgv_df['real'].plot.line(figsize=(15,6), linewidth=1.1, \n",
    "                              color='firebrick', label='Real', ax=axes)\n",
    "pred_xgv_df['prediction'].plot.line(figsize=(15,6),linewidth=0.9, alpha=0.4,\n",
    "                                     color='coral', label='Prediction', ax=axes)\n",
    "# axes.text(7000, 180, f'RMSE of SVM: {round(rmse_xgb, 2)}', fontsize=14, color='dimgray')\n",
    "axes.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training : Extra-trees Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Set model and parameter grids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regressors = {}\n",
    "\n",
    "regressors.update({\"Extra Trees Regressor\": ExtraTreesRegressor()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grids = {}\n",
    "param_grids.update({\"Extra Trees Regressor\": {\n",
    "    'n_estimators': list(np.arange(100, 500, 100)),\n",
    "    'criterion': ['mse'],\n",
    "    'min_samples_split: list(np.arange(2, 6, 1)),   # minimum number of samples required to split inner node \n",
    "    'min_samples_leaf': list(np.arange(1, 6, 1)),    # have the effect of smoothing the model\n",
    "    'max_features': ['auto', 'sqrt', 'log2']\n",
    "}})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Grid Search Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_best_hyper_param(x, y, regr, scoring='neg_root_mean_squared_error'):\n",
    "    # Select regressor algorithm\n",
    "    regressor = regressors[regr]\n",
    "    \n",
    "    # Define parameter grid\n",
    "    param_grid = param_grids[regr]\n",
    "    \n",
    "    # Initialize Grid Search object\n",
    "    gscv = GridSearchCV(estimator=regressor, param_grid=param_grid,\n",
    "                        scoring=scoring, n_jobs=1, cv=5, verbose=1)\n",
    "    \n",
    "    # Fit gscv\n",
    "    print(f'Tuning {regr}')\n",
    "    gscv.fit(x, y)\n",
    "    \n",
    "    # Get best paramters and score\n",
    "    best_params = gscv.best_params_\n",
    "    best_score = gscv.best_score_\n",
    "    \n",
    "    # Update regressor paramters\n",
    "    regressor.set_params(**best_params)\n",
    "    \n",
    "    return regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "extr_best = get_best_hyper_param(x=X_train_scaled, y=y_train['cum_res_cnt'].to_numpy(), \n",
    "                                 regr='Extra Trees Regressor')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extr_best_params = extr_best.get_params()\n",
    "extr_best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# regr_extr_best = extr_best\n",
    "\n",
    "regr_extr_best.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Save best model\n",
    "pickle.dump(regr_extr_best, open(os.path.join(save_path, 'model', 'extra_tree_best.model'), 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_extr_best = regr_extr_best.predict(X_test_scaled)\n",
    "rmse_extr_best = mean_squared_error(y_test, pred_extr_best)\n",
    "pred_extr_best_df = pd.DataFrame({'real':  y_test['cum_res_cnt'].values, 'prediction': pred_extr_best})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ExtraTreesRegressor(min_samples_split=4)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extr_best_params = {'n_estimators': 100,\n",
    "                    'min_samples_split': 4,\n",
    "                    'min_samples_leaf': 1, \n",
    "                    'max_features':'auto'}\n",
    "regr_extr_best = ExtraTreesRegressor(**extr_best_params)\n",
    "regr_extr_best.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predict Cum. Reservation Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting\n",
    "day_to_season = {day: season for day, season in zip(res['rent_day'], res['seasonality'])}\n",
    "season_init_discount = {1: 70,   # Weekdays\n",
    "                        2: 60,   # Weekends\n",
    "                        3: 45,   # Holidays\n",
    "                        4: 0}    # Peak Season\n",
    "predict_day = '2019.01.01'\n",
    "lead_time_vector = np.arange(-36, 1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get season value and initial discount rate\n",
    "predict_datetime = dt.datetime(*list(map(int, predict_day.split('.'))))\n",
    "season = day_to_season[predict_datetime]\n",
    "init_discount = season_init_discount[season]\n",
    "\n",
    "# Inintial settting to dataframe\n",
    "pred_input = pd.DataFrame({'season': season, 'lead_time': lead_time_vector, 'discount': init_discount})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_result = regr_extr_best.predict(pred_input)\n",
    "vfun = np.vectorize(lambda x: round(x, 2))\n",
    "pred_result = vfun(pred_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "lt_vec_to_pred = {lt_vec: pred for lt_vec, pred in zip(lead_time_vector, pred_result)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "lt_to_lt_vec = {}\n",
    "for i in range(0, 84, 1):\n",
    "    if i < 28:\n",
    "        val = i\n",
    "    else:\n",
    "        val = (i // 7) + 24\n",
    "    lt_to_lt_vec[i * -1] = val * -1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lead_time = np.arange(-83, 1, 1)\n",
    "exp_cum_res_cnt = pd.DataFrame({\n",
    "        'date': [predict_datetime - dt.timedelta(days=int(i*-1)) for i in lead_time],\n",
    "        'lead_time': lead_time,\n",
    "        'curr_discount': init_discount,\n",
    "        'exp_cum_res_cnt': [lt_vec_to_pred[lt_to_lt_vec[i]] for i in lead_time]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 3 : Optimize Best Pricing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# theta2 : Season 및 lead time 별 할인율 조정 전략에 관한 기준 예약률 산출\n",
    "# 과거 season 및 lead time 별 예약률 트렌드 데이터를 기준으로 산출\n",
    "def calc_tehta2(curr_exp_cnt: int, exp_res_cnt: int, exp_capa: int):\n",
    "    return round((curr_exp_cnt + exp_res_cnt) / exp_capa, 2)\n",
    "\n",
    "def cust_linear_func(r: float, ex_res_rate: float, d: float):\n",
    "    \"\"\"\n",
    "    r: Current Reservation Rate\n",
    "    d: Exp Demand Change Rate \n",
    "    ex_res_rate: Exp. Reservation Rate\n",
    "    \"\"\"\n",
    "    # Hyperparamters (need to tune)\n",
    "    theta1 = 1.2       # ratio of increasing / decreasing magnitude : discount\n",
    "    theta2 = 1         # ratio of increasing / decreasing magnitude : demand\n",
    "    phi_low = 1.7    # 1 < phi_low < phi_high < 2\n",
    "    phi_high = 1.4   # 1 < phi_low < phi_high < 2\n",
    "\n",
    "    if d > 0:\n",
    "        y = 1 - theta1 * (r ** (phi_high ** (-1 * (r * theta2 * d))) - ex_res_rate)\n",
    "    else:\n",
    "        y = 1 - theta1 * (r ** (phi_low ** (-1 * ((1 - r) * theta2 * d))) - ex_res_rate)\n",
    "        \n",
    "    return y\n",
    "\n",
    "def calc_sug_disc_rate():\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cust_linear_func(r=0.6, d=0.5, ex_res_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### After Recommendation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "season_1 = res[res['seasonality'] == 1].rent_period/24\n",
    "season_2 = res[res['seasonality'] == 2].rent_period/24\n",
    "season_3 = res[res['seasonality'] == 3].rent_period/24\n",
    "season_4 = res[res['seasonality'] == 4].rent_period/24\n",
    "\n",
    "# Remove Outlier of Rent days(above 7days)\n",
    "season_1 = season_1[season_1 < 8]\n",
    "season_2 = season_2[season_2 < 8]\n",
    "season_3 = season_3[season_3 < 8]\n",
    "season_4 = season_4[season_4 < 8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_rent_period_to_day(season):\n",
    "    conditions = [\n",
    "        (season <= 1),\n",
    "        ((season > 1) & (season <= 2)),\n",
    "        ((season > 2) & (season <= 3)),\n",
    "        ((season > 3) & (season <= 4)),\n",
    "        ((season > 4) & (season <= 5)),\n",
    "        ((season > 5) & (season <= 6)),\n",
    "        ((season > 6) & (season <= 7))\n",
    "    ]\n",
    "    \n",
    "    values = [1, 2, 3, 4, 5, 6, 7]\n",
    "    season_df = pd.DataFrame({'rent_period': season.values})\n",
    "    season_df['rent_period_hours'] = season_df['rent_period'] * 24\n",
    "    season_df['rent_days'] = season_df['rent_period_hours'] // 24\n",
    "    season_df['rent_hours'] = season_df['rent_period_hours'] % 4\n",
    "    season_df['cdw_days'] = np.select(conditions, values)\n",
    "    \n",
    "    return season_df    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rental Fee\n",
    "fee = {\n",
    "    'group_1': {\n",
    "        1: 18000, 2: 36000, 3: 54000, 4: 73000, 5: 73000, 6: 73000, \n",
    "        7: 91000, 8: 104000, 9: 104000, 10: 104000, 11: 104000, 12: 104000,\n",
    "        13: 122000, 14: 130000, 15: 130000, 16: 130000, 17: 130000, 18: 130000,\n",
    "        19: 130000, 20: 130000, 21: 130000, 22: 130000, 23: 130000, 24: 130000    \n",
    "    },\n",
    "    'group_2': {\n",
    "        1: 17000, 2: 34000, 3: 51000, 4: 67000, 5: 67000, 6: 67000, \n",
    "        7: 84000, 8: 96400, 9: 96400, 10: 96400, 11: 96400, 12: 96400,\n",
    "        13: 113000, 14: 120000, 15: 120000, 16: 120000, 17: 120000, 18: 120000,\n",
    "        19: 120000, 20: 120000, 21: 120000, 22: 120000, 23: 120000, 24: 120000    \n",
    "    }\n",
    "}\n",
    "\n",
    "cdw = {\n",
    "    'group_1': {\n",
    "        1: 24000, 2: 48000, 3: 66900, 4: 89200, 5: 111500, 6: 133800, 7: 119700\n",
    "    },\n",
    "    'group_2': {\n",
    "        1: 20000, 2: 34000, 3: 51000, 4: 67000, 5: 67000, 6: 67000, 7: 84000 \n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "season_1_df = convert_rent_period_to_day(season_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "season_1_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "res_20['res_model_nm'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additional Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "season_1 = res[res['seasonality'] == 1].rent_period/24\n",
    "season_2 = res[res['seasonality'] == 2].rent_period/24\n",
    "season_3 = res[res['seasonality'] == 3].rent_period/24\n",
    "season_4 = res[res['seasonality'] == 4].rent_period/24\n",
    "\n",
    "season_1 = season_1[season_1 < 8]\n",
    "season_2 = season_2[season_2 < 8]\n",
    "season_3 = season_3[season_3 < 8]\n",
    "season_4 = season_4[season_4 < 8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(rc={'figure.figsize':(8, 6)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(season_1, hist=True, kde=True, bins=50, color = 'darkblue',\n",
    "             hist_kws={'edgecolor':'black'}, kde_kws={'linewidth': 1.2}).set_title(\"Weekdays\")\n",
    "plt.savefig(os.path.join(save_path, 'img', 'hist', 'Rent Periods (Weekdays).png'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(season_2, hist=True, kde=True, bins=50, color = 'darkblue',\n",
    "             hist_kws={'edgecolor':'black'}, kde_kws={'linewidth': 1.2}).set_title(\"Weekends\")\n",
    "plt.savefig(os.path.join(save_path, 'img', 'hist', 'Rent Periods (Weekends).png'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(season_3, hist=True, kde=True, bins=50, color = 'darkblue',\n",
    "             hist_kws={'edgecolor':'black'}, kde_kws={'linewidth': 1.2}).set_title(\"Holidays\")\n",
    "plt.savefig(os.path.join(save_path, 'img', 'hist', 'Rent Periods (Holidays).png'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sns.distplot(season_4, hist=True, kde=True, bins=50, color = 'darkblue',\n",
    "             hist_kws={'edgecolor':'black'}, kde_kws={'linewidth': 1.2}).set_title(\"Peak Season\")\n",
    "plt.savefig(os.path.join(save_path, 'img', 'hist', 'Rent Periods (Peak Season).png'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def convert_rent_period_to_day(season):\n",
    "    conditions = [\n",
    "        (season <= 1),\n",
    "        ((season > 1) & (season <= 2)),\n",
    "        ((season > 2) & (season <= 3)),\n",
    "        ((season > 3) & (season <= 4)),\n",
    "        ((season > 4) & (season <= 5)),\n",
    "        ((season > 5) & (season <= 6)),\n",
    "        ((season > 6) & (season <= 7))\n",
    "    ]\n",
    "    \n",
    "    values = [1, 2, 3, 4, 5, 6, 7]\n",
    "    season_df = pd.DataFrame({'rent_period': season.values})\n",
    "    season_df['rent_period_hours'] = season_df['rent_period'] * 24\n",
    "    season_df['rent_days'] = season_df['rent_period_hours'] // 24\n",
    "    season_df['rent_hours'] = season_df['rent_period_hours'] % 24\n",
    "    season_df['cdw_days'] = np.select(conditions, values)\n",
    "    \n",
    "    return season_df    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "season_1_df = convert_rent_period_to_day(season_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "349.091px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
